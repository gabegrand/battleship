{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from hfppl.llms import CachedCausalLM\n",
    "from hfppl.inference import smc_standard\n",
    "\n",
    "from battleship.board import Board\n",
    "from battleship.scoring import compute_score\n",
    "from battleship.models import QuestionGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HF_AUTH_TOKEN from .hf_auth_token\n",
    "HF_AUTH_TOKEN = os.environ[\"HF_AUTH_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python311/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/python311/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da76d2a95beb48e7a427ea6e56a749b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/python311/lib/python3.11/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the HuggingFace model\n",
    "lm = CachedCausalLM.from_pretrained(\"codellama/CodeLlama-13b-hf\", auth_token=HF_AUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../battleship/prompts/examples.csv\")\n",
    "\n",
    "def format_example(user_input: str, response: str = None):\n",
    "    return f\"User: {user_input}\\n\" f\"Assistant:{' ' + response if response else ''}\"\n",
    "\n",
    "def make_question_prompt(df, board=None, instructions=None):\n",
    "    prompt = \"\"\n",
    "    if instructions != None:\n",
    "        prompt += f\"Instructions:\\n{instructions}\\n\"\n",
    "    if board != None:\n",
    "        prompt += \"Board:\\n\" + board.to_textual_description() + \"\\n\"\n",
    "    prompt += \"Questions:\\n\" + \"\\n\".join(df.question) + \"\\n\"\n",
    "    return prompt\n",
    "\n",
    "def make_question_to_code_prompt(df):\n",
    "    prompt = \"\\n\".join([format_example(q, r) for q, r in zip(df.question, df.code)]) + \"\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"User input will be a series of sentences representing a board from Battleship, the board game, that you should aim to win. Tiles in the board can either be 'Water' tiles, 'Blue Ship' tiles, 'Red Ship' tiles, and 'Purple Ship' tiles (there are only these three battleships). Some tiles may also be 'Hidden' tiles, meaning they could be any of the others but have not been revealed yet. The user will denote coordinates as follows: columns are numbered from 1 onwards, where column 1 is the leftmost column, and rows are given a letter from A onwards where row A is the topmost row (so the cell at the second row and second column is B2). Your role is to ask the most informative possible question from the context given: strictly output the question only, and make sure the questions are relevant to the context: 'Which cells should I target to sink the battleships with the least number of moves?' is not a relevant question because it is the general goal of Battleship. Questions also need to be answerable with yes or no, no other questions will be considered in scope.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single board evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1-A is a hidden tile.\\n1-B is a hidden tile.\\n1-C is a purple ship tile.\\n1-D is a hidden tile.\\n1-E is a hidden tile.\\n1-F is a hidden tile.\\n2-A is a hidden tile.\\n2-B is a hidden tile.\\n2-C is a water tile.\\n2-D is a hidden tile.\\n2-E is a hidden tile.\\n2-F is a hidden tile.\\n3-A is a hidden tile.\\n3-B is a hidden tile.\\n3-C is a hidden tile.\\n3-D is a water tile.\\n3-E is a hidden tile.\\n3-F is a hidden tile.\\n4-A is a hidden tile.\\n4-B is a hidden tile.\\n4-C is a hidden tile.\\n4-D is a blue ship tile.\\n4-E is a hidden tile.\\n4-F is a hidden tile.\\n5-A is a hidden tile.\\n5-B is a hidden tile.\\n5-C is a hidden tile.\\n5-D is a blue ship tile.\\n5-E is a hidden tile.\\n5-F is a hidden tile.\\n6-A is a hidden tile.\\n6-B is a hidden tile.\\n6-C is a hidden tile.\\n6-D is a blue ship tile.\\n6-E is a hidden tile.\\n6-F is a hidden tile.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "async def single_smc_baseline(board_id,particle_num,instructions):\n",
    "    board = Board.from_trial_id(board_id)\n",
    "    board.to_textual_description()\n",
    "\n",
    "    model = QuestionGenerationModel(\n",
    "        lm=lm,\n",
    "        board=board,\n",
    "        question_prompt=make_question_prompt(df),\n",
    "        translation_prompt=make_question_to_code_prompt(df),\n",
    "    )\n",
    "\n",
    "    model_combined = QuestionGenerationModel(\n",
    "        lm=lm,\n",
    "        board=board,\n",
    "        question_prompt=make_question_prompt(df,board=board,instructions=instructions),\n",
    "        translation_prompt=make_question_to_code_prompt(df),\n",
    "    )\n",
    "\n",
    "    particles = await smc_standard(model, n_particles=particle_num)\n",
    "    print(\"Done with standard model...\")\n",
    "    particles_c = await smc_standard(model_combined, n_particles=particle_num)\n",
    "    print(\"Done with combined model...\")\n",
    "    return [particles,particles_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model instance\n",
    "\n",
    "model = QuestionGenerationModel(\n",
    "    lm=lm,\n",
    "    board=board,\n",
    "    question_prompt=make_question_prompt(df),\n",
    "    translation_prompt=make_question_to_code_prompt(df),\n",
    ")\n",
    "\n",
    "model_combined = QuestionGenerationModel(\n",
    "    lm=lm,\n",
    "    board=board,\n",
    "    question_prompt=make_question_prompt(df,board=board,instructions=instructions),\n",
    "    translation_prompt=make_question_to_code_prompt(df),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_num = 5\n",
    "\n",
    "particles = await smc_standard(model, n_particles=particle_num)\n",
    "print(\"Done with standard model...\")\n",
    "particles_c = await smc_standard(model_combined, n_particles=particle_num)\n",
    "print(\"Done with combined model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = []\n",
    "particle_types = [particles,particles_c]\n",
    "\n",
    "for particle_type in particle_types:\n",
    "    for i, p in enumerate(particle_type):\n",
    "        df_p = pd.DataFrame(p.get_final_results())\n",
    "        df_p[\"particle\"] = i\n",
    "        df_results.append(df_p)\n",
    "    df_results = pd.concat(df_results).reset_index(drop=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in particles:\n",
    "    print(f\"Question: {str(p.context)}\")\n",
    "    print(f\"|- Program: {p.result['translation']}\")\n",
    "    print(f\"|- EIG: {compute_score(board=board, program=p.result['translation'])}\")\n",
    "    print(f\"|- Particle weight: {p.weight:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple board evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def smc_baseline(particles=5,trials=18,model_types=[\"REGULAR\",\"COMBINED\"]):\n",
    "    for model_type in model_types:\n",
    "        for trial_id in range(1,trials+1):\n",
    "            df_results = []\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"Trial {trial_id}\")\n",
    "            print(\"-\" * 80)\n",
    "            board = Board.from_trial_id(trial_id)\n",
    "            instructions_used = None if model_type == \"REGULAR\" else instructions\n",
    "            model = QuestionGenerationModel(\n",
    "                    lm=lm,\n",
    "                    board=board,\n",
    "                    question_prompt=make_question_prompt(df, board=board, instructions=instructions_used),\n",
    "                    translation_prompt=make_question_to_code_prompt(df),\n",
    "                )\n",
    "            particles = await smc_standard(model, n_particles=particles)\n",
    "            df_trial = []\n",
    "            for i, p in enumerate(particles):\n",
    "                df_p = pd.DataFrame(p.get_final_results())\n",
    "                df_p[\"particle\"] = i\n",
    "                df_p[\"model_type\"] = model_type\n",
    "                df_trial.append(df_p)\n",
    "            df_trial = pd.concat(df_trial).reset_index(drop=True)\n",
    "            df_trial[\"trial_id\"] = trial_id\n",
    "            df_results.append(df_trial)\n",
    "            df_results = pd.concat(df_results).reset_index(drop=True)\n",
    "            if not os.path.isfile('hfppl_results.csv'):\n",
    "                df_results.to_csv('hfppl_results.csv', header='column_names', index=False)\n",
    "            else:\n",
    "                df_results.to_csv(\"hfppl_results.csv\", mode=\"a\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_IDS = 19\n",
    "N_PARTICLES = 5\n",
    "\n",
    "smc_baseline(particles=N_PARTICLES,trials=TRIAL_IDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
