{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from hfppl.llms import CachedCausalLM\n",
    "from hfppl.inference import smc_standard\n",
    "\n",
    "from battleship.board import Board\n",
    "from battleship.scoring import compute_score\n",
    "from battleship.models import QuestionGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HF_AUTH_TOKEN from .hf_auth_token\n",
    "with open(os.path.join(\"../\", \".hf_auth_token\"), \"r\") as f:\n",
    "    os.environ[\"HF_AUTH_TOKEN\"] = f.read().strip()\n",
    "\n",
    "HF_AUTH_TOKEN = os.environ[\"HF_AUTH_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFace model\n",
    "lm = CachedCausalLM.from_pretrained(\"codellama/CodeLlama-13b-hf\", auth_token=HF_AUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../battleship/prompts/examples.csv\")\n",
    "\n",
    "def format_example(user_input: str, response: str = None):\n",
    "    return f\"User: {user_input}\\n\" f\"Assistant:{' ' + response if response else ''}\"\n",
    "\n",
    "def make_question_prompt(df, board=None, instructions=None):\n",
    "    prompt = \"\"\n",
    "    if instructions != None:\n",
    "        prompt += f\"Instructions:\\n{instructions}\\n\"\n",
    "    if board != None:\n",
    "        prompt += \"Board:\\n\" + board.to_textual_description() + \"\\n\"\n",
    "    prompt += \"Questions:\\n\" + \"\\n\".join(df.question) + \"\\n\"\n",
    "    return prompt\n",
    "\n",
    "def make_question_to_code_prompt(df):\n",
    "    prompt = \"\\n\".join([format_example(q, r) for q, r in zip(df.question, df.code)]) + \"\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"User input will be a series of sentences representing a board from Battleship, the board game, that you should aim to win. Tiles in the board can either be 'Water' tiles, 'Blue Ship' tiles, 'Red Ship' tiles, and 'Purple Ship' tiles (there are only these three battleships). Some tiles may also be 'Hidden' tiles, meaning they could be any of the others but have not been revealed yet. The user will denote coordinates as follows: columns are numbered from 1 onwards, where column 1 is the leftmost column, and rows are given a letter from A onwards where row A is the topmost row (so the cell at the second row and second column is B2). Your role is to ask the most informative possible question from the context given: strictly output the question only, and make sure the questions are relevant to the context: 'Which cells should I target to sink the battleships with the least number of moves?' is not a relevant question because it is the general goal of Battleship. Questions also need to be answerable with yes or no, no other questions will be considered in scope.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single board evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def single_smc_baseline(board_id,particle_num,instructions):\n",
    "    board = Board.from_trial_id(board_id)\n",
    "    board.to_textual_description()\n",
    "\n",
    "    model = QuestionGenerationModel(\n",
    "        lm=lm,\n",
    "        board=board,\n",
    "        question_prompt=make_question_prompt(df),\n",
    "        translation_prompt=make_question_to_code_prompt(df),\n",
    "    )\n",
    "\n",
    "    model_combined = QuestionGenerationModel(\n",
    "        lm=lm,\n",
    "        board=board,\n",
    "        question_prompt=make_question_prompt(df,board=board,instructions=instructions),\n",
    "        translation_prompt=make_question_to_code_prompt(df),\n",
    "    )\n",
    "\n",
    "    particles = await smc_standard(model, n_particles=particle_num)\n",
    "    print(\"Done with standard model...\")\n",
    "    particles_c = await smc_standard(model_combined, n_particles=particle_num)\n",
    "    print(\"Done with combined model...\")\n",
    "    return [particles,particles_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = []\n",
    "particle_types = [particles,particles_c]\n",
    "\n",
    "for particle_type in particle_types:\n",
    "    for i, p in enumerate(particle_type):\n",
    "        df_p = pd.DataFrame(p.get_final_results())\n",
    "        df_p[\"particle\"] = i\n",
    "        df_results.append(df_p)\n",
    "    df_results = pd.concat(df_results).reset_index(drop=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in particles:\n",
    "    print(f\"Question: {str(p.context)}\")\n",
    "    print(f\"|- Program: {p.result['translation']}\")\n",
    "    print(f\"|- EIG: {compute_score(board=board, program=p.result['translation'])}\")\n",
    "    print(f\"|- Particle weight: {p.weight:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple board evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_smc_baseline(n_particles=5, trial_ids=range(1, 19), model_types=[\"REGULAR\", \"COMBINED\"]):\n",
    "    df_results = []\n",
    "    for trial_id in trial_ids:\n",
    "        for model_type in model_types:\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"Trial {trial_id}\")\n",
    "            print(f\"Model type: {model_type}\")\n",
    "            print(\"-\" * 80)\n",
    "            board = Board.from_trial_id(trial_id)\n",
    "            instructions_used = None if model_type == \"REGULAR\" else instructions\n",
    "            model = QuestionGenerationModel(\n",
    "                    lm=lm,\n",
    "                    board=board,\n",
    "                    question_prompt=make_question_prompt(df, board=board, instructions=instructions_used),\n",
    "                    translation_prompt=make_question_to_code_prompt(df),\n",
    "                )\n",
    "            particles = await smc_standard(model, n_particles=n_particles)\n",
    "            df_trial = []\n",
    "            for i, p in enumerate(particles):\n",
    "                df_p = pd.DataFrame(p.get_final_results())\n",
    "                df_p[\"particle\"] = i\n",
    "                df_p[\"model_type\"] = model_type\n",
    "                df_trial.append(df_p)\n",
    "            df_trial = pd.concat(df_trial).reset_index(drop=True)\n",
    "            df_trial[\"trial_id\"] = trial_id\n",
    "            df_results.append(df_trial)\n",
    "            df_results = pd.concat(df_results).reset_index(drop=True)\n",
    "            df_results.to_csv(\"hfppl_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_IDS = range(1, 19)\n",
    "N_PARTICLES = 5\n",
    "\n",
    "await run_smc_baseline(n_particles=N_PARTICLES, trial_ids=TRIAL_IDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-step SMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIAL_IDS = range(1, 19)\n",
    "TRIAL_IDS = [13]\n",
    "N_PARTICLES = 1\n",
    "\n",
    "await run_smc_baseline(n_particles=N_PARTICLES, trial_ids=TRIAL_IDS, model_types=[\"REGULAR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
