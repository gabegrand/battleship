{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from hfppl.llms import CachedCausalLM\n",
    "from hfppl.inference import smc_standard\n",
    "from hfppl.distributions import LMContext\n",
    "\n",
    "from battleship.v1.board import Board, TRIAL_IDS\n",
    "from battleship.prompting import QuestionGenerationPrompt, TranslationPrompt\n",
    "from battleship.scoring import compute_score\n",
    "from battleship.models import QuestionGenerationModel, SingleStepQuestionGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the HuggingFace model\n",
    "lm = CachedCausalLM.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "# lm = CachedCausalLM.from_pretrained(\"codellama/CodeLlama-13b-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run SMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_smc_baseline(\n",
    "    n_particles=5,\n",
    "    trial_ids=TRIAL_IDS,\n",
    "    board_format=\"textual\",\n",
    "    include_board=False,\n",
    "    include_instructions=False,\n",
    "    include_system_prompt=False,\n",
    "    # question prompt\n",
    "    q_n_example_trials=3,\n",
    "    q_n_examples_per_trial=3,\n",
    "    q_cache_prompt=False,\n",
    "    # translation prompt\n",
    "    t_n_example_trials=10,\n",
    "    t_n_examples_per_trial=1,\n",
    "    t_cache_prompt=False,\n",
    "    results_file=\"hfppl_results.csv\",\n",
    "    random_seed=123,\n",
    "    verbose=False,\n",
    "):\n",
    "    results_all = []\n",
    "    for trial_id in trial_ids:\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Trial {trial_id}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        # TODO: Sample the question prompt each trial?\n",
    "        question_prompt = QuestionGenerationPrompt(\n",
    "            target_trial_id=trial_id,\n",
    "            board_format=board_format,\n",
    "            n_example_trials=q_n_example_trials,\n",
    "            n_examples_per_trial=q_n_examples_per_trial,\n",
    "            include_board=include_board,\n",
    "            include_instructions=include_instructions,\n",
    "            include_system_prompt=include_system_prompt,\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "\n",
    "        # TODO: Sample the translation prompt each trial?\n",
    "        translation_prompt = TranslationPrompt(\n",
    "            target_trial_id=trial_id,\n",
    "            n_example_trials=t_n_example_trials,\n",
    "            n_examples_per_trial=t_n_examples_per_trial,\n",
    "            include_board=include_board,\n",
    "            include_instructions=include_instructions,\n",
    "            include_system_prompt=include_system_prompt,\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        print(\"QUESTION PROMPT\")\n",
    "        print(\"-\" * 80)\n",
    "        print(str(question_prompt))\n",
    "        print(\"-\" * 80)\n",
    "        print(\"TRANSLATION PROMPT\")\n",
    "        print(\"-\" * 80)\n",
    "        print(str(translation_prompt))\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        lm.clear_cache()\n",
    "        lm.clear_kv_cache()\n",
    "\n",
    "        # Caching speeds up performance, but may result in CUDA out of memory error.\n",
    "        if q_cache_prompt:\n",
    "            lm.cache_kv(lm.tokenizer.encode(str(question_prompt)))\n",
    "        if t_cache_prompt:\n",
    "            # Additionally, this currently degrades the quality of the translations for an unknown reason.\n",
    "            lm.cache_kv(lm.tokenizer.encode(str(translation_prompt)))\n",
    "\n",
    "        model = QuestionGenerationModel(\n",
    "            lm=lm,\n",
    "            board=Board.from_trial_id(trial_id),\n",
    "            question_prompt=str(question_prompt),\n",
    "            translation_prompt=str(translation_prompt),\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        particles = await smc_standard(model, n_particles=n_particles)\n",
    "\n",
    "        results_trial = []\n",
    "        for i, p in enumerate(particles):\n",
    "            df_p = pd.DataFrame(p.get_final_results())\n",
    "            df_p[\"particle\"] = i\n",
    "            results_trial.append(df_p)\n",
    "        df_trial = pd.concat(results_trial).reset_index(drop=True)\n",
    "        df_trial[\"trial_id\"] = trial_id\n",
    "        results_all.append(df_trial)\n",
    "        df_results = pd.concat(results_all).reset_index(drop=True)\n",
    "        df_results.to_csv(results_file, index=False)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN SMC BASELINE\n",
    "df_results = await run_smc_baseline(\n",
    "    n_particles=1,\n",
    "    trial_ids=[13],\n",
    "    board_format=\"textual\",\n",
    "    include_board=False,\n",
    "    include_instructions=False,\n",
    "    include_system_prompt=False,\n",
    "    q_n_example_trials=3,\n",
    "    q_n_examples_per_trial=10,\n",
    "    t_n_example_trials=12,\n",
    "    t_n_examples_per_trial=1,\n",
    "    q_cache_prompt=True,\n",
    "    t_cache_prompt=False,\n",
    "    results_file=\"results/eval_smc.csv\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(\"score\", ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.query(\"type == 'final'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.clear_cache()\n",
    "lm.clear_kv_cache()\n",
    "\n",
    "question_prompt = QuestionGenerationPrompt(\n",
    "    target_trial_id=13,\n",
    "    board_format=\"grid\",\n",
    "    n_example_trials=3,\n",
    "    n_examples_per_trial=3,\n",
    "    include_board=False,\n",
    "    include_instructions=False,\n",
    "    include_system_prompt=False,\n",
    "    random_seed=123,\n",
    ")\n",
    "\n",
    "translation_prompt = TranslationPrompt(\n",
    "    target_trial_id=13,\n",
    "    n_example_trials=10,\n",
    "    n_examples_per_trial=1,\n",
    "    include_system_prompt=False,\n",
    "    include_instructions=False,\n",
    "    random_seed=123,\n",
    ")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(str(question_prompt))\n",
    "print(\"-\" * 80)\n",
    "print(str(translation_prompt))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# lm.cache_kv(lm.tokenizer.encode(str(translation_prompt)))\n",
    "\n",
    "# Alternative way to force the LM to cache the translation prompt\n",
    "# from hfppl.distributions import LMContext\n",
    "# from hfppl.modeling import Model\n",
    "# model2 = Model()\n",
    "# ctx = LMContext(\n",
    "#     lm,\n",
    "#     str(translation_prompt),\n",
    "#     temp=0.1,\n",
    "# )\n",
    "# token = await model2.sample(ctx.next_token())\n",
    "\n",
    "model = QuestionGenerationModel(\n",
    "    lm=lm,\n",
    "    board=Board.from_trial_id(13),\n",
    "    question_prompt=str(question_prompt),\n",
    "    translation_prompt=str(translation_prompt),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# partial_questions = [\n",
    "#     \"What\",\n",
    "#     \"Is\",\n",
    "#     \"How many\",\n",
    "# ]\n",
    "\n",
    "# print(\"-\" * 80)\n",
    "# print(\"QUESTIONS\")\n",
    "# for q in partial_questions:\n",
    "#     completion = await model._complete_question(q)\n",
    "#     print(completion)\n",
    "\n",
    "questions = [\n",
    "    \"What is the length of the red ship?\",\n",
    "    \"What is the top left corner of the blue ship?\",\n",
    "    \"Is there a ship at 3C?\",\n",
    "]\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"TRANSLATIONS\")\n",
    "for q in questions:\n",
    "    print(\"-\" * 80)\n",
    "    print(q)\n",
    "    completion = await model._translate_question(q)\n",
    "    print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token 29871 is empty space\n",
    "\n",
    "# WITH CACHING\n",
    "# tokens = [13, 2659, 29901, 1317, 727, 263, 7751, 472, 29871, 29906, 29923, 29973, 13, 3010, 29901, 1317, 727, 263, 7751, 472, 29871, 29906, 29923, 29973, 13]\n",
    "\n",
    "# WITHOUT CACHING\n",
    "tokens = [1, 4911, 29901, 2180, 825, 4423, 338, 278, 2246, 2175, 760, 310, 278, 2654, 7751, 29973, 13, 3010, 29901, 313, 3332, 1563, 313, 2780, 287, 29911, 5475, 4367, 876, 13, 2659, 29901, 1128, 1784, 260, 5475, 338, 278, 3708, 552, 7751, 29973, 13, 3010, 29901, 313, 2311, 15247, 552, 29897, 13, 2659, 29901, 1317, 727, 263, 7751, 472, 29871, 29946, 29909, 29973, 13, 3010, 29901, 313, 1333, 313, 1360, 313, 2780, 29871, 29946, 29909, 29897, 13062, 876, 13, 2659, 29901, 1724, 2927, 338, 472, 29871, 29953, 29943, 29973, 13, 3010, 29901, 313, 2780, 29871, 29953, 29943, 29897, 13, 2659, 29901, 1128, 1784, 260, 5475, 338, 278, 2654, 7751, 29973, 13, 3010, 29901, 313, 2311, 4367, 29897, 13, 2659, 29901, 1128, 1784, 260, 5475, 338, 278, 3708, 552, 7751, 29973, 13, 3010, 29901, 313, 2311, 15247, 552, 29897, 13, 2659, 29901, 1938, 278, 2654, 7751, 322, 278, 3708, 552, 7751, 6023, 29973, 13, 3010, 29901, 313, 16747, 4367, 15247, 552, 29897, 13, 2659, 29901, 1128, 1784, 260, 5475, 338, 278, 2654, 7751, 29973, 13, 3010, 29901, 313, 2311, 4367, 29897, 13, 2659, 29901, 1724, 338, 278, 4423, 310, 697, 3708, 552, 25900, 29973, 13, 3010, 29901, 313, 3332, 1563, 313, 2780, 287, 29911, 5475, 15247, 552, 876, 13, 2659, 29901, 1128, 1784, 260, 5475, 338, 278, 3708, 552, 7751, 29973, 13, 3010, 29901, 313, 2311, 15247, 552, 29897, 13, 2659, 29901, 1317, 727, 263, 7751, 472, 29871, 29945, 29909, 29973, 13, 3010, 29901, 313, 1333, 313, 1360, 313, 2780, 29871, 29945, 29909, 29897, 13062, 876, 13]\n",
    "\n",
    "for t in tokens:\n",
    "    print(t, repr(lm.tokenizer.decode([t])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
