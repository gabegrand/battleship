{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from battleship.grammar import BattleshipGrammar\n",
    "from battleship.scoring import compute_score, compute_score_parallel\n",
    "from battleship.board import Board\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "RESULTS_FILENAME = \"results.csv\"\n",
    "COMMAND_FILENAME = \"command.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = BattleshipGrammar(include_lambdas=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_generation(samples: int = 10000, min_depth: int = 1, max_depth: int = 16):\n",
    "    generations = []\n",
    "    while len(generations) != samples:\n",
    "        for _ in range(samples - len(generations)):\n",
    "            prog = grammar.sample(min_depth=min_depth,max_depth=max_depth)\n",
    "            generations.append(prog)\n",
    "        generations = [i for i in generations if i != None]\n",
    "    return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_baseline(\n",
    "    cores=int(os.cpu_count() / 2),\n",
    "    samples: int = 10000,\n",
    "    min_depth: int = 1,\n",
    "    max_depth: int = 16,\n",
    "    sample_size: int = 50,\n",
    "    output_dir: str = \"results_official\"\n",
    "):\n",
    "    time_start = time.time()\n",
    "    timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    experiment_dir = os.path.join(\n",
    "        os.getcwd(), #this is suboptimal but the __file__ variable isn't recognized in interactive shells, the correct way of doing this with __file__ is in the .py version of this script \n",
    "        output_dir,\n",
    "        f\"sampling-depths-{min_depth}-{max_depth}-{timestamp}\",\n",
    "    )\n",
    "    os.makedirs(experiment_dir, exist_ok=True)\n",
    "    results_filepath = os.path.join(experiment_dir, RESULTS_FILENAME)\n",
    "    print(f\"Results will be saved to: {results_filepath}\")\n",
    "\n",
    "\n",
    "    results = []\n",
    "    acceptable_programs = repeated_generation(samples, min_depth, max_depth)\n",
    "    for id in range(1, 18 + 1):\n",
    "        board_start_time = time.time()\n",
    "        cache = {}\n",
    "        print(f\"board {id}\", end=\" | \")\n",
    "        if cores > 1:\n",
    "            for sample_index in tqdm(range(ceil(samples / sample_size))):\n",
    "                unique_programs = []\n",
    "                program_tuples = acceptable_programs[\n",
    "                    sample_index * sample_size : ((sample_index + 1) * sample_size)\n",
    "                ]\n",
    "                program_selection = [item[0] for item in program_tuples]\n",
    "                program_selection_depths = [item[1] for item in program_tuples]\n",
    "                for program in program_selection:\n",
    "                    key = (program, id)\n",
    "                    if key in list(cache.keys()):\n",
    "                        result = {\n",
    "                            \"program\": program,\n",
    "                            \"board_id\": id,\n",
    "                            \"score\": cache[key],\n",
    "                            \"depth\": program_selection_depths[index],\n",
    "                            \"min_depth\": min_depth,\n",
    "                            \"max_depth\": max_depth,\n",
    "                        }\n",
    "                        results.append(result)\n",
    "                    else:\n",
    "                        unique_programs.append(program)\n",
    "\n",
    "                program_scores = compute_score_parallel(\n",
    "                    programs=unique_programs,\n",
    "                    board=Board.from_trial_id(id),\n",
    "                    processes=cores,\n",
    "                    show_progress=False,\n",
    "                )\n",
    "                for index in range(len(program_scores)):\n",
    "                    cache[(unique_programs[index], id)] = program_scores[index]\n",
    "                    result = {\n",
    "                        \"program\": program_selection[index],\n",
    "                        \"board_id\": id,\n",
    "                        \"score\": program_scores[index],\n",
    "                        \"depth\": program_selection_depths[index],\n",
    "                        \"min_depth\": min_depth,\n",
    "                        \"max_depth\": max_depth,\n",
    "                    }\n",
    "                    results.append(result)\n",
    "        print(f\"finished scoring in {round(time.time()-board_start_time,2)}s from the start\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(results_filepath, index=False)\n",
    "    #df.to_csv(f\"sampling_data_depths_{min_depth}_{max_depth}.csv\", mode=\"a\", header=False)\n",
    "    print(f\"finished {samples}-shot sampling at depth {(min_depth,max_depth)} in time {time.time() - time_start}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: /home/ubuntu/battleship/experiments/cogsci/results_official/sampling-depths-1-16-2024-02-19-21-47-04/results.csv\n",
      "board 1 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 2.1s from the start\n",
      "board 2 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.28s from the start\n",
      "board 3 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.07s from the start\n",
      "board 4 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.06s from the start\n",
      "board 5 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.05s from the start\n",
      "board 6 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.05s from the start\n",
      "board 7 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.06s from the start\n",
      "board 8 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.06s from the start\n",
      "board 9 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.06s from the start\n",
      "board 10 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.59s from the start\n",
      "board 11 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.09s from the start\n",
      "board 12 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.07s from the start\n",
      "board 13 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.06s from the start\n",
      "board 14 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.06s from the start\n",
      "board 15 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.05s from the start\n",
      "board 16 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.05s from the start\n",
      "board 17 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.05s from the start\n",
      "board 18 | "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished scoring in 0.05s from the start\n",
      "finished 10-shot sampling at depth (1, 16) in time 3.88108229637146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = sample_baseline(cores=os.cpu_count()-1,samples=10, min_depth=1, max_depth=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_baseline_debug(\n",
    "    cores=int(os.cpu_count() / 2),\n",
    "    samples: int = 10,\n",
    "    min_depth: int = 1,\n",
    "    max_depth: int = 16,\n",
    "    sample_size: int = 2,\n",
    "):\n",
    "    results = []\n",
    "    #acceptable_programs = repeated_generation(samples, min_depth, max_depth)\n",
    "    acceptable_programs = [(i,\"debug\") for i in pd.read_csv(\"top_10_grammar_programs.csv\")[\"program\"].to_list()]\n",
    "    for id in range(1, 18 + 1):\n",
    "        start_time = time.time()\n",
    "        cache = {}\n",
    "        print(f\"board {id}\", end=\" | \")\n",
    "        if cores > 1:\n",
    "            for sample_index in tqdm(range(ceil(samples / sample_size))):\n",
    "                unique_programs = []\n",
    "                program_tuples = acceptable_programs[\n",
    "                    sample_index * sample_size : ((sample_index + 1) * sample_size)\n",
    "                ]\n",
    "                program_selection = [item[0] for item in program_tuples]\n",
    "                program_selection_depths = [item[1] for item in program_tuples]\n",
    "                for program in program_selection:\n",
    "                    key = (program, id)\n",
    "                    if key in list(cache.keys()):\n",
    "                        result = {\n",
    "                            \"program\": program,\n",
    "                            \"board_id\": id,\n",
    "                            \"score\": cache[key],\n",
    "                            \"depth\": program_selection_depths[index],\n",
    "                            \"min_depth\": min_depth,\n",
    "                            \"max_depth\": max_depth,\n",
    "                        }\n",
    "                        results.append(result)\n",
    "                    else:\n",
    "                        unique_programs.append(program)\n",
    "\n",
    "                program_scores = compute_score_parallel(\n",
    "                    programs=unique_programs,\n",
    "                    board=Board.from_trial_id(id),\n",
    "                    processes=cores,\n",
    "                    show_progress=False,\n",
    "                )\n",
    "                for index in range(len(program_scores)):\n",
    "                    cache[(program_selection[index], id)] = program_scores[index]\n",
    "                    result = {\n",
    "                        \"program\": program_selection[index],\n",
    "                        \"board_id\": id,\n",
    "                        \"score\": program_scores[index],\n",
    "                        \"depth\": program_selection_depths[index],\n",
    "                        \"min_depth\": min_depth,\n",
    "                        \"max_depth\": max_depth,\n",
    "                    }\n",
    "                    results.append(result)\n",
    "        print(f\"finished scoring in {round(time.time()-start_time,2)}s from the start\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"sampling_data_debug!.csv\", mode=\"a\", header=False)\n",
    "    print(f\"finished {samples}-shot sampling at depth {(min_depth,max_depth)}\")\n",
    "    return df\n",
    "\n",
    "sample_baseline_debug()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
