{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e28f440",
   "metadata": {},
   "source": [
    "# Captain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from battleship.run_captain_benchmarks import rebuild_captain_summary_from_results\n",
    "from battleship.utils import resolve_project_path, PROJECT_ROOT\n",
    "from battleship.agents import EIGCalculator, CodeQuestion, Question\n",
    "from battleship.game import Board\n",
    "\n",
    "from analysis import (\n",
    "    CAPTAIN_TYPE_LABELS,\n",
    "    MODEL_DISPLAY_NAMES,\n",
    "    human_round_summaries,\n",
    "    build_competitor_column,\n",
    "    compute_pairwise_win_rates,\n",
    "    plot_grouped_winrate_heatmap,\n",
    "    load_dataset,\n",
    "    plot_question_timing,  # added\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542334d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# set seaborn color palette\n",
    "sns.set_palette(\"tab10\")\n",
    "\n",
    "# set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_EXPERIMENT_NAME = \"battleship-final-data\"\n",
    "PATH_DATA = os.path.join(\"data\", HUMAN_EXPERIMENT_NAME)\n",
    "\n",
    "# PATH_EXPORT = os.path.join(PATH_DATA, \"export\")\n",
    "PATH_EXPORT = os.path.join(PROJECT_ROOT, \"..\", \"battleship-iclr-2026\", \"iclr2026\", \"_figures_staging\") # Export directly into the paper draft\n",
    "\n",
    "CAPTAIN_EXPERIMENT_PATH = (\n",
    "    \"experiments/collaborative/captain_benchmarks/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0bcb23",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d1618",
   "metadata": {},
   "source": [
    "### Human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1eaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df = human_round_summaries(\n",
    "    experiment_path=PATH_DATA,\n",
    ")\n",
    "human_df = pd.DataFrame(human_df)\n",
    "\n",
    "human_df = human_df.assign(llm=\"Human\")\n",
    "human_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e47cd7",
   "metadata": {},
   "source": [
    "### Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77baafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_round_data_unresolved_paths = [\n",
    "    (\"gpt-4o\", \"run_2025_08_25_16_28_19\"),\n",
    "    (\"gpt-5\", \"run_2025_08_25_22_02_29\"),\n",
    "    (\"llama-4-scout\", \"run_2025_08_26_17_56_46\"),\n",
    "    (\"Baseline\", \"run_2025_08_26_17_23_23\"),\n",
    "]\n",
    "\n",
    "model_round_data_paths = [\n",
    "    (name, resolve_project_path(os.path.join(CAPTAIN_EXPERIMENT_PATH, path)))\n",
    "    for name, path in model_round_data_unresolved_paths\n",
    "]\n",
    "for name, path in model_round_data_paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"The path {path} does not exist.\")\n",
    "\n",
    "dfs = []\n",
    "for name, path in model_round_data_paths:\n",
    "    df = pd.DataFrame(rebuild_captain_summary_from_results(path))\n",
    "    if df.empty:\n",
    "        continue\n",
    "    df[\"llm\"] = name\n",
    "    df[\"run_dir\"] = path  # retain run directory for downstream file access\n",
    "    dfs.append(df)\n",
    "\n",
    "model_df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58184756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append summary_df to round_df\n",
    "df = pd.concat([human_df, model_df], ignore_index=True)\n",
    "\n",
    "primary_columns = [\"captain_type_display\", \"llm_display\", \"board_id\", \"seed\"]\n",
    "\n",
    "# Create categorical column for captain_type_display\n",
    "df[\"captain_type_display\"] = pd.Categorical(\n",
    "    df[\"captain_type\"].map(CAPTAIN_TYPE_LABELS),\n",
    "    categories=list(dict.fromkeys(CAPTAIN_TYPE_LABELS.values())),\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "# Create categorical column for llm_display\n",
    "df[\"llm_display\"] = pd.Categorical(\n",
    "    df[\"llm\"],\n",
    "    categories=[\"Human\", \"Baseline\"] + [x for x in MODEL_DISPLAY_NAMES.values() if x in df[\"llm\"].unique()],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "# Move primary columns to the front\n",
    "df = df[primary_columns + [col for col in df.columns if col not in primary_columns]]\n",
    "\n",
    "# Sort the DataFrame by primary columns\n",
    "df = df.sort_values(by=primary_columns, ascending=True).reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7299bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group sizes, then drop any groups with size == 0\n",
    "group_sizes = df.groupby([\"captain_type_display\", \"llm_display\"]).size()\n",
    "group_sizes = group_sizes[group_sizes != 0]\n",
    "\n",
    "# nicer tabular view if needed\n",
    "group_counts = group_sizes.reset_index(name=\"count\")\n",
    "group_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad155118",
   "metadata": {},
   "source": [
    "## Precision/Recall Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBreakdown by captain_type_display:\")\n",
    "for captain_type in df['captain_type_display'].cat.categories:\n",
    "    llms = df[df['captain_type_display'] == captain_type]['llm'].unique()\n",
    "    print(f\"{captain_type}: {llms}\")\n",
    "\n",
    "\n",
    "# Colorblind-friendly palette (Okabe–Ito)\n",
    "llm_palette = {\n",
    "    \"Human\": \"#009E73\",  # green\n",
    "    \"Baseline\": \"#0072B2\",  # blue\n",
    "    \"llama-4-scout\": \"#CC79A7\",  # purple\n",
    "    \"gpt-4o\": \"#E69F00\",  # orange (similar to gpt-5)\n",
    "    \"gpt-5\": \"#D55E00\",  # vermillion\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcce8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Targeting Score (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85577287",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Prepare ordered captain categories that actually appear in the data\n",
    "captain_categories = [\n",
    "    c for c in df[\"captain_type_display\"].cat.categories\n",
    "    if c in df[\"captain_type_display\"].values\n",
    "]\n",
    "\n",
    "# Determine max number of llm groups present for any captain (for consistent box widths)\n",
    "llm_counts = df.groupby(\"captain_type_display\")[\"llm\"].nunique()\n",
    "max_llms = int(llm_counts.max()) if len(llm_counts) > 0 else 1\n",
    "\n",
    "# Base positions for each captain on the x axis\n",
    "x_positions = np.arange(len(captain_categories))\n",
    "\n",
    "# Box width: leave some padding between captain groups\n",
    "group_width = 0.5  # total width occupied by boxes for one captain\n",
    "box_width = group_width / max_llms\n",
    "\n",
    "# Ensure grid lines are drawn below plot elements and only horizontal gridlines are shown\n",
    "ax.set_axisbelow(True)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# Map captain -> present llms to ensure we only plot existing combinations\n",
    "# Use the llm_display categorical ordering so order is consistent across plots\n",
    "llm_order = list(df[\"llm_display\"].cat.categories) if \"llm_display\" in df.columns else sorted(df[\"llm\"].unique())\n",
    "\n",
    "for i, captain in enumerate(captain_categories):\n",
    "    present_llms_unsorted = df[df[\"captain_type_display\"] == captain][\"llm\"].unique()\n",
    "    # Preserve the display order\n",
    "    present_llms = [llm for llm in llm_order if llm in present_llms_unsorted]\n",
    "\n",
    "    m = len(present_llms)\n",
    "    if m == 0:\n",
    "        continue\n",
    "\n",
    "    # Offsets to center m boxes around the captain x position\n",
    "    offsets = (np.arange(m) - (m - 1) / 2.0) * box_width\n",
    "\n",
    "    for j, llm in enumerate(present_llms):\n",
    "        subset = df[(df[\"captain_type_display\"] == captain) & (df[\"llm\"] == llm)][\"f1_score\"].dropna()\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        pos = x_positions[i] + offsets[j]\n",
    "        color = llm_palette.get(llm, \"#808080\")\n",
    "\n",
    "        # Use matplotlib's boxplot to place each box at the computed numeric position\n",
    "        bp = ax.boxplot(subset.values,\n",
    "                        positions=[pos],\n",
    "                        widths=box_width * 0.9,\n",
    "                        patch_artist=True,\n",
    "                        manage_ticks=False)\n",
    "\n",
    "        # Style the box elements\n",
    "        for element in [\"boxes\", \"whiskers\", \"caps\", \"medians\"]:\n",
    "            plt.setp(bp[element], color=color)\n",
    "        for patch in bp[\"boxes\"]:\n",
    "            patch.set(facecolor=color, alpha=0.6)\n",
    "\n",
    "        # Make fliers (outliers) less visually distinctive: smaller, lower-alpha, and same color as box\n",
    "        if \"fliers\" in bp:\n",
    "            for f in bp[\"fliers\"]:\n",
    "                f.set(marker='o', markersize=3, markerfacecolor=color, markeredgecolor=color, alpha=0.35, markeredgewidth=0)\n",
    "\n",
    "# Create legend handles for llm types present in the full DataFrame, in llm_display order\n",
    "from matplotlib.patches import Patch\n",
    "all_present_llms = [llm for llm in llm_order if llm in df[\"llm\"].unique()]\n",
    "legend_handles = [Patch(facecolor=llm_palette[k], label=k, alpha=0.6) for k in all_present_llms]\n",
    "\n",
    "# ax.legend(handles=legend_handles, loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "# Final formatting\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(captain_categories, rotation=90)\n",
    "ax.set_xlabel(\"Captain Type\")\n",
    "ax.set_ylabel(\"Targeting Score (F1)\")\n",
    "ax.set_xlim(-0.5, len(captain_categories) - 0.5)\n",
    "\n",
    "sns.despine()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "#plt.savefig(os.path.join(PATH_EXPORT, \"captain_f1_boxplot.pdf\"), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd10a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sns.stripplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    alpha=0.7,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Targeting Score (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    # alpha=0.7,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Targeting Score (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d45249",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    data=df,\n",
    "    kind=\"ecdf\",\n",
    "    x=\"f1_score\",\n",
    "    hue=\"captain_type_display\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3115c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"move_count\"] = df[\"hits\"] + df[\"misses\"]\n",
    "\n",
    "sns.barplot(data=df, x=\"captain_type\", y=\"move_count\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da997ec9",
   "metadata": {},
   "source": [
    "## Win rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f463cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build competitor column (LLM first | Captain)\n",
    "df = build_competitor_column(df, llm_col=\"llm_display\", captain_col=\"captain_type_display\", out_col=\"competitor\")\n",
    "\n",
    "# Compute win rates for F1 (higher better)\n",
    "win_results_f1_comp = compute_pairwise_win_rates(\n",
    "    df, metric=\"f1_score\", higher_is_better=True, competitor_col=\"competitor\", board_col=\"board_id\"\n",
    ")\n",
    "print(\"Mean board win rate matrix (F1, competitor-level):\")\n",
    "display(win_results_f1_comp[\"mean_board_win_rate_matrix\"])\n",
    "# print(\"Weighted all-pairs win rate matrix (F1, competitor-level):\")\n",
    "# display(win_results_f1_comp[\"weighted_win_rate_matrix\"])\n",
    "\n",
    "# # Compute win rates for move count (lower better) if available\n",
    "# if \"move_count\" in df.columns:\n",
    "#     win_results_moves_comp = compute_pairwise_win_rates(\n",
    "#         df, metric=\"move_count\", higher_is_better=False, competitor_col=\"competitor\", board_col=\"board_id\"\n",
    "#     )\n",
    "#     print(\"Mean board win rate matrix (Move Count, competitor-level):\")\n",
    "#     display(win_results_moves_comp[\"mean_board_win_rate_matrix\"])\n",
    "#     print(\"Weighted all-pairs win rate matrix (Move Count, competitor-level):\")\n",
    "#     display(win_results_moves_comp[\"weighted_win_rate_matrix\"])\n",
    "# else:\n",
    "#     print(\"Column 'move_count' not found; skip move-count win rates.\")\n",
    "\n",
    "# Aggregate summary for F1\n",
    "# f1_comp_summary = win_results_f1_comp[\"aggregate\"][[\n",
    "#     \"competitor_a\", \"competitor_b\", \"mean_board_win_rate\", \"weighted_all_pairs_win_rate\", \"boards_considered\", \"board_wins\", \"board_comparisons\"\n",
    "# ]].sort_values([\"competitor_a\", \"competitor_b\"]).reset_index(drop=True)\n",
    "# print(\"Pairwise aggregate win rates (F1, competitor-level):\")\n",
    "# display(f1_comp_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082484a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped heatmap using refactored helper\n",
    "base_matrix = win_results_f1_comp[\"weighted_win_rate_matrix\"].copy().astype(float)\n",
    "\n",
    "fig, ax = plot_grouped_winrate_heatmap(\n",
    "    base_matrix,\n",
    "    llm_palette=llm_palette,\n",
    "    cmap=\"cividis\",\n",
    "    annotate=True,\n",
    "    captain_tick_fontsize=6,\n",
    "    row_alpha=1.0,\n",
    "    col_alpha=1.0,\n",
    "    show_group_separators=True,\n",
    "    separator_width=4,\n",
    "    shade_rows=True,\n",
    "    shade_cols=True,\n",
    "    group_label_rotation=90,\n",
    "    group_label_fontsize=10,\n",
    "    output_path=os.path.join(PATH_EXPORT, \"f1_winrate_heatmap.pdf\"),\n",
    "    title=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20f64d",
   "metadata": {},
   "source": [
    "## EIG Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Theoretical Maximum EIG (shared) -------------------------------------------\n",
    "# Defines a reusable constant THEORETICAL_MAX_EIG based on epsilon.\n",
    "# Other cells (ECDF delta, running max curves) will reference this instead of recomputing.\n",
    "import numpy as np\n",
    "from battleship.agents import binary_entropy\n",
    "\n",
    "EIG_EPSILON = 0.1  # noise / error probability parameter\n",
    "THEORETICAL_MAX_EIG = binary_entropy(p=0.5) - binary_entropy(EIG_EPSILON)\n",
    "print(f\"Set THEORETICAL_MAX_EIG={THEORETICAL_MAX_EIG:.4f} for epsilon={EIG_EPSILON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model EIG Extraction (All Runs) -----------------------------------------\n",
    "# Build EIG table directly from model_df metadata (no filename round parsing).\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Guard: ensure model_df exists\n",
    "if 'model_df' not in globals() or model_df.empty:\n",
    "    model_question_df = pd.DataFrame()\n",
    "else:\n",
    "    # We expect columns: llm, run_dir, round_id, captain_type\n",
    "    required = {\"llm\", \"run_dir\", \"round_id\", \"captain_type\"}\n",
    "    missing = required - set(model_df.columns)\n",
    "    if missing:\n",
    "        print(f\"Missing columns in model_df: {missing}; cannot extract EIG.\")\n",
    "        model_question_df = pd.DataFrame()\n",
    "    else:\n",
    "        records = []\n",
    "        # Iterate unique (llm, run_dir, round_id, captain_type)\n",
    "        for _, row in model_df.iterrows():\n",
    "            captain_json = Path(row.run_dir) / \"rounds\" / f\"round_{row.round_id}\" / \"captain\" / \"captain.json\"\n",
    "            if not captain_json.exists():\n",
    "                continue\n",
    "            try:\n",
    "                with captain_json.open() as f:\n",
    "                    entries = json.load(f)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            def extract_stage_index(entry):\n",
    "                stage_index = entry.get(\"stage_index\")\n",
    "                if not stage_index:\n",
    "                    stage_index = 0\n",
    "                return stage_index\n",
    "\n",
    "            # compute max stage and check monotonicity of stage_index across entries\n",
    "            stage_indices = [extract_stage_index(e) for e in entries]\n",
    "            max_stage_index = max(stage_indices) if stage_indices else 0\n",
    "\n",
    "            # Data-integrity check: stage_index should never decrease\n",
    "            decreases = []\n",
    "            for i in range(1, len(stage_indices)):\n",
    "                if stage_indices[i] < stage_indices[i - 1]:\n",
    "                    decreases.append((i - 1, i, stage_indices[i - 1], stage_indices[i]))\n",
    "\n",
    "            if decreases:\n",
    "                print(\n",
    "                    \"WARNING: Stage index decreased for run:\",\n",
    "                    f\"captain_type={row.captain_type}, llm={row.llm}, board_id={row.board_id}, seed={row.seed}\"\n",
    "                )\n",
    "                for prev_i, cur_i, prev_s, cur_s in decreases:\n",
    "                    print(f\"  entries[{prev_i}].stage_index={prev_s} -> entries[{cur_i}].stage_index={cur_s}\")\n",
    "\n",
    "            for idx, entry in enumerate(entries):\n",
    "                stage_index = extract_stage_index(entry)\n",
    "                stage_completion = stage_index / max_stage_index if max_stage_index > 0 else 0.0\n",
    "                eig_value = entry.get(\"eig\")\n",
    "                q_block = entry.get(\"question\")\n",
    "                if eig_value is None or q_block is None:\n",
    "                    continue\n",
    "                # Nested question text extraction\n",
    "                if isinstance(q_block, dict):\n",
    "                    inner_q = q_block.get(\"question\")\n",
    "                    q_text = inner_q.get(\"text\") if isinstance(inner_q, dict) else None\n",
    "                else:\n",
    "                    q_text = None\n",
    "                q_text = q_text or \"No question text\"\n",
    "\n",
    "                raw_candidates = entry.get(\"eig_questions\") or []\n",
    "                processed = None\n",
    "                if raw_candidates:\n",
    "                    processed = [\n",
    "                        (\n",
    "                            c.get(\"question\", {})\n",
    "                            .get(\"question\", {})\n",
    "                            .get(\"text\"),\n",
    "                            c.get(\"eig\"),\n",
    "                            None,\n",
    "                        )\n",
    "                        for c in raw_candidates\n",
    "                    ]\n",
    "                    vals = [c[1] for c in processed if c[1] is not None]\n",
    "                    if vals:\n",
    "                        mx = max(vals)\n",
    "                        processed = [(qt, ev, ev == mx) for (qt, ev, _) in processed]\n",
    "\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"captain_type\": row.captain_type,\n",
    "                        \"llm\": row.llm,\n",
    "                        \"board_id\": row.board_id,\n",
    "                        \"seed\": row.seed,\n",
    "                        \"stage_index\": stage_index,\n",
    "                        \"stage_completion\": stage_completion,\n",
    "                        \"run_dir\": row.run_dir,\n",
    "                        \"round_id\": row.round_id,\n",
    "                        \"index\": idx,\n",
    "                        \"question\": q_text,\n",
    "                        \"eig\": eig_value,\n",
    "                        \"eig_candidates\": processed,\n",
    "                    }\n",
    "                )\n",
    "        model_question_df = pd.DataFrame(records)\n",
    "        if not model_question_df.empty:\n",
    "            model_question_df = model_question_df.sort_values([\"llm\", \"round_id\", \"index\"]).reset_index(drop=True)\n",
    "\n",
    "# Add captain_type_display similar to main df\n",
    "if not model_question_df.empty and 'CAPTAIN_TYPE_LABELS' in globals():\n",
    "    model_question_df['captain_type_display'] = model_question_df['captain_type'].map(CAPTAIN_TYPE_LABELS)\n",
    "    # Preserve order used elsewhere\n",
    "    cat_order = [x for x in dict.fromkeys(CAPTAIN_TYPE_LABELS.values()) if x in model_question_df['captain_type_display'].unique()]\n",
    "    model_question_df['captain_type_display'] = pd.Categorical(model_question_df['captain_type_display'], categories=cat_order, ordered=True)\n",
    "\n",
    "# Derive llm_display here (moved earlier as requested)\n",
    "if not model_question_df.empty and 'llm_display' not in model_question_df.columns:\n",
    "    if 'df' in globals() and 'llm_display' in df.columns:\n",
    "        llm_display_map = (\n",
    "            df.dropna(subset=['llm_display'])\n",
    "              .drop_duplicates('llm')\n",
    "              [['llm','llm_display']]\n",
    "              .set_index('llm')['llm_display']\n",
    "              .to_dict()\n",
    "        )\n",
    "        model_question_df['llm_display'] = model_question_df['llm'].map(llm_display_map).fillna(model_question_df['llm'])\n",
    "    else:\n",
    "        model_question_df['llm_display'] = model_question_df['llm']\n",
    "\n",
    "# Align categorical ordering with main df if available\n",
    "if not model_question_df.empty and 'df' in globals() and 'llm_display' in df.columns and hasattr(df['llm_display'], 'cat'):\n",
    "    model_question_df['llm_display'] = pd.Categorical(\n",
    "        model_question_df['llm_display'],\n",
    "        categories=list(df['llm_display'].cat.categories),\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "model_question_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-Captain EIG Distribution --------------------------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "if 'model_question_df' not in globals() or model_question_df.empty:\n",
    "    print(\"model_eig_df is empty; run the EIG extraction cell first.\")\n",
    "else:\n",
    "    x_col = 'captain_type_display' if 'captain_type_display' in model_question_df.columns else 'captain_type'\n",
    "\n",
    "    plt.figure(figsize=(8, 4.5))\n",
    "    ax = sns.boxplot(\n",
    "        data=model_question_df,\n",
    "        x=x_col,\n",
    "        y='eig',\n",
    "        hue='llm',\n",
    "        palette=llm_palette,\n",
    "        showfliers=False,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('Captain Type')\n",
    "    ax.set_ylabel('EIG')\n",
    "    ax.set_title('Per-Captain EIG Distribution (Models)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    ax.legend(title='LLM', bbox_to_anchor=(1, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64679a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_question_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.displot(\n",
    "    data=model_question_df,\n",
    "    kind=\"ecdf\",\n",
    "    col=\"captain_type_display\",\n",
    "    x=\"eig\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    complementary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ECDF Delta (LLM vs EIG captain types) -----------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "_ecdf_categories = (\"LLM\", \"EIG\")  # (baseline, enhanced)\n",
    "complementary = True  # match earlier visualization style (survival curves)\n",
    "alpha_fill = 0.18\n",
    "linewidth = 2.0\n",
    "\n",
    "if 'model_question_df' not in globals() or model_question_df.empty:\n",
    "    print(\"model_question_df empty; run extraction first.\")\n",
    "else:\n",
    "    # Ensure required captain types exist\n",
    "    present_cats = set(model_question_df['captain_type_display'].dropna().unique())\n",
    "    missing = [c for c in _ecdf_categories if c not in present_cats]\n",
    "    if missing:\n",
    "        print(f\"Missing categories for delta plot: {missing}\")\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(7.5, 5))\n",
    "\n",
    "        legend_elements = []\n",
    "        area_rows = []\n",
    "\n",
    "        for llm in [c for c in llm_palette.keys() if c in model_question_df['llm'].unique()]:\n",
    "            sub = model_question_df[model_question_df['llm'] == llm]\n",
    "            # Need both categories for this llm\n",
    "            if not set(_ecdf_categories).issubset(set(sub['captain_type_display'].unique())):\n",
    "                continue\n",
    "            data_a = sub[sub['captain_type_display'] == _ecdf_categories[0]]['eig'].dropna().values\n",
    "            data_b = sub[sub['captain_type_display'] == _ecdf_categories[1]]['eig'].dropna().values\n",
    "            if len(data_a) == 0 or len(data_b) == 0:\n",
    "                continue\n",
    "\n",
    "            # Build common grid\n",
    "            grid = np.unique(np.concatenate([data_a, data_b]))\n",
    "            n_a = len(data_a)\n",
    "            n_b = len(data_b)\n",
    "            # ECDF values using searchsorted (right) for F(x) = P(X <= x)\n",
    "            y_a = np.searchsorted(np.sort(data_a), grid, side='right') / n_a\n",
    "            y_b = np.searchsorted(np.sort(data_b), grid, side='right') / n_b\n",
    "            if complementary:\n",
    "                y_a = 1 - y_a\n",
    "                y_b = 1 - y_b\n",
    "\n",
    "            color = llm_palette.get(llm, '#444444')\n",
    "            # Plot baseline (LLM) dashed, enhanced (EIG) solid\n",
    "            ax.plot(grid, y_a, linestyle='--', color=color, linewidth=linewidth, alpha=0.9)\n",
    "            ax.plot(grid, y_b, linestyle='-', color=color, linewidth=linewidth, alpha=0.9)\n",
    "\n",
    "            # Shade region between curves\n",
    "            ax.fill_between(grid, y_a, y_b, color=color, alpha=alpha_fill, linewidth=0)\n",
    "\n",
    "            # Approximate absolute area difference (integral of |delta|) for reference\n",
    "            area_diff = np.trapz(np.abs(y_b - y_a), grid)\n",
    "            area_rows.append({'llm': llm, 'area_abs_diff': area_diff})\n",
    "\n",
    "        # Use shared THEORETICAL_MAX_EIG\n",
    "        if 'THEORETICAL_MAX_EIG' in globals():\n",
    "            ax.axvline(THEORETICAL_MAX_EIG, color='k', linestyle=':', linewidth=1.5)\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "            ax.text(\n",
    "                THEORETICAL_MAX_EIG * 1.01,\n",
    "                ymax * 0.97,\n",
    "                f\"Theoretical Max EIG (ε={EIG_EPSILON})≈{THEORETICAL_MAX_EIG:.3f}\",\n",
    "                rotation=90,\n",
    "                va='top',\n",
    "                ha='left',\n",
    "                fontsize=9,\n",
    "                color='k'\n",
    "            )\n",
    "\n",
    "        ax.set_xlim(left=0)\n",
    "        ax.set_xlabel('EIG')\n",
    "        ax.set_ylabel('Proportion (CDF)' if not complementary else 'Proportion (1 - CDF)')\n",
    "        ax.set_title(r\"$\\Delta$EIG w/r/t Base LLM\")\n",
    "        ax.grid(alpha=0.3, linestyle=\"-\")\n",
    "\n",
    "        # Simplified legend: only distinguish line styles (color meaning shown elsewhere)\n",
    "        from matplotlib.lines import Line2D\n",
    "        style_handles = [\n",
    "            Line2D([0,1],[0,1], color='k', linestyle='--', linewidth=linewidth, label=_ecdf_categories[0]),\n",
    "            Line2D([0,1],[0,1], color='k', linestyle='-', linewidth=linewidth, label=_ecdf_categories[1]),\n",
    "        ]\n",
    "        ax.legend(handles=style_handles, loc='lower left', frameon=True, title='Captain Type')\n",
    "\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(\n",
    "            os.path.join(PATH_EXPORT, \"eig_ecdf_delta.pdf\"),\n",
    "            bbox_inches='tight',\n",
    "            dpi=300,\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        if area_rows:\n",
    "            area_df = pd.DataFrame(area_rows).sort_values('area_abs_diff', ascending=False)\n",
    "            display(area_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e70a4",
   "metadata": {},
   "source": [
    "### Growth of max EIG with number of candidate questions\n",
    "We fix the captain type to `EIG` and, for every model/round row in `model_eig_df`, compute the running maximum EIG obtainable when only the first k candidate questions are considered (k = 1..N). We then average these curves across rows for each LLM type and plot one line per LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running max EIG vs number of candidate questions per LLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configurable captain type display to analyze\n",
    "CAPTAIN_TYPE_DISPLAY = \"EIG\"  # change this to any value present in model_eig_df['captain_type_display']\n",
    "\n",
    "required_cols = {\"llm_display\", \"captain_type_display\", \"eig_candidates\"}\n",
    "missing = required_cols - set(model_question_df.columns)\n",
    "assert not missing, f\"model_eig_df missing required columns: {missing}\"  # Fail fast\n",
    "\n",
    "subset = model_question_df[model_question_df[\"captain_type_display\"] == CAPTAIN_TYPE_DISPLAY].copy()\n",
    "if subset.empty:\n",
    "    print(f\"No rows with captain_type_display == '{CAPTAIN_TYPE_DISPLAY}'.\")\n",
    "else:\n",
    "    preview = None\n",
    "    for v in subset[\"eig_candidates\"]:\n",
    "        if isinstance(v, list) and v:\n",
    "            preview = v[:3]\n",
    "            break\n",
    "    print(\"Preview eig_candidates (first 3 of first non-empty row):\", preview)\n",
    "\n",
    "    def extract_eigs(candidates):\n",
    "        if not isinstance(candidates, list):\n",
    "            return []\n",
    "        vals = []\n",
    "        for item in candidates:\n",
    "            if isinstance(item, (list, tuple)) and len(item) >= 2:\n",
    "                val = item[1]\n",
    "                if val is not None:\n",
    "                    try:\n",
    "                        vals.append(float(val))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            elif isinstance(item, dict):\n",
    "                for k in [\"eig\", \"EIG\", \"value\"]:\n",
    "                    if k in item and item[k] is not None:\n",
    "                        try:\n",
    "                            vals.append(float(item[k]))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                        break\n",
    "        return vals\n",
    "\n",
    "    records = []\n",
    "    for _, row in subset.iterrows():\n",
    "        eigs = extract_eigs(row[\"eig_candidates\"])\n",
    "        if not eigs:\n",
    "            continue\n",
    "        running_max = np.maximum.accumulate(eigs)\n",
    "        for k, val in enumerate(running_max, start=1):\n",
    "            records.append({\n",
    "                \"llm_display\": row[\"llm_display\"],\n",
    "                \"k\": k,\n",
    "                \"running_max_eig\": val,\n",
    "            })\n",
    "\n",
    "    curve_df = pd.DataFrame(records)\n",
    "    if curve_df.empty:\n",
    "        print(\"No candidate EIG values extracted.\")\n",
    "    else:\n",
    "        agg = (\n",
    "            curve_df.groupby([\"llm_display\", \"k\"])\n",
    "            [\"running_max_eig\"].agg([\"mean\", \"count\", \"std\"]).reset_index()\n",
    "        )\n",
    "        agg[\"se\"] = agg[\"std\"] / np.sqrt(agg[\"count\"]).replace(0, np.nan)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(7, 4.5))\n",
    "        palette = llm_palette if 'llm_palette' in globals() else None\n",
    "        for llm_name, g in agg.groupby(\"llm_display\"):\n",
    "            g_sorted = g.sort_values(\"k\")\n",
    "            color = palette.get(llm_name, None) if palette else None\n",
    "            ax.plot(g_sorted[\"k\"], g_sorted[\"mean\"], label=llm_name, color=color)\n",
    "            if g_sorted[\"se\"].notna().any():\n",
    "                ax.fill_between(\n",
    "                    g_sorted[\"k\"],\n",
    "                    g_sorted[\"mean\"] - g_sorted[\"se\"],\n",
    "                    g_sorted[\"mean\"] + g_sorted[\"se\"],\n",
    "                    alpha=0.18,\n",
    "                    color=color,\n",
    "                )\n",
    "        # Add theoretical max EIG horizontal line if available\n",
    "        if 'THEORETICAL_MAX_EIG' in globals():\n",
    "            ax.axhline(THEORETICAL_MAX_EIG, color='k', linestyle=':', linewidth=1.25, label=f\"Theoretical max (ε={EIG_EPSILON})\")\n",
    "            xmin, xmax = ax.get_xlim()\n",
    "            x_center = 0.5 * (xmin + xmax)\n",
    "            ax.text(\n",
    "                x_center,\n",
    "                THEORETICAL_MAX_EIG * 0.98,\n",
    "                f\"Theoretical Max EIG (ε={EIG_EPSILON})≈{THEORETICAL_MAX_EIG:.3f}\",\n",
    "                va=\"top\",\n",
    "                ha=\"center\",\n",
    "                fontsize=9,\n",
    "                color=\"k\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.6, edgecolor=\"none\"),\n",
    "            )\n",
    "\n",
    "        ax.set_xlabel(\"Number of candidate questions considered (k)\")\n",
    "        ax.set_ylabel(\"Max EIG\")\n",
    "        # ax.set_title(f\"Growth of max EIG with candidate set size (captain = {CAPTAIN_TYPE_DISPLAY})\")\n",
    "        # ax.legend(title=\"LLM\")\n",
    "        ax.grid(alpha=0.3, linestyle=\"-\")\n",
    "\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.savefig(\n",
    "            os.path.join(PATH_EXPORT, \"eig_max_vs_k.pdf\"),\n",
    "            bbox_inches='tight',\n",
    "            dpi=300\n",
    "        )\n",
    "\n",
    "        plt.show()\n",
    "        display(agg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3d9d9",
   "metadata": {},
   "source": [
    "### Human question EIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Human Question EIG Calculation ------------------------------------------\n",
    "# Computes EIG for human questions and caches to human_eig_df.csv (≈1–2 min first run).\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_JSON_PATH = resolve_project_path(\n",
    "    \"experiments/collaborative/spotter_benchmarks/o4-mini_CodeSpotterModel_True.json\"\n",
    ")\n",
    "CACHE_PATH = Path(\"human_eig_df.csv\")\n",
    "\n",
    "\n",
    "# def load_human_interactions(json_path: Path) -> list[dict]:\n",
    "#     with json_path.open() as f:\n",
    "#         return json.load(f)\n",
    "\n",
    "\n",
    "# def build_human_df(entries: list[dict]) -> pd.DataFrame:\n",
    "#     rows = []\n",
    "#     for e in entries:\n",
    "#         if not (\"question\" in e and \"occTiles\" in e):\n",
    "#             continue\n",
    "#         answer = e.get(\"answer\", \"\").lower()\n",
    "#         # Normalize boolean text answers.\n",
    "#         if answer == \"true\":\n",
    "#             answer = \"yes\"\n",
    "#         elif answer == \"false\":\n",
    "#             answer = \"no\"\n",
    "#         true_answer = e.get(\"true_answer\")\n",
    "#         rows.append(\n",
    "#             {\n",
    "#                 \"question\": e.get(\"question\"),\n",
    "#                 \"program\": e.get(\"program\"),\n",
    "#                 \"board_state\": e.get(\"occTiles\"),\n",
    "#                 \"answer\": answer,\n",
    "#                 \"true_answer\": true_answer,\n",
    "#                 \"correct\": answer == true_answer,\n",
    "#             }\n",
    "#         )\n",
    "#     return pd.DataFrame(rows)\n",
    "\n",
    "# RUN_ANALYSIS = False\n",
    "\n",
    "# if RUN_ANALYSIS:\n",
    "#     if CACHE_PATH.exists():\n",
    "#         human_eig_df = pd.read_csv(CACHE_PATH)\n",
    "#     else:\n",
    "#         raw_entries = load_human_interactions(Path(INPUT_JSON_PATH))\n",
    "#         human_eig_df = build_human_df(raw_entries)\n",
    "#         # Keep only correctly answered questions.\n",
    "#         human_eig_df = human_eig_df[human_eig_df[\"correct\"]]\n",
    "\n",
    "#         eig_calculator = EIGCalculator(samples=1000, timeout=15, epsilon=0)\n",
    "#         human_eig_df[\"calculated_eig\"] = None\n",
    "\n",
    "#         for idx, row in human_eig_df.iterrows():\n",
    "#             code_question = CodeQuestion(\n",
    "#                 question=Question(row[\"question\"]),\n",
    "#                 fn_text=row[\"program\"],\n",
    "#                 translation_prompt=\"\",\n",
    "#                 completion={},\n",
    "#             )\n",
    "#             board = Board.from_occ_tiles(row[\"board_state\"])  # reconstruct board\n",
    "#             human_eig_df.at[idx, \"calculated_eig\"] = eig_calculator(code_question, board)\n",
    "\n",
    "#         human_eig_df.to_csv(CACHE_PATH, index=False)\n",
    "\n",
    "#     human_eig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_human_eig():\n",
    "    \"\"\"\n",
    "    Calculate EIG for human questions by:\n",
    "    - Loading questions from stage.csv where messageType = \"question\"\n",
    "    - Looking up previous questions in o4-mini_CodeSpotterModel_True.json to extract code questions\n",
    "    - Getting true board tiles from round.csv \n",
    "    - Computing EIG for each question using EIGCalculator with constraints\n",
    "    - Returning the EIG dataframe\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    import ast\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    from battleship.agents import EIGCalculator, CodeQuestion, Question\n",
    "    from battleship.board import Board\n",
    "\n",
    "    # Load stage data for questions\n",
    "    stage_path = Path(PATH_DATA) / \"stage.csv\"\n",
    "    if not stage_path.exists():\n",
    "        print(f\"Error: {stage_path} not found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    stage_df = pd.read_csv(stage_path)\n",
    "    print(f\"Loaded {len(stage_df)} stage entries\")\n",
    "\n",
    "    # Load round data for true board tiles\n",
    "    rounds_path = Path(PATH_DATA) / \"round.csv\"\n",
    "    if not rounds_path.exists():\n",
    "        print(f\"Error: {rounds_path} not found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rounds_df = pd.read_csv(rounds_path)\n",
    "    print(f\"Loaded {len(rounds_df)} rounds\")\n",
    "\n",
    "    # Load spotter data for code questions\n",
    "    spotter_json_path = resolve_project_path(\n",
    "        \"experiments/collaborative/spotter_benchmarks/o4-mini_CodeSpotterModel_True.json\"\n",
    "    )\n",
    "\n",
    "    if not Path(spotter_json_path).exists():\n",
    "        print(f\"Error: {spotter_json_path} not found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    with open(spotter_json_path, 'r') as f:\n",
    "        spotter_data = json.load(f)\n",
    "\n",
    "    print(f\"Loaded {len(spotter_data)} spotter entries\")\n",
    "\n",
    "    # Create lookup dictionary for code questions using composite key\n",
    "    code_question_lookup = {}\n",
    "    \n",
    "    for entry in spotter_data:\n",
    "        if \"question\" in entry and \"program\" in entry:\n",
    "            key = (entry[\"question\"], entry[\"roundID\"], entry[\"questionID\"])\n",
    "            code_question_lookup[key] = {\n",
    "                \"program\": entry[\"program\"],\n",
    "                \"board_state\": entry[\"occTiles\"],\n",
    "                \"answer\": entry[\"answer\"],\n",
    "                \"true_answer\": entry[\"true_answer\"]\n",
    "            }\n",
    "\n",
    "    print(f\"Created lookup for {len(code_question_lookup)} code questions\")\n",
    "\n",
    "    # Create lookup for true board tiles by round ID\n",
    "    round_boards_lookup = {}\n",
    "    for _, round_row in rounds_df.iterrows():\n",
    "        round_id = round_row[\"id\"]\n",
    "        true_tiles_str = round_row[\"trueTiles\"]\n",
    "        true_tiles = ast.literal_eval(true_tiles_str)\n",
    "        round_boards_lookup[round_id] = np.array(true_tiles)\n",
    "\n",
    "    # Filter stages to only questions\n",
    "    question_stages = stage_df[stage_df[\"messageType\"] == \"question\"].copy()\n",
    "    print(f\"Found {len(question_stages)} question stages\")\n",
    "\n",
    "    # Extract questions from stage data\n",
    "    eig_records = []\n",
    "    eig_calculator = EIGCalculator(samples=1000, timeout=15, epsilon=0.1)\n",
    "\n",
    "    question_count = 0\n",
    "    matched_questions = 0\n",
    "    lost_questions = 0\n",
    "\n",
    "    # Group question stages by roundID to process games\n",
    "    for round_idx, (round_id, round_stages) in enumerate(question_stages.groupby(\"roundID\")):\n",
    "        print(f\"Processing round {round_idx} ({round_id}) with {len(round_stages)} questions\")\n",
    "\n",
    "        # Get true board for this round\n",
    "        if round_id not in round_boards_lookup:\n",
    "            print(f\"Skipping round {round_id}: no true board data found\")\n",
    "            continue\n",
    "\n",
    "        true_board = Board(round_boards_lookup[round_id])\n",
    "\n",
    "        # Sort stages by index to maintain chronological order\n",
    "        round_stages = round_stages.sort_values(\"index\")\n",
    "\n",
    "        # Collect all questions and answers from this round to build constraints\n",
    "        game_questions = []\n",
    "\n",
    "        # First pass: collect all correctly answered questions as potential constraints\n",
    "        for _, stage in round_stages.iterrows():\n",
    "            question_text = stage.get(\"messageText\", \"\")\n",
    "            stage_round_id = stage.get(\"roundID\")\n",
    "            question_id = stage.get(\"questionID\")\n",
    "\n",
    "            lookup_key = (question_text, stage_round_id, question_id)\n",
    "            if lookup_key in code_question_lookup:\n",
    "                code_data = code_question_lookup[lookup_key]\n",
    "\n",
    "                # Only include correctly answered questions as potential constraints\n",
    "                if code_data[\"answer\"] == code_data[\"true_answer\"]:\n",
    "                    answer_value = None\n",
    "                    if code_data[\"true_answer\"].lower() == \"yes\":\n",
    "                        answer_value = True\n",
    "                    elif code_data[\"true_answer\"].lower() == \"no\":\n",
    "                        answer_value = False\n",
    "\n",
    "                    if answer_value is not None:\n",
    "                        try:\n",
    "                            constraint_code_question = CodeQuestion(\n",
    "                                question=Question(question_text),\n",
    "                                fn_text=code_data[\"program\"],\n",
    "                                translation_prompt=\"\",\n",
    "                                completion={}\n",
    "                            )\n",
    "                            game_questions.append((constraint_code_question, answer_value))\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error creating constraint CodeQuestion: {e}\")\n",
    "                            continue\n",
    "\n",
    "        # Second pass: process each question for EIG calculation\n",
    "        for _, stage in round_stages.iterrows():\n",
    "            question_count += 1\n",
    "            question_text = stage.get(\"messageText\", \"\")\n",
    "            stage_round_id = stage.get(\"roundID\")\n",
    "            question_id = stage.get(\"questionID\")\n",
    "\n",
    "            lookup_key = (question_text, stage_round_id, question_id)\n",
    "\n",
    "            if not question_text or lookup_key not in code_question_lookup:\n",
    "                print(f\"Skipping question with no spotter response: {question_text}\")\n",
    "                lost_questions += 1\n",
    "                continue\n",
    "\n",
    "            matched_questions += 1\n",
    "            code_data = code_question_lookup[lookup_key]\n",
    "\n",
    "        # try:\n",
    "            # Create partial board from occluded tiles at the time of this question\n",
    "            partial_board = Board.from_occ_tiles(code_data[\"board_state\"])\n",
    "\n",
    "            # Create ship tracker by comparing true board vs partial board\n",
    "            ship_tracker = true_board.ship_tracker(partial_board)\n",
    "\n",
    "            # Create code question for this EIG calculation\n",
    "            code_question = CodeQuestion(\n",
    "                question=Question(question_text),\n",
    "                fn_text=code_data[\"program\"],\n",
    "                translation_prompt=\"\",\n",
    "                completion={}\n",
    "            )\n",
    "\n",
    "            # Build constraints from all previous questions in this game\n",
    "            # Find the index of current question in game_questions list\n",
    "            current_question_idx = -1\n",
    "            for i, (cq, _) in enumerate(game_questions):\n",
    "                if cq.question.text == question_text:\n",
    "                    current_question_idx = i\n",
    "                    break\n",
    "\n",
    "            # Use all questions before this one as constraints\n",
    "            constraints = []\n",
    "            if current_question_idx > 0:\n",
    "                constraints = game_questions[:current_question_idx]\n",
    "\n",
    "            # Calculate EIG with ship tracker and constraints\n",
    "            eig_value = eig_calculator(code_question, partial_board,\n",
    "                                    ship_tracker=ship_tracker,\n",
    "                                    constraints=constraints)\n",
    "            eig_records.append({\n",
    "                \"round_id\": round_id,\n",
    "                \"question_id\": question_id,\n",
    "                \"stage_id\": stage[\"id\"],\n",
    "                \"stage_index\": stage.get(\"index\", 0),\n",
    "                \"question\": question_text,\n",
    "                \"program\": code_data[\"program\"],\n",
    "                \"eig\": eig_value,\n",
    "                \"captain_type\": \"Human\",\n",
    "                \"answer\": code_data[\"true_answer\"],\n",
    "                \"n_constraints\": len(constraints),\n",
    "                \"ship_tracker\": ship_tracker\n",
    "            })\n",
    "\n",
    "            # except Exception as e:\n",
    "            #     print(f\"Error processing question '{question_text}': {e}\")\n",
    "            #     continue\n",
    "\n",
    "    print(f\"Total questions found: {question_count}\")\n",
    "    print(f\"Matched questions: {matched_questions}\")\n",
    "\n",
    "    if not eig_records:\n",
    "        print(\"No EIG records generated\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame(eig_records)\n",
    "    print(f\"Generated {len(result_df)} EIG calculations with ship tracker and constraints\")\n",
    "    print(f\"Lost questions: {lost_questions}\")\n",
    "    print(f\"Matched questions: {matched_questions}\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "RUN_ANALYSIS = False\n",
    "\n",
    "if RUN_ANALYSIS:\n",
    "    # Execute the function\n",
    "    human_eig_df = calculate_human_eig()\n",
    "\n",
    "    human_eig_df[\"llm_display\"] = \"Human\"\n",
    "    human_eig_df[\"captain_type_display\"] = \"Human\"\n",
    "    human_eig_df.to_csv(CACHE_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare EIG Distributions (Model vs Human) by Captain Type --------------\n",
    "# Combine human and model EIG data for comparison\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the CSV file and load it into a DataFrame\n",
    "human_eig_df = pd.read_csv(resolve_project_path(\"experiments/collaborative/human_eig_df.csv\"))\n",
    "\n",
    "human_eig_data = human_eig_df[['eig', 'llm_display', 'captain_type_display']].copy()\n",
    "model_eig_data = model_question_df[['eig', 'llm_display', 'captain_type_display']].copy()\n",
    "\n",
    "# Combine datasets\n",
    "combined_eig_df = pd.concat([human_eig_data, model_eig_data], ignore_index=True)\n",
    "\n",
    "# Create comparison plot with captain type breakdown\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Boxplot by LLM type\n",
    "sns.boxplot(\n",
    "    data=combined_eig_df, \n",
    "    x=\"llm_display\", \n",
    "    y=\"eig\", \n",
    "    palette=llm_palette,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"EIG Distribution by Agent Type\")\n",
    "axes[0].set_xlabel(\"Agent Type\")\n",
    "axes[0].set_ylabel(\"Expected Information Gain (EIG)\")\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Boxplot by captain type, colored by LLM\n",
    "sns.boxplot(\n",
    "    data=combined_eig_df, \n",
    "    x=\"captain_type_display\", \n",
    "    y=\"eig\", \n",
    "    hue=\"llm_display\",\n",
    "    palette=llm_palette,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"EIG Distribution by Captain Type\")\n",
    "axes[1].set_xlabel(\"Captain Type\")\n",
    "axes[1].set_ylabel(\"Expected Information Gain (EIG)\")\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Add theoretical max EIG line to both plots\n",
    "if 'THEORETICAL_MAX_EIG' in globals():\n",
    "    for ax in axes:\n",
    "        ax.axhline(THEORETICAL_MAX_EIG, color='k', linestyle=':', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PATH_EXPORT, \"human_vs_model_eig_by_captain_type.pdf\"), dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15d3c9",
   "metadata": {},
   "source": [
    "## Question timing analysis\n",
    "When do humans and models ask questions during the game?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff34b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df_human = load_dataset(experiment_path=PATH_DATA, use_gold=True)\n",
    "\n",
    "stage_df_human[\"captain_type\"] = \"Human\"\n",
    "stage_df_human[\"captain_type_display\"] = \"Human\"\n",
    "stage_df_human[\"llm\"] = \"Human\"\n",
    "stage_df_human[\"llm_display\"] = \"Human\"\n",
    "\n",
    "stage_df_human = stage_df_human.rename(columns={\"roundID\": \"round_id\"})\n",
    "\n",
    "stage_df_human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686ad3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter on \"question\" rows\n",
    "stage_df_human_question = stage_df_human[stage_df_human[\"questionAsked\"] == True]\n",
    "stage_df_human_question[\"question\"] = stage_df_human_question[\"messageText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e26178",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df_question = pd.concat(\n",
    "    [stage_df_human_question, model_question_df], axis=\"rows\", ignore_index=True\n",
    ")\n",
    "stage_df_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100da8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_question_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb82946",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_question_df[(model_question_df.captain_type_display == \"LLM\") & (model_question_df.llm_display == \"llama-4-scout\")].groupby(\n",
    "    [\"captain_type_display\", \"llm_display\", \"board_id\", \"seed\"], observed=True\n",
    ")[\"question\"].count().to_frame().to_csv(\"llm-llama-4-scout.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6783cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_question_df[\n",
    "    (model_question_df.captain_type_display == \"LLM\")\n",
    "    & (model_question_df.llm_display == \"llama-4-scout\")\n",
    "    & (model_question_df.board_id == \"B10\")\n",
    "    & (model_question_df.seed == 1337)\n",
    "].round_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ead3781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with common normalization (absolute density comparable across LLMs)\n",
    "fig_norm, ax_norm = plot_question_timing(\n",
    "    stage_df=stage_df_question,\n",
    "    llm_palette=llm_palette,\n",
    "    captain_types=[\"Human\", \"LLM\"],\n",
    "    common_norm=True,\n",
    "    legend=False,\n",
    "    fig_width=6,\n",
    "    fig_height=4,\n",
    "    output_path=os.path.join(PATH_EXPORT, \"question_timing.pdf\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a444f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(\n",
    "    data=stage_df_question[\n",
    "        stage_df_question[\"captain_type_display\"].isin([\"Human\", \"LLM\"])\n",
    "    ].copy(),\n",
    "    x=\"stage_completion\",\n",
    "    y=\"captain_type_display\",\n",
    "    hue=\"llm_display\",\n",
    "    palette=llm_palette,\n",
    "    common_norm=False,\n",
    "    density_norm=\"area\",\n",
    "    legend=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687de30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    data=df,\n",
    "    # kind=\"ecdf\",\n",
    "    y=\"question_count\",\n",
    "    x=\"captain_type_display\",\n",
    "    hue=\"llm_display\",\n",
    "    palette=llm_palette,\n",
    "    # stat=\"proportion\",\n",
    "    # common_norm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75719a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(\n",
    "    data=df,\n",
    "    col=\"captain_type_display\",\n",
    "    x=\"question_count\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm_display\",\n",
    "    palette=llm_palette,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b2536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bedc158",
   "metadata": {},
   "source": [
    "## Question content analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bea17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189f1b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9e893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e68dacf7",
   "metadata": {},
   "source": [
    "### Question Embedding & Content Clustering\n",
    "We embed each unique question (human + model) using the OpenAI `text-embedding-3-small` model with robust on-disk caching to avoid recomputation. Then we explore two clustering / visualization approaches:\n",
    "\n",
    "1. t-SNE (after PCA) + k-means.\n",
    "2. UMAP + HDBSCAN (density based; can assign noise).\n",
    "\n",
    "Columns propagated:\n",
    "- `question`\n",
    "- `captain_type_display`\n",
    "- `llm_display`\n",
    "\n",
    "Caching:\n",
    "- Cache file `question_embeddings_cache.pkl` stores columns: `question`, `embedding` (list[float]), `model`.\n",
    "- Only new questions are sent to the API.\n",
    "- Safe failure if API key not present or request errors occur.\n",
    "\n",
    "Methodology notes:\n",
    "- Questions are stripped & deduplicated.\n",
    "- PCA(50) => t-SNE(2) for speed/stability; perplexity auto-tuned.\n",
    "- k-means k chosen heuristically (bounded) with silhouette printed.\n",
    "- UMAP(2) then HDBSCAN; noise labeled -1.\n",
    "- Representative exemplar questions per cluster selected by cosine similarity to cluster centroid.\n",
    "\n",
    "Outputs:\n",
    "- `question_embeddings_df`: per unique question embedding table.\n",
    "- `stage_df_question_emb`: original `stage_df_question` with embedding-derived projections & cluster labels.\n",
    "- Figures saved under `PATH_EXPORT`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5143975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding generation & caching\n",
    "import os, time, pickle, math, sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    _openai_available = True\n",
    "except Exception:\n",
    "    _openai_available = False\n",
    "\n",
    "EMBED_MODEL = \"text-embedding-3-small\"\n",
    "EMBED_CACHE_PATH = Path(\"question_embeddings_cache.pkl\")\n",
    "BATCH_SIZE = 500  # OpenAI recommends batching for efficiency\n",
    "SLEEP_BETWEEN_BATCHES = 0.4  # basic pacing; adjust if rate limited\n",
    "\n",
    "# Prepare unique questions\n",
    "assert 'stage_df_question' in globals(), \"stage_df_question missing. Run earlier cells.\"\n",
    "questions_series = stage_df_question['question'].astype(str).str.strip()\n",
    "unique_questions = pd.Series(sorted(set(q for q in questions_series if q)))\n",
    "\n",
    "# Load cache if exists\n",
    "if EMBED_CACHE_PATH.exists():\n",
    "    with EMBED_CACHE_PATH.open('rb') as f:\n",
    "        cache_df = pickle.load(f)\n",
    "    if not isinstance(cache_df, pd.DataFrame) or 'question' not in cache_df.columns or 'embedding' not in cache_df.columns:\n",
    "        print(\"Cache format invalid; rebuilding.\")\n",
    "        cache_df = pd.DataFrame(columns=['question','embedding','model'])\n",
    "else:\n",
    "    cache_df = pd.DataFrame(columns=['question','embedding','model'])\n",
    "\n",
    "cached_set = set(cache_df['question'])\n",
    "missing_questions = [q for q in unique_questions if q not in cached_set]\n",
    "print(f\"Total unique questions: {len(unique_questions)} | Cached: {len(cached_set)} | Missing: {len(missing_questions)}\")\n",
    "\n",
    "client = None\n",
    "if missing_questions and _openai_available:\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"OPENAI_API_KEY not set; skipping new embeddings.\")\n",
    "    else:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "\n",
    "new_rows = []\n",
    "if client and missing_questions:\n",
    "    for i in range(0, len(missing_questions), BATCH_SIZE):\n",
    "        batch = missing_questions[i:i+BATCH_SIZE]\n",
    "        try:\n",
    "            resp = client.embeddings.create(input=batch, model=EMBED_MODEL)\n",
    "            # The response order matches the input order\n",
    "            for q, d in zip(batch, resp.data):\n",
    "                new_rows.append({\n",
    "                    'question': q,\n",
    "                    'embedding': d.embedding,\n",
    "                    'model': EMBED_MODEL,\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Embedding batch failed at index {i}: {e}\")\n",
    "            # Simple fallback: skip remaining\n",
    "            break\n",
    "        time.sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "if new_rows:\n",
    "    cache_df = pd.concat([cache_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    # Drop duplicates favoring first occurrence (oldest) to avoid churn\n",
    "    cache_df = cache_df.sort_values('question').drop_duplicates('question', keep='first')\n",
    "    with EMBED_CACHE_PATH.open('wb') as f:\n",
    "        pickle.dump(cache_df, f)\n",
    "    print(f\"Cached embeddings updated with {len(new_rows)} new rows.\")\n",
    "\n",
    "# Final embedding dataframe\n",
    "question_embeddings_df = cache_df.copy()\n",
    "print(question_embeddings_df.head())\n",
    "print(f\"Embedding dim (first row): {len(question_embeddings_df.iloc[0].embedding) if not question_embeddings_df.empty else 'NA'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBSET = ['Human', 'LLM']\n",
    "SUBSET = ['Human']\n",
    "SUBSET_LABEL = \"_\".join(SUBSET).lower()\n",
    "\n",
    "# Join embeddings back to full question rows (many rows may share a question string)\n",
    "assert 'stage_df_question' in globals(), \"stage_df_question missing\"\n",
    "\n",
    "emb_map = question_embeddings_df.set_index('question')['embedding'].to_dict()\n",
    "\n",
    "stage_df_question_emb = stage_df_question[stage_df_question['captain_type_display'].isin(SUBSET)].copy()\n",
    "\n",
    "stage_df_question_emb['embedding'] = stage_df_question_emb['question'].map(emb_map)\n",
    "missing_emb = stage_df_question_emb['embedding'].isna().sum()\n",
    "print(f\"Rows without embedding (likely due to API skip or empty text): {missing_emb}\")\n",
    "\n",
    "# Drop rows without embeddings for downstream projections\n",
    "stage_df_question_emb = stage_df_question_emb.dropna(subset=['embedding']).reset_index(drop=True)\n",
    "\n",
    "stage_df_question_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bb2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE + k-means: Dimensionality Reduction & Clustering (compute only)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Flags / config\n",
    "PCA_COMPONENTS = 50\n",
    "TSNE_PERPLEXITY_HEURISTIC_DIVISOR = 150\n",
    "TSNE_MAX_PERPLEXITY = 40\n",
    "TSNE_MIN_PERPLEXITY = 5\n",
    "# K_RANGE = range(2, 30)\n",
    "K_RANGE = [10]\n",
    "RANDOM_STATE = 42\n",
    "FORCE_RECOMPUTE_REDUCTION = False   # Force BOTH PCA and t-SNE\n",
    "FORCE_RECOMPUTE_KMEANS = True      # Force k-means reselection / refit\n",
    "\n",
    "# Build embedding matrix (will raise if missing)\n",
    "X = np.vstack(stage_df_question_emb['embedding'].tolist())\n",
    "emb_dim = X.shape[1]\n",
    "print(f\"Embedding matrix shape: {X.shape}\")\n",
    "\n",
    "# PCA (recompute if forced or cache absent / size mismatch)\n",
    "recompute_pca = (\n",
    "    FORCE_RECOMPUTE_REDUCTION or\n",
    "    ('X_pca_cached' not in globals()) or\n",
    "    (globals().get('X_pca_cached').shape[0] != X.shape[0])\n",
    ")\n",
    "if recompute_pca:\n",
    "    pca_dim = min(PCA_COMPONENTS, emb_dim)\n",
    "    pca_model = PCA(n_components=pca_dim, random_state=RANDOM_STATE)\n",
    "    X_pca = pca_model.fit_transform(X)\n",
    "    X_pca_cached = X_pca\n",
    "    pca_model_cached = pca_model\n",
    "    print(f\"Computed PCA: shape={X_pca.shape}\")\n",
    "    print(\"Explained variance (first 10 comps):\", pca_model.explained_variance_ratio_[:10].round(4))\n",
    "else:\n",
    "    X_pca = X_pca_cached\n",
    "    pca_model = pca_model_cached\n",
    "    print(\"Reusing cached PCA result.\")\n",
    "\n",
    "# t-SNE (recompute if forced or coordinates missing)\n",
    "recompute_tsne = FORCE_RECOMPUTE_REDUCTION or ('tsne_x' not in stage_df_question_emb.columns) or ('tsne_y' not in stage_df_question_emb.columns)\n",
    "if recompute_tsne:\n",
    "    n_samples = X_pca.shape[0]\n",
    "    perplexity = min(TSNE_MAX_PERPLEXITY, max(TSNE_MIN_PERPLEXITY, n_samples // TSNE_PERPLEXITY_HEURISTIC_DIVISOR))\n",
    "    perplexity = max(2, min(perplexity, n_samples - 1))\n",
    "    print(f\"Computing t-SNE with perplexity={perplexity}\")\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, learning_rate='auto', init='pca', random_state=RANDOM_STATE)\n",
    "    X_tsne = tsne.fit_transform(X_pca)\n",
    "    stage_df_question_emb['tsne_x'] = X_tsne[:, 0]\n",
    "    stage_df_question_emb['tsne_y'] = X_tsne[:, 1]\n",
    "else:\n",
    "    print(\"Reusing existing t-SNE coordinates (set FORCE_RECOMPUTE_REDUCTION=True to recompute).\")\n",
    "\n",
    "# k-means model selection (recompute if forced or labels absent)\n",
    "recompute_kmeans = FORCE_RECOMPUTE_KMEANS or ('kmeans_label' not in stage_df_question_emb.columns)\n",
    "if recompute_kmeans:\n",
    "    n_samples = X_pca.shape[0]\n",
    "    possible_k = [k for k in K_RANGE if k < n_samples]\n",
    "    best_k = None\n",
    "    best_score = -1\n",
    "    for k in possible_k:\n",
    "        km = KMeans(n_clusters=k, n_init='auto', random_state=RANDOM_STATE)\n",
    "        labels = km.fit_predict(X_pca)\n",
    "        if len(set(labels)) > 1:\n",
    "            try:\n",
    "                score = silhouette_score(X_pca, labels)\n",
    "            except Exception:\n",
    "                score = np.nan\n",
    "        else:\n",
    "            score = np.nan\n",
    "        print(f\"k={k}: silhouette={score:.3f}\")\n",
    "        if not np.isnan(score) and score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "    if best_k is None:\n",
    "        best_k = 5\n",
    "        print(\"Silhouette selection failed; defaulting k=5\")\n",
    "    print(f\"Selected k={best_k} (silhouette={best_score:.3f})\")\n",
    "    kmeans_model = KMeans(n_clusters=best_k, n_init='auto', random_state=RANDOM_STATE)\n",
    "    k_labels = kmeans_model.fit_predict(X_pca)\n",
    "    stage_df_question_emb['kmeans_k'] = best_k\n",
    "    stage_df_question_emb['kmeans_label'] = k_labels\n",
    "    kmeans_model_cached = kmeans_model\n",
    "else:\n",
    "    print(\"Reusing existing k-means labels (set FORCE_RECOMPUTE_KMEANS=True to recompute).\")\n",
    "\n",
    "# Cluster size summary\n",
    "cluster_sizes = (stage_df_question_emb\n",
    "                 .groupby('kmeans_label')\n",
    "                 .size()\n",
    "                 .reset_index(name='count')\n",
    "                 .sort_values('count', ascending=False))\n",
    "print(\"Cluster size summary (top 20):\")\n",
    "display(cluster_sizes.head(20))\n",
    "\n",
    "print(\"Dimensionality reduction & clustering step complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE plotting utility: configurable coloring + optional stochastic point representatives + optional per-group KDE background\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np, pandas as pd, os, textwrap\n",
    "from typing import Optional\n",
    "\n",
    "# Global configuration defaults (override per call)\n",
    "TSNE_POINT_SIZE = 18\n",
    "TSNE_ALPHA = 0.85\n",
    "TSNE_RANDOM_STATE = 42\n",
    "TSNE_WRAP_WIDTH = 55\n",
    "TSNE_MAX_CHARS = 160\n",
    "TSNE_SAVE_FIGS = True\n",
    "TSNE_SAVE_REPS = True\n",
    "REP_EPS = 1e-9  # epsilon to avoid div-by-zero in distance weighting\n",
    "\n",
    "\n",
    "def _select_representatives(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    embedding_col: str,\n",
    "    pca_model,\n",
    "    n_examples: int,\n",
    "    max_chars: int = 160,\n",
    "    seed: Optional[int] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Select N representative rows per group via stochastic distance-weighted sampling.\"\"\"\n",
    "    if n_examples <= 0:\n",
    "        return pd.DataFrame()\n",
    "    if pca_model is None:\n",
    "        raise RuntimeError(\"PCA model required for representative selection.\")\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = np.vstack(df[embedding_col].tolist())\n",
    "    X_pca = pca_model.transform(X)\n",
    "    labels = df[group_col].to_numpy()\n",
    "    unique_groups = sorted(pd.unique(labels))\n",
    "\n",
    "    records = []\n",
    "    for g in unique_groups:\n",
    "        mask = labels == g\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        pts = X_pca[mask]\n",
    "        centroid = pts.mean(axis=0)\n",
    "        dists = np.linalg.norm(pts - centroid, axis=1)\n",
    "        weights = 1.0 / (dists + REP_EPS)\n",
    "        weights = weights / weights.sum()\n",
    "        orig_indices = df.index[mask].to_numpy()\n",
    "        k = min(n_examples, len(orig_indices))\n",
    "        if k == len(orig_indices):\n",
    "            chosen_local = np.arange(k)\n",
    "        else:\n",
    "            chosen_local = rng.choice(len(orig_indices), size=k, replace=False, p=weights)\n",
    "        chosen_global = orig_indices[chosen_local]\n",
    "        rep_questions, rep_indices, rep_cols, rep_idx_cols = [], [], {}, {}\n",
    "        for j, idx_row in enumerate(chosen_global):\n",
    "            q = df.loc[idx_row, 'question']\n",
    "            q_trunc = q[:max_chars] if isinstance(q, str) else str(q)\n",
    "            rep_questions.append(q_trunc)\n",
    "            rep_indices.append(int(idx_row))\n",
    "            rep_cols[f'rep{j+1}'] = q_trunc\n",
    "            rep_idx_cols[f'rep_idx{j+1}'] = int(idx_row)\n",
    "        records.append({\n",
    "            group_col: g,\n",
    "            'size': int(mask.sum()),\n",
    "            'rep_questions': rep_questions,\n",
    "            'rep_indices': rep_indices,\n",
    "            'example_question': rep_questions[0] if rep_questions else None,\n",
    "            **rep_cols,\n",
    "            **rep_idx_cols,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records).sort_values(group_col).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def plot_tsne_groups(\n",
    "    df: pd.DataFrame,\n",
    "    color_col: str,\n",
    "    *,\n",
    "    tsne_x: str = 'tsne_x',\n",
    "    tsne_y: str = 'tsne_y',\n",
    "    embedding_col: str = 'embedding',\n",
    "    pca_model=None,\n",
    "    n_label_examples: int = 0,\n",
    "    label_wrap_width: int = TSNE_WRAP_WIDTH,\n",
    "    max_chars: int = TSNE_MAX_CHARS,\n",
    "    point_size: int = TSNE_POINT_SIZE,\n",
    "    alpha: float = TSNE_ALPHA,\n",
    "    palette=None,\n",
    "    title: Optional[str] = None,\n",
    "    outfile: Optional[str] = None,\n",
    "    save_reps: bool = False,\n",
    "    legend: bool = True,\n",
    "    random_state: int = TSNE_RANDOM_STATE,\n",
    "    fig_width: float = 7,\n",
    "    fig_height: float = 5,\n",
    "    rep_seed: Optional[int] = None,\n",
    "    # KDE options\n",
    "    show_kde: bool = False,\n",
    "    kde_fill: bool = True,\n",
    "    kde_thresh: float = 0.02,\n",
    "    kde_alpha: float = 0.40,\n",
    "    kde_bw_method: Optional[float] = None,\n",
    "    kde_common_norm: bool = False,\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Generic t-SNE scatter with optional per-point representative labels and per-group KDE background.\n",
    "\n",
    "    KDE now uses hue=color_col so each group has its own density.\n",
    "    Legend updated: uses rectangular patches instead of point markers.\n",
    "    \"\"\"\n",
    "    required_cols = {tsne_x, tsne_y, color_col}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns for plotting: {missing}\")\n",
    "\n",
    "    # Derive palette\n",
    "    if palette is None:\n",
    "        if color_col == 'kmeans_label':\n",
    "            palette = 'tab10'\n",
    "        elif color_col == 'llm_display' and 'llm_palette' in globals():\n",
    "            palette = llm_palette\n",
    "        else:\n",
    "            palette = 'Set2'\n",
    "\n",
    "    plot_df = df\n",
    "\n",
    "    with sns.axes_style(\"white\"):\n",
    "        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "\n",
    "    # Build a color map for later legend Patch handles\n",
    "    unique_vals = list(plot_df[color_col].dropna().unique())\n",
    "    if isinstance(palette, dict):\n",
    "        color_map = {k: palette.get(k, \"#808080\") for k in unique_vals}\n",
    "    else:\n",
    "        pal_colors = sns.color_palette(palette, n_colors=len(unique_vals))\n",
    "        color_map = dict(zip(unique_vals, pal_colors))\n",
    "\n",
    "    # Per-group KDE background\n",
    "    if show_kde:\n",
    "        sns.kdeplot(\n",
    "            data=plot_df,\n",
    "            x=tsne_x,\n",
    "            y=tsne_y,\n",
    "            hue=color_col,\n",
    "            fill=kde_fill,\n",
    "            thresh=kde_thresh,\n",
    "            alpha=kde_alpha,\n",
    "            bw_method=kde_bw_method,\n",
    "            common_norm=kde_common_norm,\n",
    "            palette=color_map,\n",
    "            ax=ax,\n",
    "            legend=False,\n",
    "        )\n",
    "\n",
    "    with sns.axes_style(\"white\"):\n",
    "        sns.scatterplot(\n",
    "            data=plot_df,\n",
    "            x=tsne_x, y=tsne_y,\n",
    "            hue=color_col,\n",
    "            palette=color_map,  # pass explicit mapping\n",
    "            s=point_size,\n",
    "            linewidth=0,\n",
    "            ax=ax,\n",
    "            alpha=alpha,\n",
    "            zorder=3 if show_kde else 1,\n",
    "            legend=False,  # disable automatic legend so we can substitute patches\n",
    "        )\n",
    "\n",
    "    # Custom rectangular legend\n",
    "    if legend:\n",
    "        from matplotlib.patches import Patch\n",
    "        handles = [\n",
    "            Patch(facecolor=color_map[v], edgecolor='none', label=str(v), alpha=alpha)\n",
    "            for v in unique_vals\n",
    "        ]\n",
    "        ax.legend(\n",
    "            handles=handles,\n",
    "            bbox_to_anchor=(0.5, 1),\n",
    "            loc='upper center',\n",
    "            title=None,\n",
    "            ncol=len(unique_vals),\n",
    "            frameon=True,\n",
    "        )\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    ax.set_xlabel('t-SNE 1'); ax.set_ylabel('t-SNE 2')\n",
    "\n",
    "    reps_df = None\n",
    "    if n_label_examples > 0:\n",
    "        if pca_model is None:\n",
    "            raise RuntimeError(\"pca_model required for representative labels.\")\n",
    "        reps_df = _select_representatives(\n",
    "            df=df,\n",
    "            group_col=color_col,\n",
    "            embedding_col=embedding_col,\n",
    "            pca_model=pca_model,\n",
    "            n_examples=n_label_examples,\n",
    "            max_chars=max_chars,\n",
    "            seed=rep_seed,\n",
    "        )\n",
    "        for _, row in reps_df.iterrows():\n",
    "            rep_indices = row.get('rep_indices', [])\n",
    "            rep_questions = row.get('rep_questions', [])\n",
    "            for idx_pt, q_text in zip(rep_indices, rep_questions):\n",
    "                pt = df.loc[idx_pt]\n",
    "                x_pt = pt[tsne_x]\n",
    "                y_pt = pt[tsne_y]\n",
    "                short = textwrap.shorten(str(q_text).replace('\\n',' '), width=label_wrap_width, placeholder='…')\n",
    "                ax.text(\n",
    "                    x_pt, y_pt, short,\n",
    "                    ha='center', va='center', fontsize=7.0,\n",
    "                    bbox=dict(boxstyle='round,pad=0.20', fc='white', ec='none', alpha=0.72),\n",
    "                    zorder=4,\n",
    "                )\n",
    "        if color_col == 'kmeans_label':\n",
    "            globals()['cluster_reps_df'] = reps_df.copy()\n",
    "            if save_reps and TSNE_SAVE_REPS:\n",
    "                reps_path = os.path.join(PATH_EXPORT, 'kmeans_cluster_representatives.csv')\n",
    "                reps_df.to_csv(reps_path, index=False)\n",
    "                print(f\"Saved representatives -> {reps_path}\\n\")\n",
    "\n",
    "    sns.despine(); plt.tight_layout()\n",
    "    if outfile and TSNE_SAVE_FIGS:\n",
    "        plt.savefig(outfile, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return reps_df\n",
    "\n",
    "# --- Example usage (k-means clusters) ---\n",
    "if 'pca_model_cached' not in globals():\n",
    "    raise RuntimeError(\"pca_model_cached not found; run compute cell first.\")\n",
    "\n",
    "cluster_reps_df = plot_tsne_groups(\n",
    "    stage_df_question_emb,\n",
    "    color_col='kmeans_label',\n",
    "    pca_model=pca_model_cached,\n",
    "    n_label_examples=2,\n",
    "    outfile=os.path.join(PATH_EXPORT, f'tsne_kmeans_{SUBSET_LABEL}.pdf'),\n",
    "    save_reps=True,\n",
    "    fig_width=7,\n",
    "    fig_height=6,\n",
    "    rep_seed=123,\n",
    "    show_kde=False,\n",
    "    legend=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95104d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE colored by Captain Type using unified plot_tsne_groups\n",
    "if 'pca_model_cached' not in globals():\n",
    "    raise RuntimeError(\"pca_model_cached not found; run compute cell first.\")\n",
    "\n",
    "plot_tsne_groups(\n",
    "    stage_df_question_emb,\n",
    "    color_col=\"captain_type_display\",\n",
    "    fig_width=7,\n",
    "    fig_height=6,\n",
    "    pca_model=pca_model_cached,\n",
    "    n_label_examples=6,\n",
    "    rep_seed=123,\n",
    "    outfile=os.path.join(PATH_EXPORT, f\"tsne_captain_type_{SUBSET_LABEL}.pdf\"),\n",
    "    save_reps=False,\n",
    "    show_kde=True,\n",
    "    alpha=0.5,\n",
    "    kde_alpha=0.2,\n",
    "    legend=True,\n",
    ")\n",
    "print(\"Captain type t-SNE plot generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00174a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE colored by LLM using unified plot_tsne_groups\n",
    "if 'pca_model_cached' not in globals():\n",
    "    raise RuntimeError(\"pca_model_cached not found; run compute cell first.\")\n",
    "\n",
    "plot_tsne_groups(\n",
    "    stage_df_question_emb,\n",
    "    color_col='llm_display',\n",
    "    pca_model=pca_model_cached,\n",
    "    outfile=os.path.join(PATH_EXPORT, f\"tsne_{SUBSET_LABEL}.pdf\"),\n",
    "    save_reps=False,\n",
    "    legend=False,\n",
    "    fig_width=7,\n",
    "    fig_height=6,\n",
    "    n_label_examples=3,\n",
    "    rep_seed=111,\n",
    "    show_kde=False,\n",
    ")\n",
    "print(\"LLM t-SNE plot generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5fae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battleship_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
