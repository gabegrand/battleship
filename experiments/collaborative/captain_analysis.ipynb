{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e28f440",
   "metadata": {},
   "source": [
    "# Captain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from battleship.run_captain_benchmarks import rebuild_captain_summary_from_results\n",
    "from battleship.utils import resolve_project_path\n",
    "from battleship.agents import EIGCalculator, CodeQuestion, Question\n",
    "from battleship.game import Board\n",
    "\n",
    "from analysis import (\n",
    "    CAPTAIN_TYPE_LABELS,\n",
    "    MODEL_DISPLAY_NAMES,\n",
    "    human_round_summaries,\n",
    "    build_competitor_column,\n",
    "    compute_pairwise_win_rates,\n",
    "    plot_grouped_winrate_heatmap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542334d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# set seaborn color palette\n",
    "sns.set_palette(\"tab10\")\n",
    "\n",
    "# set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_EXPERIMENT_NAME = \"battleship-final-data\"\n",
    "PATH_DATA = os.path.join(\"data\", HUMAN_EXPERIMENT_NAME)\n",
    "PATH_EXPORT = os.path.join(PATH_DATA, \"export\")\n",
    "\n",
    "CAPTAIN_EXPERIMENT_PATH = (\n",
    "    \"experiments/collaborative/captain_benchmarks/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0bcb23",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d1618",
   "metadata": {},
   "source": [
    "### Human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1eaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df = human_round_summaries(\n",
    "    experiment_path=PATH_DATA,\n",
    ")\n",
    "human_df = pd.DataFrame(human_df)\n",
    "\n",
    "human_df = human_df.assign(llm=\"Human\")\n",
    "human_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e47cd7",
   "metadata": {},
   "source": [
    "### Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77baafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_round_data_unresolved_paths = [\n",
    "    (\"gpt-4o\", \"run_2025_08_25_16_28_19\"),\n",
    "    (\"gpt-5\", \"run_2025_08_25_22_02_29\"),\n",
    "    (\"llama-4-scout\", \"run_2025_08_26_17_56_46\"),\n",
    "    (\"Baseline\", \"run_2025_08_26_17_23_23\"),\n",
    "]\n",
    "\n",
    "model_round_data_paths = [\n",
    "    (name, resolve_project_path(os.path.join(CAPTAIN_EXPERIMENT_PATH, path)))\n",
    "    for name, path in model_round_data_unresolved_paths\n",
    "]\n",
    "for name, path in model_round_data_paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"The path {path} does not exist.\")\n",
    "\n",
    "dfs = []\n",
    "for name, path in model_round_data_paths:\n",
    "    df = pd.DataFrame(rebuild_captain_summary_from_results(path))\n",
    "    if df.empty:\n",
    "        continue\n",
    "    df[\"llm\"] = name\n",
    "    df[\"run_dir\"] = path  # retain run directory for downstream file access\n",
    "    dfs.append(df)\n",
    "\n",
    "model_df = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58184756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append summary_df to round_df\n",
    "df = pd.concat([human_df, model_df], ignore_index=True)\n",
    "\n",
    "primary_columns = [\"captain_type_display\", \"llm_display\", \"board_id\", \"seed\"]\n",
    "\n",
    "# Create categorical column for captain_type_display\n",
    "df[\"captain_type_display\"] = pd.Categorical(\n",
    "    df[\"captain_type\"].map(CAPTAIN_TYPE_LABELS),\n",
    "    categories=list(dict.fromkeys(CAPTAIN_TYPE_LABELS.values())),\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "# Create categorical column for llm_display\n",
    "df[\"llm_display\"] = pd.Categorical(\n",
    "    df[\"llm\"],\n",
    "    categories=[\"Human\", \"Baseline\"] + [x for x in MODEL_DISPLAY_NAMES.values() if x in df[\"llm\"].unique()],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "# Move primary columns to the front\n",
    "df = df[primary_columns + [col for col in df.columns if col not in primary_columns]]\n",
    "\n",
    "# Sort the DataFrame by primary columns\n",
    "df = df.sort_values(by=primary_columns, ascending=True).reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad155118",
   "metadata": {},
   "source": [
    "## Precision/Recall Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBreakdown by captain_type_display:\")\n",
    "for captain_type in df['captain_type_display'].cat.categories:\n",
    "    llms = df[df['captain_type_display'] == captain_type]['llm'].unique()\n",
    "    print(f\"{captain_type}: {llms}\")\n",
    "\n",
    "\n",
    "# Colorblind-friendly palette (Okabe–Ito)\n",
    "llm_palette = {\n",
    "    \"Human\": \"#009E73\",  # green\n",
    "    \"Baseline\": \"#0072B2\",  # blue\n",
    "    \"llama-4-scout\": \"#CC79A7\",  # purple\n",
    "    \"gpt-4o\": \"#E69F00\",  # orange (similar to gpt-5)\n",
    "    \"gpt-5\": \"#D55E00\",  # vermillion\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcce8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Firing Accuracy (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85577287",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Prepare ordered captain categories that actually appear in the data\n",
    "captain_categories = [\n",
    "    c for c in df[\"captain_type_display\"].cat.categories\n",
    "    if c in df[\"captain_type_display\"].values\n",
    "]\n",
    "\n",
    "# Determine max number of llm groups present for any captain (for consistent box widths)\n",
    "llm_counts = df.groupby(\"captain_type_display\")[\"llm\"].nunique()\n",
    "max_llms = int(llm_counts.max()) if len(llm_counts) > 0 else 1\n",
    "\n",
    "# Base positions for each captain on the x axis\n",
    "x_positions = np.arange(len(captain_categories))\n",
    "\n",
    "# Box width: leave some padding between captain groups\n",
    "group_width = 0.5  # total width occupied by boxes for one captain\n",
    "box_width = group_width / max_llms\n",
    "\n",
    "# Ensure grid lines are drawn below plot elements and only horizontal gridlines are shown\n",
    "ax.set_axisbelow(True)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# Map captain -> present llms to ensure we only plot existing combinations\n",
    "# Use the llm_display categorical ordering so order is consistent across plots\n",
    "llm_order = list(df[\"llm_display\"].cat.categories) if \"llm_display\" in df.columns else sorted(df[\"llm\"].unique())\n",
    "\n",
    "for i, captain in enumerate(captain_categories):\n",
    "    present_llms_unsorted = df[df[\"captain_type_display\"] == captain][\"llm\"].unique()\n",
    "    # Preserve the display order\n",
    "    present_llms = [llm for llm in llm_order if llm in present_llms_unsorted]\n",
    "\n",
    "    m = len(present_llms)\n",
    "    if m == 0:\n",
    "        continue\n",
    "\n",
    "    # Offsets to center m boxes around the captain x position\n",
    "    offsets = (np.arange(m) - (m - 1) / 2.0) * box_width\n",
    "\n",
    "    for j, llm in enumerate(present_llms):\n",
    "        subset = df[(df[\"captain_type_display\"] == captain) & (df[\"llm\"] == llm)][\"f1_score\"].dropna()\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        pos = x_positions[i] + offsets[j]\n",
    "        color = llm_palette.get(llm, \"#808080\")\n",
    "\n",
    "        # Use matplotlib's boxplot to place each box at the computed numeric position\n",
    "        bp = ax.boxplot(subset.values,\n",
    "                        positions=[pos],\n",
    "                        widths=box_width * 0.9,\n",
    "                        patch_artist=True,\n",
    "                        manage_ticks=False)\n",
    "\n",
    "        # Style the box elements\n",
    "        for element in [\"boxes\", \"whiskers\", \"caps\", \"medians\"]:\n",
    "            plt.setp(bp[element], color=color)\n",
    "        for patch in bp[\"boxes\"]:\n",
    "            patch.set(facecolor=color, alpha=0.6)\n",
    "\n",
    "        # Make fliers (outliers) less visually distinctive: smaller, lower-alpha, and same color as box\n",
    "        if \"fliers\" in bp:\n",
    "            for f in bp[\"fliers\"]:\n",
    "                f.set(marker='o', markersize=3, markerfacecolor=color, markeredgecolor=color, alpha=0.35, markeredgewidth=0)\n",
    "\n",
    "# Create legend handles for llm types present in the full DataFrame, in llm_display order\n",
    "from matplotlib.patches import Patch\n",
    "all_present_llms = [llm for llm in llm_order if llm in df[\"llm\"].unique()]\n",
    "legend_handles = [Patch(facecolor=llm_palette[k], label=k, alpha=0.6) for k in all_present_llms]\n",
    "\n",
    "ax.legend(handles=legend_handles, loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "# Final formatting\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(captain_categories, rotation=90)\n",
    "ax.set_xlabel(\"Captain Type\")\n",
    "ax.set_ylabel(\"Firing Accuracy (F1)\")\n",
    "ax.set_xlim(-0.5, len(captain_categories) - 0.5)\n",
    "\n",
    "sns.despine()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(os.path.join(PATH_EXPORT, \"captain_f1_boxplot.pdf\"), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd10a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sns.stripplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    alpha=0.7,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Firing Accuracy (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    # alpha=0.7,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Firing Accuracy (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d45249",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    data=df,\n",
    "    kind=\"ecdf\",\n",
    "    x=\"f1_score\",\n",
    "    hue=\"captain_type_display\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3115c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"move_count\"] = df[\"hits\"] + df[\"misses\"]\n",
    "\n",
    "sns.barplot(data=df, x=\"captain_type\", y=\"move_count\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da997ec9",
   "metadata": {},
   "source": [
    "## Win rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f463cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build competitor column (LLM first | Captain)\n",
    "df = build_competitor_column(df, llm_col=\"llm_display\", captain_col=\"captain_type_display\", out_col=\"competitor\")\n",
    "\n",
    "# Compute win rates for F1 (higher better)\n",
    "win_results_f1_comp = compute_pairwise_win_rates(\n",
    "    df, metric=\"f1_score\", higher_is_better=True, competitor_col=\"competitor\", board_col=\"board_id\"\n",
    ")\n",
    "print(\"Mean board win rate matrix (F1, competitor-level):\")\n",
    "display(win_results_f1_comp[\"mean_board_win_rate_matrix\"])\n",
    "# print(\"Weighted all-pairs win rate matrix (F1, competitor-level):\")\n",
    "# display(win_results_f1_comp[\"weighted_win_rate_matrix\"])\n",
    "\n",
    "# # Compute win rates for move count (lower better) if available\n",
    "# if \"move_count\" in df.columns:\n",
    "#     win_results_moves_comp = compute_pairwise_win_rates(\n",
    "#         df, metric=\"move_count\", higher_is_better=False, competitor_col=\"competitor\", board_col=\"board_id\"\n",
    "#     )\n",
    "#     print(\"Mean board win rate matrix (Move Count, competitor-level):\")\n",
    "#     display(win_results_moves_comp[\"mean_board_win_rate_matrix\"])\n",
    "#     print(\"Weighted all-pairs win rate matrix (Move Count, competitor-level):\")\n",
    "#     display(win_results_moves_comp[\"weighted_win_rate_matrix\"])\n",
    "# else:\n",
    "#     print(\"Column 'move_count' not found; skip move-count win rates.\")\n",
    "\n",
    "# Aggregate summary for F1\n",
    "# f1_comp_summary = win_results_f1_comp[\"aggregate\"][[\n",
    "#     \"competitor_a\", \"competitor_b\", \"mean_board_win_rate\", \"weighted_all_pairs_win_rate\", \"boards_considered\", \"board_wins\", \"board_comparisons\"\n",
    "# ]].sort_values([\"competitor_a\", \"competitor_b\"]).reset_index(drop=True)\n",
    "# print(\"Pairwise aggregate win rates (F1, competitor-level):\")\n",
    "# display(f1_comp_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped heatmap using refactored helper\n",
    "base_matrix = win_results_f1_comp[\"weighted_win_rate_matrix\"].copy().astype(float)\n",
    "\n",
    "fig, ax = plot_grouped_winrate_heatmap(\n",
    "    base_matrix,\n",
    "    llm_palette=llm_palette,\n",
    "    cmap=\"cividis\",\n",
    "    annotate=True,\n",
    "    captain_tick_fontsize=6,\n",
    "    row_alpha=1.0,\n",
    "    col_alpha=1.0,\n",
    "    show_group_separators=True,\n",
    "    separator_width=4,\n",
    "    shade_rows=True,\n",
    "    shade_cols=True,\n",
    "    group_label_rotation=90,\n",
    "    group_label_fontsize=10,\n",
    "    output_path=os.path.join(PATH_EXPORT, \"f1_winrate_heatmap.pdf\"),\n",
    "    title=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20f64d",
   "metadata": {},
   "source": [
    "## EIG Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model EIG Extraction (All Runs) -----------------------------------------\n",
    "# Build EIG table directly from model_df metadata (no filename round parsing).\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Guard: ensure model_df exists\n",
    "if 'model_df' not in globals() or model_df.empty:\n",
    "    model_eig_df = pd.DataFrame()\n",
    "else:\n",
    "    # We expect columns: llm, run_dir, round_id, captain_type\n",
    "    required = {\"llm\", \"run_dir\", \"round_id\", \"captain_type\"}\n",
    "    missing = required - set(model_df.columns)\n",
    "    if missing:\n",
    "        print(f\"Missing columns in model_df: {missing}; cannot extract EIG.\")\n",
    "        model_eig_df = pd.DataFrame()\n",
    "    else:\n",
    "        records = []\n",
    "        # Iterate unique (llm, run_dir, round_id, captain_type)\n",
    "        for (llm, run_dir, round_id, captain_type) in (\n",
    "            model_df[[\"llm\", \"run_dir\", \"round_id\", \"captain_type\"]]\n",
    "            .drop_duplicates()\n",
    "            .itertuples(index=False, name=None)\n",
    "        ):\n",
    "            captain_json = Path(run_dir) / \"rounds\" / f\"round_{round_id}\" / \"captain\" / \"captain.json\"\n",
    "            if not captain_json.exists():\n",
    "                continue\n",
    "            try:\n",
    "                with captain_json.open() as f:\n",
    "                    entries = json.load(f)\n",
    "            except Exception:\n",
    "                continue\n",
    "            for q_idx, entry in enumerate(entries):\n",
    "                eig_value = entry.get(\"eig\")\n",
    "                q_block = entry.get(\"question\")\n",
    "                if eig_value is None or q_block is None:\n",
    "                    continue\n",
    "                # Nested question text extraction\n",
    "                if isinstance(q_block, dict):\n",
    "                    inner_q = q_block.get(\"question\")\n",
    "                    q_text = inner_q.get(\"text\") if isinstance(inner_q, dict) else None\n",
    "                else:\n",
    "                    q_text = None\n",
    "                q_text = q_text or \"No question text\"\n",
    "\n",
    "                raw_candidates = entry.get(\"eig_questions\") or []\n",
    "                processed = None\n",
    "                if raw_candidates:\n",
    "                    processed = [\n",
    "                        (\n",
    "                            c.get(\"question\", {})\n",
    "                            .get(\"question\", {})\n",
    "                            .get(\"text\"),\n",
    "                            c.get(\"eig\"),\n",
    "                            None,\n",
    "                        )\n",
    "                        for c in raw_candidates\n",
    "                    ]\n",
    "                    vals = [c[1] for c in processed if c[1] is not None]\n",
    "                    if vals:\n",
    "                        mx = max(vals)\n",
    "                        processed = [(qt, ev, ev == mx) for (qt, ev, _) in processed]\n",
    "\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"llm\": llm,\n",
    "                        \"captain_type\": captain_type,\n",
    "                        \"run_dir\": run_dir,\n",
    "                        \"round_id\": round_id,\n",
    "                        \"question_idx\": q_idx,\n",
    "                        \"question\": q_text,\n",
    "                        \"eig\": eig_value,\n",
    "                        \"eig_candidates\": processed,\n",
    "                    }\n",
    "                )\n",
    "        model_eig_df = pd.DataFrame(records)\n",
    "        if not model_eig_df.empty:\n",
    "            model_eig_df = model_eig_df.sort_values([\"llm\", \"round_id\", \"question_idx\"]).reset_index(drop=True)\n",
    "\n",
    "# Add captain_type_display similar to main df\n",
    "if not model_eig_df.empty and 'CAPTAIN_TYPE_LABELS' in globals():\n",
    "    model_eig_df['captain_type_display'] = model_eig_df['captain_type'].map(CAPTAIN_TYPE_LABELS)\n",
    "    # Preserve order used elsewhere\n",
    "    cat_order = [x for x in dict.fromkeys(CAPTAIN_TYPE_LABELS.values()) if x in model_eig_df['captain_type_display'].unique()]\n",
    "    model_eig_df['captain_type_display'] = pd.Categorical(model_eig_df['captain_type_display'], categories=cat_order, ordered=True)\n",
    "\n",
    "model_eig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-Captain EIG Distribution --------------------------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "if 'model_eig_df' not in globals() or model_eig_df.empty:\n",
    "    print(\"model_eig_df is empty; run the EIG extraction cell first.\")\n",
    "else:\n",
    "    x_col = 'captain_type_display' if 'captain_type_display' in model_eig_df.columns else 'captain_type'\n",
    "\n",
    "    plt.figure(figsize=(8, 4.5))\n",
    "    ax = sns.boxplot(\n",
    "        data=model_eig_df,\n",
    "        x=x_col,\n",
    "        y='eig',\n",
    "        hue='llm',\n",
    "        palette=llm_palette,\n",
    "        showfliers=False,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel('Captain Type')\n",
    "    ax.set_ylabel('EIG')\n",
    "    ax.set_title('Per-Captain EIG Distribution (Models)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    ax.legend(title='LLM', bbox_to_anchor=(1, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64679a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eig_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.displot(\n",
    "    data=model_eig_df,\n",
    "    kind=\"ecdf\",\n",
    "    col=\"captain_type_display\",\n",
    "    x=\"eig\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    complementary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ECDF Delta (LLM vs EIG captain types) -----------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "_ecdf_categories = (\"LLM\", \"EIG\")  # (baseline, enhanced)\n",
    "complementary = True  # match earlier visualization style (survival curves)\n",
    "alpha_fill = 0.18\n",
    "linewidth = 2.0\n",
    "\n",
    "if 'model_eig_df' not in globals() or model_eig_df.empty:\n",
    "    print(\"model_eig_df empty; run extraction first.\")\n",
    "else:\n",
    "    # Ensure required captain types exist\n",
    "    present_cats = set(model_eig_df['captain_type_display'].dropna().unique())\n",
    "    missing = [c for c in _ecdf_categories if c not in present_cats]\n",
    "    if missing:\n",
    "        print(f\"Missing categories for delta plot: {missing}\")\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(7.5, 5))\n",
    "\n",
    "        legend_elements = []\n",
    "        area_rows = []\n",
    "\n",
    "        for llm in [c for c in llm_palette.keys() if c in model_eig_df['llm'].unique()]:\n",
    "            sub = model_eig_df[model_eig_df['llm'] == llm]\n",
    "            # Need both categories for this llm\n",
    "            if not set(_ecdf_categories).issubset(set(sub['captain_type_display'].unique())):\n",
    "                continue\n",
    "            data_a = sub[sub['captain_type_display'] == _ecdf_categories[0]]['eig'].dropna().values\n",
    "            data_b = sub[sub['captain_type_display'] == _ecdf_categories[1]]['eig'].dropna().values\n",
    "            if len(data_a) == 0 or len(data_b) == 0:\n",
    "                continue\n",
    "\n",
    "            # Build common grid\n",
    "            grid = np.unique(np.concatenate([data_a, data_b]))\n",
    "            n_a = len(data_a)\n",
    "            n_b = len(data_b)\n",
    "            # ECDF values using searchsorted (right) for F(x) = P(X <= x)\n",
    "            y_a = np.searchsorted(np.sort(data_a), grid, side='right') / n_a\n",
    "            y_b = np.searchsorted(np.sort(data_b), grid, side='right') / n_b\n",
    "            if complementary:\n",
    "                y_a = 1 - y_a\n",
    "                y_b = 1 - y_b\n",
    "\n",
    "            color = llm_palette.get(llm, '#444444')\n",
    "            # Plot baseline (LLM) dashed, enhanced (EIG) solid\n",
    "            ax.plot(grid, y_a, linestyle='--', color=color, linewidth=linewidth, alpha=0.9)\n",
    "            ax.plot(grid, y_b, linestyle='-', color=color, linewidth=linewidth, alpha=0.9)\n",
    "\n",
    "            # Shade region between curves\n",
    "            ax.fill_between(grid, y_a, y_b, color=color, alpha=alpha_fill, linewidth=0)\n",
    "\n",
    "            # Approximate absolute area difference (integral of |delta|) for reference\n",
    "            area_diff = np.trapz(np.abs(y_b - y_a), grid)\n",
    "            area_rows.append({'llm': llm, 'area_abs_diff': area_diff})\n",
    "\n",
    "        # Theoretical maximum EIG for binary uniform truth with noise epsilon (1 - H(eps))\n",
    "        def _binary_entropy(p: float) -> float:\n",
    "            if p <= 0 or p >= 1:\n",
    "                return 0.0\n",
    "            return -(p * np.log2(p) + (1 - p) * np.log2(1 - p))\n",
    "        eps = 0.1\n",
    "        theoretical_max_eig = 1 - _binary_entropy(eps)\n",
    "        ax.axvline(theoretical_max_eig, color='k', linestyle=':', linewidth=1.5)\n",
    "        ymin, ymax = ax.get_ylim()\n",
    "        ax.text(theoretical_max_eig, ymax * 0.97, f\"max EIG (ε={eps})≈{theoretical_max_eig:.3f}\", rotation=90, va='top', ha='right', fontsize=9, color='k')\n",
    "\n",
    "        ax.set_xlabel('EIG')\n",
    "        ax.set_ylabel('Proportion (CDF)' if not complementary else 'Proportion (1 - CDF)')\n",
    "        ax.set_title(r\"$\\Delta$EIG w/r/t Base LLM\")\n",
    "        ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "        # Custom legend (one entry per llm color, showing both line styles)\n",
    "        from matplotlib.lines import Line2D\n",
    "        custom_handles = []\n",
    "        for llm in [c for c in llm_palette.keys() if c in model_eig_df['llm'].unique()]:\n",
    "            color = llm_palette.get(llm, '#444444')\n",
    "            custom_handles.append(Line2D([0,1],[0,1], color=color, linestyle='-', label=f'{llm} ({_ecdf_categories[1]})'))\n",
    "            custom_handles.append(Line2D([0,1],[0,1], color=color, linestyle='--', label=f'{llm} ({_ecdf_categories[0]})'))\n",
    "        ax.legend(handles=custom_handles, bbox_to_anchor=(1,1), loc='upper left', title='LLM (Captain Type)')\n",
    "\n",
    "        sns.despine()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if area_rows:\n",
    "            area_df = pd.DataFrame(area_rows).sort_values('area_abs_diff', ascending=False)\n",
    "            display(area_df.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Human Question EIG Calculation ------------------------------------------\n",
    "# Computes EIG for human questions and caches to human_eig_df.csv (≈1–2 min first run).\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "INPUT_JSON_PATH = resolve_project_path(\n",
    "    \"experiments/collaborative/spotter_benchmarks/o4-mini_CodeSpotterModel_True.json\"\n",
    ")\n",
    "CACHE_PATH = Path(\"human_eig_df.csv\")\n",
    "\n",
    "\n",
    "def load_human_interactions(json_path: Path) -> list[dict]:\n",
    "    with json_path.open() as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def build_human_df(entries: list[dict]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for e in entries:\n",
    "        if not (\"question\" in e and \"occTiles\" in e):\n",
    "            continue\n",
    "        answer = e.get(\"answer\", \"\").lower()\n",
    "        # Normalize boolean text answers.\n",
    "        if answer == \"true\":\n",
    "            answer = \"yes\"\n",
    "        elif answer == \"false\":\n",
    "            answer = \"no\"\n",
    "        true_answer = e.get(\"true_answer\")\n",
    "        rows.append(\n",
    "            {\n",
    "                \"question\": e.get(\"question\"),\n",
    "                \"program\": e.get(\"program\"),\n",
    "                \"board_state\": e.get(\"occTiles\"),\n",
    "                \"answer\": answer,\n",
    "                \"true_answer\": true_answer,\n",
    "                \"correct\": answer == true_answer,\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "if CACHE_PATH.exists():\n",
    "    human_eig_df = pd.read_csv(CACHE_PATH)\n",
    "else:\n",
    "    raw_entries = load_human_interactions(Path(INPUT_JSON_PATH))\n",
    "    human_eig_df = build_human_df(raw_entries)\n",
    "    # Keep only correctly answered questions.\n",
    "    human_eig_df = human_eig_df[human_eig_df[\"correct\"]]\n",
    "\n",
    "    eig_calculator = EIGCalculator(samples=1000, timeout=15, epsilon=0)\n",
    "    human_eig_df[\"calculated_eig\"] = None\n",
    "\n",
    "    for idx, row in human_eig_df.iterrows():\n",
    "        code_question = CodeQuestion(\n",
    "            question=Question(row[\"question\"]),\n",
    "            fn_text=row[\"program\"],\n",
    "            translation_prompt=\"\",\n",
    "            completion={},\n",
    "        )\n",
    "        board = Board.from_occ_tiles(row[\"board_state\"])  # reconstruct board\n",
    "        human_eig_df.at[idx, \"calculated_eig\"] = eig_calculator(code_question, board)\n",
    "\n",
    "    human_eig_df.to_csv(CACHE_PATH, index=False)\n",
    "\n",
    "human_eig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare EIG Distributions (Model vs Human) ------------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plot_data = pd.DataFrame(\n",
    "    {\n",
    "        \"EIG\": pd.concat([model_eig_df[\"eig\"], human_eig_df[\"calculated_eig\"]], ignore_index=True),\n",
    "        \"Source\": [\"model\"] * len(model_eig_df) + [\"human\"] * len(human_eig_df),\n",
    "    }\n",
    ")\n",
    "\n",
    "ax = sns.boxplot(data=plot_data, x=\"Source\", y=\"EIG\", palette=\"Set2\")\n",
    "ax.set(\n",
    "    title=\"EIG Distribution (Model vs Human)\",\n",
    "    xlabel=\"Source\",\n",
    "    ylabel=\"EIG\",\n",
    ")\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "avg_model = model_eig_df[\"eig\"].mean()\n",
    "avg_human = pd.to_numeric(human_eig_df[\"calculated_eig\"], errors=\"coerce\").mean()\n",
    "print(f\"Average model EIG: {avg_model:.4f}\")\n",
    "print(f\"Average human EIG: {avg_human:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8fc129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battleship-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
