{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e28f440",
   "metadata": {},
   "source": [
    "# Captain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from battleship.run_captain_benchmarks import rebuild_captain_summary_from_results\n",
    "from battleship.utils import resolve_project_path\n",
    "from battleship.agents import EIGCalculator, CodeQuestion, Question\n",
    "from battleship.game import Board\n",
    "\n",
    "from analysis import CAPTAIN_TYPE_LABELS, MODEL_DISPLAY_NAMES\n",
    "from analysis import human_round_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542334d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# set seaborn color palette\n",
    "sns.set_palette(\"tab10\")\n",
    "\n",
    "# set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_EXPERIMENT_NAME = \"battleship-final-data\"\n",
    "PATH_DATA = os.path.join(\"data\", HUMAN_EXPERIMENT_NAME)\n",
    "PATH_EXPORT = os.path.join(PATH_DATA, \"export\")\n",
    "\n",
    "CAPTAIN_EXPERIMENT_PATH = (\n",
    "    \"experiments/collaborative/captain_benchmarks/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0bcb23",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d1618",
   "metadata": {},
   "source": [
    "### Human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1eaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df = human_round_summaries(\n",
    "    experiment_path=PATH_DATA,\n",
    ")\n",
    "human_df = pd.DataFrame(human_df)\n",
    "\n",
    "human_df = human_df.assign(llm=\"Human\")\n",
    "human_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e47cd7",
   "metadata": {},
   "source": [
    "### Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77baafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_round_data_unresolved_paths = [\n",
    "    (\"gpt-4o\", \"run_2025_08_25_16_28_19\"),\n",
    "    (\"gpt-5\", \"run_2025_08_25_22_02_29\"),\n",
    "    (\"llama-4-scout\", \"run_2025_08_26_17_56_46\"),\n",
    "    (\"Baseline\", \"run_2025_08_26_17_23_23\"),\n",
    "]\n",
    "\n",
    "model_round_data_paths = [\n",
    "    (name, resolve_project_path(os.path.join(CAPTAIN_EXPERIMENT_PATH, path)))\n",
    "    for name, path in model_round_data_unresolved_paths\n",
    "]\n",
    "for name, path in model_round_data_paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"The path {path} does not exist.\")\n",
    "\n",
    "dfs = []\n",
    "for name, path in model_round_data_paths:\n",
    "    df = pd.DataFrame(rebuild_captain_summary_from_results(path))\n",
    "    df[\"llm\"] = name\n",
    "    dfs.append(df)\n",
    "\n",
    "model_df = pd.concat(dfs, ignore_index=True)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58184756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append summary_df to round_df\n",
    "df = pd.concat([human_df, model_df], ignore_index=True)\n",
    "\n",
    "primary_columns = [\"captain_type_display\", \"llm_display\", \"board_id\", \"seed\"]\n",
    "\n",
    "# Create categorical column for captain_type_display\n",
    "df[\"captain_type_display\"] = pd.Categorical(\n",
    "    df[\"captain_type\"].map(CAPTAIN_TYPE_LABELS),\n",
    "    categories=list(dict.fromkeys(CAPTAIN_TYPE_LABELS.values())),\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "# Create categorical column for llm_display\n",
    "df[\"llm_display\"] = pd.Categorical(\n",
    "    df[\"llm\"],\n",
    "    categories=[\"Human\", \"Baseline\"] + [x for x in MODEL_DISPLAY_NAMES.values() if x in df[\"llm\"].unique()],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "# Move primary columns to the front\n",
    "df = df[primary_columns + [col for col in df.columns if col not in primary_columns]]\n",
    "\n",
    "# Sort the DataFrame by primary columns\n",
    "df = df.sort_values(by=primary_columns, ascending=True).reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad155118",
   "metadata": {},
   "source": [
    "## Precision/Recall Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBreakdown by captain_type_display:\")\n",
    "for captain_type in df['captain_type_display'].cat.categories:\n",
    "    llms = df[df['captain_type_display'] == captain_type]['llm'].unique()\n",
    "    print(f\"{captain_type}: {llms}\")\n",
    "\n",
    "\n",
    "# Colorblind-friendly palette (Okabeâ€“Ito)\n",
    "llm_palette = {\n",
    "    \"Human\": \"#009E73\",  # green\n",
    "    \"Baseline\": \"#0072B2\",  # blue\n",
    "    \"llama-4-scout\": \"#CC79A7\",  # purple\n",
    "    \"gpt-4o\": \"#E69F00\",  # orange (similar to gpt-5)\n",
    "    \"gpt-5\": \"#D55E00\",  # vermillion\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(data=df, kind=\"box\", x=\"captain_type_display\", y=\"f1_score\", hue=\"llm\", palette=llm_palette)\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Firing Accuracy (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "g._legend.set_title(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=df,\n",
    "    kind=\"strip\",\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Firing Accuracy (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "g._legend.set_title(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd10a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sns.stripplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    alpha=0.7,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Firing Accuracy (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    # alpha=0.7,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Firing Accuracy (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75887334",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for captain_type, group in df.groupby('captain_type'):\n",
    "    f1_scores = np.sort(group['f1_score'].dropna())\n",
    "    cdf = np.arange(1, len(f1_scores) + 1) / len(f1_scores)\n",
    "    plt.step(f1_scores, cdf, where='post', label=captain_type)\n",
    "\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('CDF of F1 Score by Captain Type')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3115c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"move_count\"] = df[\"hits\"] + df[\"misses\"]\n",
    "\n",
    "sns.barplot(data=df, x=\"captain_type\", y=\"move_count\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20f64d",
   "metadata": {},
   "source": [
    "## EIG Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to run directory\n",
    "# base_path = resolve_project_path(\"experiments/collaborative/captain_benchmarks/run_combined/run_4o_mapeig_cot_captain\")\n",
    "base_path = resolve_project_path(\n",
    "    \"experiments/collaborative/captain_benchmarks/run_combined/run_4o_llmdecision_captain\"\n",
    ")\n",
    "\n",
    "# Find all captain.json files in subdirectories\n",
    "captain_files = glob.glob(os.path.join(base_path, '**/captain/captain.json'), recursive=True)\n",
    "\n",
    "# Dictionary to store eig values by file\n",
    "eig_values_by_file = {}\n",
    "# Initialize list to store data for DataFrame\n",
    "eig_data_list = []\n",
    "\n",
    "# Extract eig values from each file\n",
    "for file_path in captain_files:\n",
    "    # Get relative path for naming\n",
    "    rel_path = os.path.relpath(file_path, base_path)\n",
    "\n",
    "    # Extract round_id from path\n",
    "    # Use regex to extract the part after 'round_' in the relative path\n",
    "    match = re.search(r'round_([a-zA-Z0-9]+)', rel_path)\n",
    "    round_id = match.group(1) if match else None\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract eig values, skipping None/null values\n",
    "    for idx, datum in enumerate(data):\n",
    "        if 'eig' in datum and datum['eig'] is not None and 'question' in datum and datum['question'] is not None:\n",
    "            question_text = datum['question']['question']['text'] if datum['question']['question'] and 'text' in datum['question']['question'] else \"No question text\"\n",
    "            eig_value = datum['eig']\n",
    "\n",
    "            eig_questions = datum.get(\"eig_questions\", [])\n",
    "\n",
    "\n",
    "            if eig_questions is not None:\n",
    "                if len(eig_questions) != 0:\n",
    "                    eig_questions = [(q['question']['question']['text'],q['eig'], None) for q in eig_questions]\n",
    "                    max_eig = max([eq[1] for eq in eig_questions if eq[1] is not None])\n",
    "                    eig_questions = [(q[0], q[1], q[1] == max_eig) for q in eig_questions]\n",
    "\n",
    "            # Add to data list\n",
    "            eig_data_list.append({\n",
    "                'round_id': round_id,\n",
    "                'question_idx': idx,\n",
    "                'question': question_text,\n",
    "                'eig': eig_value,\n",
    "                'eig_questions': eig_questions,\n",
    "            })\n",
    "\n",
    "# Create DataFrame from the list\n",
    "model_eig_df = pd.DataFrame(eig_data_list)\n",
    "\n",
    "model_eig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eig_df[\"eig\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /////////////////////////////////////////////////\n",
    "# This cell calculates EIG for human questions (and saves it to notebooks/human_eig_df.csv)\n",
    "# Caution: This will take 1-2 mins to run if human_eig_df.csv doesn't exist in the notebooks directory\n",
    "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "# JSON file to pull the code translations of human questions from\n",
    "input_json_path = resolve_project_path(\"experiments/collaborative/spotter_benchmarks/o4-mini_CodeSpotterModel_True.json\")\n",
    "\n",
    "def extract_questions_and_boards_to_dataframe(json_path):\n",
    "    \"\"\"\n",
    "    Extracts all questions asked and the board state at the time they were asked from a JSON file\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the input JSON file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted questions and board states.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"The file {json_path} does not exist.\")\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    extracted_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        if \"question\" in entry and \"occTiles\" in entry:\n",
    "            question = entry[\"question\"]\n",
    "            program = entry[\"program\"]\n",
    "            board_state = entry[\"occTiles\"]\n",
    "            answer = entry[\"answer\"]\n",
    "            true_answer = entry[\"true_answer\"]\n",
    "\n",
    "            if answer.lower() == \"true\":\n",
    "                answer = \"yes\"\n",
    "            if answer.lower() == \"false\":\n",
    "                answer = \"no\"\n",
    "\n",
    "            extracted_data.append({\n",
    "                \"question\": question,\n",
    "                \"program\": program,\n",
    "                \"board_state\": board_state,\n",
    "                \"answer\": answer,\n",
    "                \"true_answer\": true_answer,\n",
    "                \"correct\": answer == true_answer\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(extracted_data)\n",
    "\n",
    "\n",
    "if os.path.exists('human_eig_df.csv'):\n",
    "    human_eig_df = pd.read_csv('human_eig_df.csv')\n",
    "else:\n",
    "    human_eig_df = extract_questions_and_boards_to_dataframe(input_json_path)\n",
    "    human_eig_df = human_eig_df[human_eig_df['correct'] == True]\n",
    "\n",
    "    eig_calculator = EIGCalculator(samples=1000, timeout=15, epsilon=0)\n",
    "\n",
    "    # Add a new column to store EIG values\n",
    "    human_eig_df[\"calculated_eig\"] = None\n",
    "\n",
    "    for idx, row in human_eig_df.iterrows():\n",
    "            # Create a CodeQuestion instance\n",
    "            code_question = CodeQuestion(\n",
    "                question=Question(row[\"question\"]),\n",
    "                fn_text=row[\"program\"],\n",
    "                translation_prompt=\"\",\n",
    "                completion={}\n",
    "            )\n",
    "\n",
    "            # Convert board_state to a Board instance\n",
    "            board = Board.from_occ_tiles(row[\"board_state\"])\n",
    "\n",
    "            # Calculate EIG\n",
    "            eig_value = eig_calculator(code_question, board)\n",
    "            human_eig_df.at[idx, \"calculated_eig\"] = eig_value\n",
    "\n",
    "    human_eig_df.to_csv('human_eig_df.csv', index=False)\n",
    "\n",
    "human_eig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'EIG Values': pd.concat([model_eig_df[\"eig\"], human_eig_df[\"calculated_eig\"]], ignore_index=True),\n",
    "    'Source': ['model_eig_df'] * len(model_eig_df) + ['human_eig_df'] * len(human_eig_df)\n",
    "})\n",
    "\n",
    "# Create a boxplot instead of a scatter plot\n",
    "sns.boxplot(data=plot_data, x='Source', y='EIG Values', palette='Set2')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Categorical Scatter Plot of EIG Values')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('EIG Values')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the average EIG values for both distributions\n",
    "avg_model_eig = model_eig_df[\"eig\"].mean()\n",
    "avg_human_eig = human_eig_df[\"calculated_eig\"].astype(float).mean()\n",
    "\n",
    "print(f\"Average EIG for eig_df: {avg_model_eig:.4f}\")\n",
    "print(f\"Average EIG for output_df: {avg_human_eig:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946b50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
