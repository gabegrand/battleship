{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e28f440",
   "metadata": {},
   "source": [
    "# Captain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from battleship.run_captain_benchmarks import rebuild_captain_summary_from_results\n",
    "from battleship.utils import resolve_project_path\n",
    "from battleship.agents import EIGCalculator, CodeQuestion, Question\n",
    "from battleship.game import Board\n",
    "\n",
    "from analysis import CAPTAIN_TYPE_LABELS, MODEL_DISPLAY_NAMES\n",
    "from analysis import human_round_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542334d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# set seaborn color palette\n",
    "sns.set_palette(\"tab10\")\n",
    "\n",
    "# set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_EXPERIMENT_NAME = \"battleship-final-data\"\n",
    "PATH_DATA = os.path.join(\"data\", HUMAN_EXPERIMENT_NAME)\n",
    "PATH_EXPORT = os.path.join(PATH_DATA, \"export\")\n",
    "\n",
    "CAPTAIN_EXPERIMENT_PATH = (\n",
    "    \"experiments/collaborative/captain_benchmarks/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0bcb23",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d1618",
   "metadata": {},
   "source": [
    "### Human data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1eaa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df = human_round_summaries(\n",
    "    experiment_path=PATH_DATA,\n",
    ")\n",
    "human_df = pd.DataFrame(human_df)\n",
    "\n",
    "human_df = human_df.assign(llm=\"Human\")\n",
    "human_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e47cd7",
   "metadata": {},
   "source": [
    "### Model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77baafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_round_data_unresolved_paths = [\n",
    "    (\"gpt-4o\", \"run_2025_08_25_16_28_19\"),\n",
    "    (\"gpt-5\", \"run_2025_08_25_22_02_29\"),\n",
    "    (\"llama-4-scout\", \"run_2025_08_26_17_56_46\"),\n",
    "    (\"Baseline\", \"run_2025_08_26_17_23_23\"),\n",
    "]\n",
    "\n",
    "model_round_data_paths = [\n",
    "    (name, resolve_project_path(os.path.join(CAPTAIN_EXPERIMENT_PATH, path)))\n",
    "    for name, path in model_round_data_unresolved_paths\n",
    "]\n",
    "for name, path in model_round_data_paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"The path {path} does not exist.\")\n",
    "\n",
    "dfs = []\n",
    "for name, path in model_round_data_paths:\n",
    "    df = pd.DataFrame(rebuild_captain_summary_from_results(path))\n",
    "    df[\"llm\"] = name\n",
    "    dfs.append(df)\n",
    "\n",
    "model_df = pd.concat(dfs, ignore_index=True)\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58184756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append summary_df to round_df\n",
    "df = pd.concat([human_df, model_df], ignore_index=True)\n",
    "\n",
    "primary_columns = [\"captain_type_display\", \"llm_display\", \"board_id\", \"seed\"]\n",
    "\n",
    "# Create categorical column for captain_type_display\n",
    "df[\"captain_type_display\"] = pd.Categorical(\n",
    "    df[\"captain_type\"].map(CAPTAIN_TYPE_LABELS),\n",
    "    categories=list(dict.fromkeys(CAPTAIN_TYPE_LABELS.values())),\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "# Create categorical column for llm_display\n",
    "df[\"llm_display\"] = pd.Categorical(\n",
    "    df[\"llm\"],\n",
    "    categories=[\"Human\", \"Baseline\"] + [x for x in MODEL_DISPLAY_NAMES.values() if x in df[\"llm\"].unique()],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "# Move primary columns to the front\n",
    "df = df[primary_columns + [col for col in df.columns if col not in primary_columns]]\n",
    "\n",
    "# Sort the DataFrame by primary columns\n",
    "df = df.sort_values(by=primary_columns, ascending=True).reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad155118",
   "metadata": {},
   "source": [
    "## Precision/Recall Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547fef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBreakdown by captain_type_display:\")\n",
    "for captain_type in df['captain_type_display'].cat.categories:\n",
    "    llms = df[df['captain_type_display'] == captain_type]['llm'].unique()\n",
    "    print(f\"{captain_type}: {llms}\")\n",
    "\n",
    "\n",
    "# Colorblind-friendly palette (Okabe–Ito)\n",
    "llm_palette = {\n",
    "    \"Human\": \"#009E73\",  # green\n",
    "    \"Baseline\": \"#0072B2\",  # blue\n",
    "    \"llama-4-scout\": \"#CC79A7\",  # purple\n",
    "    \"gpt-4o\": \"#E69F00\",  # orange (similar to gpt-5)\n",
    "    \"gpt-5\": \"#D55E00\",  # vermillion\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcce8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Firing Accuracy (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85577287",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Prepare ordered captain categories that actually appear in the data\n",
    "captain_categories = [\n",
    "    c for c in df[\"captain_type_display\"].cat.categories\n",
    "    if c in df[\"captain_type_display\"].values\n",
    "]\n",
    "\n",
    "# Determine max number of llm groups present for any captain (for consistent box widths)\n",
    "llm_counts = df.groupby(\"captain_type_display\")[\"llm\"].nunique()\n",
    "max_llms = int(llm_counts.max()) if len(llm_counts) > 0 else 1\n",
    "\n",
    "# Base positions for each captain on the x axis\n",
    "x_positions = np.arange(len(captain_categories))\n",
    "\n",
    "# Box width: leave some padding between captain groups\n",
    "group_width = 0.5  # total width occupied by boxes for one captain\n",
    "box_width = group_width / max_llms\n",
    "\n",
    "# Ensure grid lines are drawn below plot elements and only horizontal gridlines are shown\n",
    "ax.set_axisbelow(True)\n",
    "ax.xaxis.grid(False)\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# Map captain -> present llms to ensure we only plot existing combinations\n",
    "# Use the llm_display categorical ordering so order is consistent across plots\n",
    "llm_order = list(df[\"llm_display\"].cat.categories) if \"llm_display\" in df.columns else sorted(df[\"llm\"].unique())\n",
    "\n",
    "for i, captain in enumerate(captain_categories):\n",
    "    present_llms_unsorted = df[df[\"captain_type_display\"] == captain][\"llm\"].unique()\n",
    "    # Preserve the display order\n",
    "    present_llms = [llm for llm in llm_order if llm in present_llms_unsorted]\n",
    "\n",
    "    m = len(present_llms)\n",
    "    if m == 0:\n",
    "        continue\n",
    "\n",
    "    # Offsets to center m boxes around the captain x position\n",
    "    offsets = (np.arange(m) - (m - 1) / 2.0) * box_width\n",
    "\n",
    "    for j, llm in enumerate(present_llms):\n",
    "        subset = df[(df[\"captain_type_display\"] == captain) & (df[\"llm\"] == llm)][\"f1_score\"].dropna()\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        pos = x_positions[i] + offsets[j]\n",
    "        color = llm_palette.get(llm, \"#808080\")\n",
    "\n",
    "        # Use matplotlib's boxplot to place each box at the computed numeric position\n",
    "        bp = ax.boxplot(subset.values,\n",
    "                        positions=[pos],\n",
    "                        widths=box_width * 0.9,\n",
    "                        patch_artist=True,\n",
    "                        manage_ticks=False)\n",
    "\n",
    "        # Style the box elements\n",
    "        for element in [\"boxes\", \"whiskers\", \"caps\", \"medians\"]:\n",
    "            plt.setp(bp[element], color=color)\n",
    "        for patch in bp[\"boxes\"]:\n",
    "            patch.set(facecolor=color, alpha=0.6)\n",
    "\n",
    "        # Make fliers (outliers) less visually distinctive: smaller, lower-alpha, and same color as box\n",
    "        if \"fliers\" in bp:\n",
    "            for f in bp[\"fliers\"]:\n",
    "                f.set(marker='o', markersize=3, markerfacecolor=color, markeredgecolor=color, alpha=0.35, markeredgewidth=0)\n",
    "\n",
    "# Create legend handles for llm types present in the full DataFrame, in llm_display order\n",
    "from matplotlib.patches import Patch\n",
    "all_present_llms = [llm for llm in llm_order if llm in df[\"llm\"].unique()]\n",
    "legend_handles = [Patch(facecolor=llm_palette[k], label=k, alpha=0.6) for k in all_present_llms]\n",
    "\n",
    "ax.legend(handles=legend_handles, loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "# Final formatting\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(captain_categories, rotation=90)\n",
    "ax.set_xlabel(\"Captain Type\")\n",
    "ax.set_ylabel(\"Firing Accuracy (F1)\")\n",
    "ax.set_xlim(-0.5, len(captain_categories) - 0.5)\n",
    "\n",
    "sns.despine()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "plt.savefig(os.path.join(PATH_EXPORT, \"captain_f1_boxplot.pdf\"), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd10a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "sns.stripplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    alpha=0.7,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Firing Accuracy (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace22cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.swarmplot(\n",
    "    data=df,\n",
    "    x=\"captain_type_display\",\n",
    "    y=\"f1_score\",\n",
    "    hue=\"llm\",\n",
    "    palette=llm_palette,\n",
    "    # alpha=0.7,\n",
    "    ax=ax,\n",
    ")\n",
    "sns.despine()\n",
    "\n",
    "plt.xlabel(\"Captain Type\")\n",
    "plt.ylabel(\"Firing Accuracy (F1)\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1), title=\"\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d45249",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(\n",
    "    data=df,\n",
    "    kind=\"ecdf\",\n",
    "    x=\"f1_score\",\n",
    "    hue=\"captain_type_display\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3115c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"move_count\"] = df[\"hits\"] + df[\"misses\"]\n",
    "\n",
    "sns.barplot(data=df, x=\"captain_type\", y=\"move_count\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da997ec9",
   "metadata": {},
   "source": [
    "## Win rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280902a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby([\"captain_type_display\", \"llm_display\"])[\"board_id\"].count()\n",
    "counts_nonzero = counts[counts > 0].reset_index(name=\"count\")\n",
    "counts_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40cb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f463cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Win Rate Computation Utilities ---\n",
    "from itertools import product, combinations\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "# Create a composite competitor identifier (LLM first, then captain type)\n",
    "# This treats each (llm_display, captain_type_display) pair as a distinct competitor.\n",
    "df[\"competitor\"] = (\n",
    "    df[\"llm_display\"].astype(str)\n",
    "    + \" | \"\n",
    "    + df[\"captain_type_display\"].astype(str)\n",
    ")\n",
    "\n",
    "# Enforce categorical ordering: iterate LLMs (primary) then captain types (secondary) in their existing category orders.\n",
    "competitor_order = []\n",
    "llm_categories = list(df[\"llm_display\"].cat.categories) if isinstance(df[\"llm_display\"].dtype, pd.CategoricalDtype) else sorted(df[\"llm_display\"].dropna().unique())\n",
    "cap_categories = list(df[\"captain_type_display\"].cat.categories) if isinstance(df[\"captain_type_display\"].dtype, pd.CategoricalDtype) else sorted(df[\"captain_type_display\"].dropna().unique())\n",
    "for llm in llm_categories:\n",
    "    # Which captain types exist for this llm\n",
    "    present_caps = [c for c in cap_categories if ((df[\"llm_display\"] == llm) & (df[\"captain_type_display\"] == c)).any()]\n",
    "    for cap in present_caps:\n",
    "        competitor_order.append(f\"{llm} | {cap}\")\n",
    "\n",
    "df[\"competitor\"] = pd.Categorical(df[\"competitor\"], categories=competitor_order, ordered=True)\n",
    "\n",
    "\n",
    "def compute_board_level_win_rate(a_vals, b_vals, higher_is_better=True):\n",
    "    \"\"\"Compute win rate of A over B for all pairwise comparisons of metric values.\n",
    "    Ties count as 0.5 wins. Returns (win_rate, wins, comparisons).\"\"\"\n",
    "    wins = 0.0\n",
    "    comparisons = 0\n",
    "    for a, b in product(a_vals, b_vals):\n",
    "        if pd.isna(a) or pd.isna(b):\n",
    "            continue\n",
    "        comparisons += 1\n",
    "        if higher_is_better:\n",
    "            if a > b:\n",
    "                wins += 1\n",
    "            elif a == b:\n",
    "                wins += 0.5\n",
    "        else:  # lower is better\n",
    "            if a < b:\n",
    "                wins += 1\n",
    "            elif a == b:\n",
    "                wins += 0.5\n",
    "    if comparisons == 0:\n",
    "        return np.nan, wins, comparisons\n",
    "    return wins / comparisons, wins, comparisons\n",
    "\n",
    "\n",
    "def compute_pairwise_win_rates(\n",
    "    df,\n",
    "    metric=\"f1_score\",\n",
    "    higher_is_better=True,\n",
    "    captain_col=\"captain_type_display\",\n",
    "    board_col=\"board_id\",\n",
    "    round_col=\"round_id\",\n",
    "):\n",
    "    \"\"\"Compute pairwise win rates between all competitors (captain_col values).\n",
    "\n",
    "    For each pair of competitors (A,B) and each board, take the cross product of all rounds\n",
    "    (identified by board_id + round_id) for A vs B and compute the proportion of wins.\n",
    "    A board-level win rate is that proportion (ties=0.5 wins).\n",
    "\n",
    "    Two aggregate views:\n",
    "      * mean_board_win_rate: unweighted mean over boards with ≥1 round for both competitors.\n",
    "      * weighted_all_pairs_win_rate: total wins / total pairwise comparisons (pooled).\n",
    "    \"\"\"\n",
    "    if metric not in df.columns:\n",
    "        raise ValueError(f\"Metric '{metric}' not found in DataFrame columns.\")\n",
    "\n",
    "    competitors = [c for c in df[captain_col].dropna().unique()]\n",
    "    boards = sorted(df[board_col].dropna().unique())\n",
    "\n",
    "    records = []\n",
    "    grouped = df.groupby([captain_col, board_col])\n",
    "\n",
    "    for ca, cb in combinations(competitors, 2):\n",
    "        board_results = []\n",
    "        total_wins = 0.0\n",
    "        total_comparisons = 0\n",
    "        boards_considered = 0\n",
    "        for board in boards:\n",
    "            try:\n",
    "                a_group = grouped.get_group((ca, board))\n",
    "            except KeyError:\n",
    "                a_group = pd.DataFrame(columns=df.columns)\n",
    "            try:\n",
    "                b_group = grouped.get_group((cb, board))\n",
    "            except KeyError:\n",
    "                b_group = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "            a_vals = a_group[metric].dropna().values\n",
    "            b_vals = b_group[metric].dropna().values\n",
    "            if len(a_vals) == 0 or len(b_vals) == 0:\n",
    "                continue\n",
    "\n",
    "            board_win_rate, wins, comps = compute_board_level_win_rate(\n",
    "                a_vals, b_vals, higher_is_better=higher_is_better\n",
    "            )\n",
    "            if not np.isnan(board_win_rate):\n",
    "                board_results.append((board, board_win_rate, wins, comps))\n",
    "                total_wins += wins\n",
    "                total_comparisons += comps\n",
    "                boards_considered += 1\n",
    "\n",
    "        if boards_considered == 0:\n",
    "            mean_board_win_rate = np.nan\n",
    "            weighted_all_pairs_win_rate = np.nan\n",
    "        else:\n",
    "            mean_board_win_rate = (\n",
    "                np.nanmean([br for _, br, _, _ in board_results]) if board_results else np.nan\n",
    "            )\n",
    "            weighted_all_pairs_win_rate = (\n",
    "                total_wins / total_comparisons if total_comparisons > 0 else np.nan\n",
    "            )\n",
    "\n",
    "        for board, br, wins, comps in board_results:\n",
    "            records.append(\n",
    "                {\n",
    "                    \"competitor_a\": ca,\n",
    "                    \"competitor_b\": cb,\n",
    "                    \"metric\": metric,\n",
    "                    \"higher_is_better\": higher_is_better,\n",
    "                    \"board_id\": board,\n",
    "                    \"board_win_rate\": br,\n",
    "                    \"board_wins\": wins,\n",
    "                    \"board_comparisons\": comps,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"competitor_a\": ca,\n",
    "                \"competitor_b\": cb,\n",
    "                \"metric\": metric,\n",
    "                \"higher_is_better\": higher_is_better,\n",
    "                \"board_id\": None,\n",
    "                \"board_win_rate\": mean_board_win_rate,\n",
    "                \"board_wins\": total_wins,\n",
    "                \"board_comparisons\": total_comparisons,\n",
    "                \"weighted_all_pairs_win_rate\": weighted_all_pairs_win_rate,\n",
    "                \"boards_considered\": boards_considered,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    detailed_df = pd.DataFrame(records)\n",
    "    aggregate_df = detailed_df[detailed_df[\"board_id\"].isna()].copy()\n",
    "    aggregate_df = aggregate_df.assign(\n",
    "        mean_board_win_rate=aggregate_df[\"board_win_rate\"],\n",
    "        weighted_all_pairs_win_rate=aggregate_df[\"weighted_all_pairs_win_rate\"].fillna(\n",
    "            aggregate_df[\"board_wins\"] / aggregate_df[\"board_comparisons\"].replace({0: np.nan})\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Preserve original categorical order if available\n",
    "    if pd.api.types.is_categorical_dtype(df[captain_col]):\n",
    "        competitors_sorted = [c for c in df[captain_col].cat.categories if c in competitors]\n",
    "    else:\n",
    "        competitors_sorted = sorted(competitors)\n",
    "\n",
    "    mean_matrix = pd.DataFrame(np.nan, index=competitors_sorted, columns=competitors_sorted)\n",
    "    weighted_matrix = pd.DataFrame(np.nan, index=competitors_sorted, columns=competitors_sorted)\n",
    "\n",
    "    for _, row in aggregate_df.iterrows():\n",
    "        ca, cb = row[\"competitor_a\"], row[\"competitor_b\"]\n",
    "        mean_ab = row[\"mean_board_win_rate\"]\n",
    "        weighted_ab = row[\"weighted_all_pairs_win_rate\"]\n",
    "        if ca in mean_matrix.index and cb in mean_matrix.columns:\n",
    "            mean_matrix.loc[ca, cb] = mean_ab\n",
    "            weighted_matrix.loc[ca, cb] = weighted_ab\n",
    "            if not pd.isna(mean_ab):\n",
    "                mean_matrix.loc[cb, ca] = 1 - mean_ab\n",
    "            if not pd.isna(weighted_ab):\n",
    "                weighted_matrix.loc[cb, ca] = 1 - weighted_ab\n",
    "            mean_matrix.loc[ca, ca] = 0.5\n",
    "            mean_matrix.loc[cb, ca] = mean_matrix.loc[cb, ca]\n",
    "            weighted_matrix.loc[ca, ca] = 0.5\n",
    "            weighted_matrix.loc[cb, ca] = weighted_matrix.loc[cb, ca]\n",
    "\n",
    "    return {\n",
    "        \"detailed\": detailed_df,\n",
    "        \"aggregate\": aggregate_df,\n",
    "        \"mean_board_win_rate_matrix\": mean_matrix,\n",
    "        \"weighted_win_rate_matrix\": weighted_matrix,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Run win rate analysis for F1 (higher better) at competitor (LLM+captain) granularity ---\n",
    "win_results_f1_comp = compute_pairwise_win_rates(\n",
    "    df, metric=\"f1_score\", higher_is_better=True, captain_col=\"competitor\"\n",
    ")\n",
    "print(\"Mean board win rate matrix (F1, competitor-level):\")\n",
    "display(win_results_f1_comp[\"mean_board_win_rate_matrix\"])\n",
    "print(\"Weighted all-pairs win rate matrix (F1, competitor-level):\")\n",
    "display(win_results_f1_comp[\"weighted_win_rate_matrix\"])\n",
    "\n",
    "# --- Run win rate analysis for move count (lower better) ---\n",
    "if \"move_count\" in df.columns:\n",
    "    win_results_moves_comp = compute_pairwise_win_rates(\n",
    "        df, metric=\"move_count\", higher_is_better=False, captain_col=\"competitor\"\n",
    "    )\n",
    "    print(\"Mean board win rate matrix (Move Count, competitor-level):\")\n",
    "    display(win_results_moves_comp[\"mean_board_win_rate_matrix\"])\n",
    "    print(\"Weighted all-pairs win rate matrix (Move Count, competitor-level):\")\n",
    "    display(win_results_moves_comp[\"weighted_win_rate_matrix\"])\n",
    "else:\n",
    "    print(\"Column 'move_count' not found; skip move-count win rates.\")\n",
    "\n",
    "# Aggregate summary for F1\n",
    "f1_comp_summary = win_results_f1_comp[\"aggregate\"][ [\n",
    "    \"competitor_a\", \"competitor_b\", \"mean_board_win_rate\", \"weighted_all_pairs_win_rate\", \"boards_considered\", \"board_wins\", \"board_comparisons\"\n",
    "] ].sort_values([\"competitor_a\", \"competitor_b\"]).reset_index(drop=True)\n",
    "print(\"Pairwise aggregate win rates (F1, competitor-level):\")\n",
    "display(f1_comp_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of F1 Weighted All-Pairs Win Rates (competitor-level)\n",
    "weighted_matrix = win_results_f1_comp[\"weighted_win_rate_matrix\"].copy()\n",
    "\n",
    "# Ensure numeric dtype\n",
    "weighted_matrix = weighted_matrix.astype(float)\n",
    "\n",
    "# Optionally mask the lower triangle (since it's redundant: win(B,A)=1-win(A,B))\n",
    "show_full = True\n",
    "if not show_full:\n",
    "    mask = np.tril(np.ones_like(weighted_matrix, dtype=bool), k=-1)\n",
    "else:\n",
    "    mask = None\n",
    "\n",
    "figsize_base = 0.55  # width/height scaling per competitor\n",
    "n = len(weighted_matrix)\n",
    "fig, ax = plt.subplots(figsize=(min(24, 2 + n * figsize_base), min(24, 2 + n * figsize_base)))\n",
    "\n",
    "# Dynamic annotation font size: shrink as matrix grows\n",
    "annot_font_size = max(6, min(12, 120 / max(n, 1)))\n",
    "\n",
    "sns.heatmap(\n",
    "    weighted_matrix,\n",
    "    mask=mask,\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    center=0.5,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    annot_kws={\"size\": annot_font_size},\n",
    "    linewidths=0.4,\n",
    "    linecolor=\"white\",\n",
    "    cbar_kws={\"label\": \"Win rate (row beats column)\"},\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "ax.set_title(\"F1 Weighted All-Pairs Win Rates (Row beats Column)\", fontsize=14)\n",
    "ax.set_xlabel(\"Opponent (Column)\", fontsize=12)\n",
    "ax.set_ylabel(\"Competitor (Row)\", fontsize=12)\n",
    "plt.xticks(rotation=90, fontsize=9)\n",
    "plt.yticks(rotation=0, fontsize=9)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Export\n",
    "os.makedirs(PATH_EXPORT, exist_ok=True)\n",
    "heatmap_path = os.path.join(PATH_EXPORT, \"f1_weighted_winrate_heatmap.pdf\")\n",
    "plt.savefig(heatmap_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved heatmap to {heatmap_path}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined grouped heatmap: clearer LLM separation, captain-only tick labels, centered vertical LLM group labels,\n",
    "# no legend, tighter colorbar, optional within-group clustering, optional column shading.\n",
    "\n",
    "import colorsys\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "base_matrix = win_results_f1_comp[\"weighted_win_rate_matrix\"].copy().astype(float)\n",
    "\n",
    "# Config\n",
    "show_group_separators = True\n",
    "shade_group_rows = True\n",
    "shade_group_cols = True          # also lightly shade column bands\n",
    "annotate_group_labels = True\n",
    "group_label_rotation = 90        # vertical to save horizontal space\n",
    "group_label_color = \"#222222\"\n",
    "captain_tick_fontsize = 6\n",
    "group_label_fontsize = 10\n",
    "row_shade_alpha = 0.30\n",
    "col_shade_alpha = 0.18\n",
    "\n",
    "llm_group_palette = {k: v for k, v in llm_palette.items()}\n",
    "\n",
    "# Parse competitor into (LLM, Captain)\n",
    "def split_comp(c):\n",
    "    parts = c.split(\"|\", 1)\n",
    "    if len(parts) == 2:\n",
    "        return parts[0].strip(), parts[1].strip()\n",
    "    return c.strip(), \"\"\n",
    "\n",
    "competitors = list(base_matrix.index)\n",
    "competitor_llm = {c: split_comp(c)[0] for c in competitors}\n",
    "competitor_captain = {c: split_comp(c)[1] for c in competitors}\n",
    "\n",
    "# Build contiguous LLM blocks (current ordering already LLM-primary)\n",
    "blocks = []\n",
    "current_llm, current_block = None, []\n",
    "for c in competitors:\n",
    "    llm = competitor_llm[c]\n",
    "    if llm != current_llm:\n",
    "        if current_block:\n",
    "            blocks.append((current_llm, current_block))\n",
    "        current_llm = llm\n",
    "        current_block = [c]\n",
    "    else:\n",
    "        current_block.append(c)\n",
    "if current_block:\n",
    "    blocks.append((current_llm, current_block))\n",
    "\n",
    "# New order\n",
    "new_order = [c for _, comps in blocks for c in comps]\n",
    "mat = base_matrix.loc[new_order, new_order]\n",
    "\n",
    "# Helper to lighten color\n",
    "def lighten(hex_color, factor=0.9):\n",
    "    try:\n",
    "        hex_color = hex_color.lstrip('#')\n",
    "        r, g, b = [int(hex_color[i:i+2], 16) for i in (0, 2, 4)]\n",
    "        h, l, s = colorsys.rgb_to_hls(r/255, g/255, b/255)\n",
    "        l = 1 - (1 - l) * factor\n",
    "        r2, g2, b2 = colorsys.hls_to_rgb(h, l, s)\n",
    "        return f\"#{int(r2*255):02x}{int(g2*255):02x}{int(b2*255):02x}\"\n",
    "    except Exception:\n",
    "        return \"#f0f0f0\"\n",
    "\n",
    "row_shade_colors = {llm: lighten(llm_group_palette.get(llm, '#888888'), 0.93) for llm, _ in blocks}\n",
    "\n",
    "n = len(mat)\n",
    "figsize_base = 0.50\n",
    "fig, ax = plt.subplots(figsize=(min(24, 1.4 + n * figsize_base), min(24, 1.4 + n * figsize_base)))\n",
    "annot_font_size = max(6, min(12, 120 / max(n, 1)))\n",
    "\n",
    "hm = sns.heatmap(\n",
    "    mat,\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=0, vmax=1, center=0.5,\n",
    "    annot=True, fmt=\".2f\",\n",
    "    annot_kws={\"size\": annot_font_size},\n",
    "    linewidths=0.4, linecolor=\"white\",\n",
    "    cbar_kws={\"shrink\": 0.6, \"pad\": 0.02},\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Tick labels: captain type only\n",
    "captain_labels = [competitor_captain[c] for c in new_order]\n",
    "ax.set_xticks(np.arange(n) + 0.5)\n",
    "ax.set_yticks(np.arange(n) + 0.5)\n",
    "ax.set_xticklabels(captain_labels, rotation=90, ha='center', va='top', fontsize=captain_tick_fontsize)\n",
    "ax.set_yticklabels(captain_labels, rotation=0, ha='right', va='center', fontsize=captain_tick_fontsize)\n",
    "\n",
    "# Remove default axis labels (we'll use custom title)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "\n",
    "# Shading rows\n",
    "if shade_group_rows:\n",
    "    for llm, comps in blocks:\n",
    "        start = new_order.index(comps[0])\n",
    "        end = new_order.index(comps[-1])\n",
    "        h = end - start + 1\n",
    "        ax.add_patch(\n",
    "            Rectangle(\n",
    "                (0, start),\n",
    "                width=n,\n",
    "                height=h,\n",
    "                facecolor=row_shade_colors[llm],\n",
    "                edgecolor='none',\n",
    "                alpha=row_shade_alpha,\n",
    "                zorder=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Shading columns (lighter)\n",
    "if shade_group_cols:\n",
    "    for llm, comps in blocks:\n",
    "        start = new_order.index(comps[0])\n",
    "        end = new_order.index(comps[-1])\n",
    "        w = end - start + 1\n",
    "        ax.add_patch(\n",
    "            Rectangle(\n",
    "                (start, 0),\n",
    "                width=w,\n",
    "                height=n,\n",
    "                facecolor=row_shade_colors[llm],\n",
    "                edgecolor='none',\n",
    "                alpha=col_shade_alpha,\n",
    "                zorder=0\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Redraw heatmap mesh above shading\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Group separators\n",
    "if show_group_separators:\n",
    "    cum = 0\n",
    "    for llm, comps in blocks:\n",
    "        size = len(comps)\n",
    "        cum += size\n",
    "        if cum < n:\n",
    "            ax.axhline(cum, color='black', linewidth=2)\n",
    "            ax.axvline(cum, color='black', linewidth=2)\n",
    "\n",
    "# Group labels (centered, vertical above; vertical lines already separate)\n",
    "if annotate_group_labels:\n",
    "    cum_start = 0\n",
    "    for llm, comps in blocks:\n",
    "        size = len(comps)\n",
    "        midpoint = cum_start + size / 2\n",
    "        # Column (top) label\n",
    "        ax.text(\n",
    "            midpoint,\n",
    "            n + 1.0,  # slightly below the matrix\n",
    "            llm,\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            rotation=0,\n",
    "            fontsize=group_label_fontsize,\n",
    "            fontweight=\"bold\",\n",
    "            color=llm_group_palette.get(llm, group_label_color),\n",
    "            clip_on=False,\n",
    "        )\n",
    "        # Row (left) label (rotated primary label)\n",
    "        ax.text(\n",
    "            -1.0,\n",
    "            midpoint,\n",
    "            llm,\n",
    "            ha=\"right\",\n",
    "            va=\"center\",\n",
    "            rotation=group_label_rotation,  # was 0; rotate 90°\n",
    "            fontsize=group_label_fontsize,\n",
    "            fontweight=\"bold\",\n",
    "            color=llm_group_palette.get(llm, group_label_color),\n",
    "            clip_on=False,\n",
    "        )\n",
    "        cum_start += size\n",
    "\n",
    "# Adjust colorbar (already shrunk)\n",
    "cbar = hm.collections[0].colorbar\n",
    "cbar.ax.set_title(\"Win rate\\n(row > col)\", fontsize=9, pad=6)\n",
    "cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "# ax.set_title(f\"F1 Weighted Win Rates (Grouped by LLM)\", fontsize=14, pad=12)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Export\n",
    "os.makedirs(PATH_EXPORT, exist_ok=True)\n",
    "out_path = os.path.join(PATH_EXPORT, f\"f1_weighted_winrate_heatmap_grouped_refined.pdf\")\n",
    "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(f\"Saved refined grouped heatmap to {out_path} (annotation font size={annot_font_size})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d20f64d",
   "metadata": {},
   "source": [
    "## EIG Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to run directory\n",
    "# base_path = resolve_project_path(\"experiments/collaborative/captain_benchmarks/run_combined/run_4o_mapeig_cot_captain\")\n",
    "base_path = resolve_project_path(\n",
    "    \"experiments/collaborative/captain_benchmarks/run_combined/run_4o_llmdecision_captain\"\n",
    ")\n",
    "\n",
    "# Find all captain.json files in subdirectories\n",
    "captain_files = glob.glob(os.path.join(base_path, '**/captain/captain.json'), recursive=True)\n",
    "\n",
    "# Dictionary to store eig values by file\n",
    "eig_values_by_file = {}\n",
    "# Initialize list to store data for DataFrame\n",
    "eig_data_list = []\n",
    "\n",
    "# Extract eig values from each file\n",
    "for file_path in captain_files:\n",
    "    # Get relative path for naming\n",
    "    rel_path = os.path.relpath(file_path, base_path)\n",
    "\n",
    "    # Extract round_id from path\n",
    "    # Use regex to extract the part after 'round_' in the relative path\n",
    "    match = re.search(r'round_([a-zA-Z0-9]+)', rel_path)\n",
    "    round_id = match.group(1) if match else None\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract eig values, skipping None/null values\n",
    "    for idx, datum in enumerate(data):\n",
    "        if 'eig' in datum and datum['eig'] is not None and 'question' in datum and datum['question'] is not None:\n",
    "            question_text = datum['question']['question']['text'] if datum['question']['question'] and 'text' in datum['question']['question'] else \"No question text\"\n",
    "            eig_value = datum['eig']\n",
    "\n",
    "            eig_questions = datum.get(\"eig_questions\", [])\n",
    "\n",
    "\n",
    "            if eig_questions is not None:\n",
    "                if len(eig_questions) != 0:\n",
    "                    eig_questions = [(q['question']['question']['text'],q['eig'], None) for q in eig_questions]\n",
    "                    max_eig = max([eq[1] for eq in eig_questions if eq[1] is not None])\n",
    "                    eig_questions = [(q[0], q[1], q[1] == max_eig) for q in eig_questions]\n",
    "\n",
    "            # Add to data list\n",
    "            eig_data_list.append({\n",
    "                'round_id': round_id,\n",
    "                'question_idx': idx,\n",
    "                'question': question_text,\n",
    "                'eig': eig_value,\n",
    "                'eig_questions': eig_questions,\n",
    "            })\n",
    "\n",
    "# Create DataFrame from the list\n",
    "model_eig_df = pd.DataFrame(eig_data_list)\n",
    "\n",
    "model_eig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eig_df[\"eig\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /////////////////////////////////////////////////\n",
    "# This cell calculates EIG for human questions (and saves it to notebooks/human_eig_df.csv)\n",
    "# Caution: This will take 1-2 mins to run if human_eig_df.csv doesn't exist in the notebooks directory\n",
    "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "# JSON file to pull the code translations of human questions from\n",
    "input_json_path = resolve_project_path(\"experiments/collaborative/spotter_benchmarks/o4-mini_CodeSpotterModel_True.json\")\n",
    "\n",
    "def extract_questions_and_boards_to_dataframe(json_path):\n",
    "    \"\"\"\n",
    "    Extracts all questions asked and the board state at the time they were asked from a JSON file\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the input JSON file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted questions and board states.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"The file {json_path} does not exist.\")\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    extracted_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        if \"question\" in entry and \"occTiles\" in entry:\n",
    "            question = entry[\"question\"]\n",
    "            program = entry[\"program\"]\n",
    "            board_state = entry[\"occTiles\"]\n",
    "            answer = entry[\"answer\"]\n",
    "            true_answer = entry[\"true_answer\"]\n",
    "\n",
    "            if answer.lower() == \"true\":\n",
    "                answer = \"yes\"\n",
    "            if answer.lower() == \"false\":\n",
    "                answer = \"no\"\n",
    "\n",
    "            extracted_data.append({\n",
    "                \"question\": question,\n",
    "                \"program\": program,\n",
    "                \"board_state\": board_state,\n",
    "                \"answer\": answer,\n",
    "                \"true_answer\": true_answer,\n",
    "                \"correct\": answer == true_answer\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(extracted_data)\n",
    "\n",
    "\n",
    "if os.path.exists('human_eig_df.csv'):\n",
    "    human_eig_df = pd.read_csv('human_eig_df.csv')\n",
    "else:\n",
    "    human_eig_df = extract_questions_and_boards_to_dataframe(input_json_path)\n",
    "    human_eig_df = human_eig_df[human_eig_df['correct'] == True]\n",
    "\n",
    "    eig_calculator = EIGCalculator(samples=1000, timeout=15, epsilon=0)\n",
    "\n",
    "    # Add a new column to store EIG values\n",
    "    human_eig_df[\"calculated_eig\"] = None\n",
    "\n",
    "    for idx, row in human_eig_df.iterrows():\n",
    "            # Create a CodeQuestion instance\n",
    "            code_question = CodeQuestion(\n",
    "                question=Question(row[\"question\"]),\n",
    "                fn_text=row[\"program\"],\n",
    "                translation_prompt=\"\",\n",
    "                completion={}\n",
    "            )\n",
    "\n",
    "            # Convert board_state to a Board instance\n",
    "            board = Board.from_occ_tiles(row[\"board_state\"])\n",
    "\n",
    "            # Calculate EIG\n",
    "            eig_value = eig_calculator(code_question, board)\n",
    "            human_eig_df.at[idx, \"calculated_eig\"] = eig_value\n",
    "\n",
    "    human_eig_df.to_csv('human_eig_df.csv', index=False)\n",
    "\n",
    "human_eig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'EIG Values': pd.concat([model_eig_df[\"eig\"], human_eig_df[\"calculated_eig\"]], ignore_index=True),\n",
    "    'Source': ['model_eig_df'] * len(model_eig_df) + ['human_eig_df'] * len(human_eig_df)\n",
    "})\n",
    "\n",
    "# Create a boxplot instead of a scatter plot\n",
    "sns.boxplot(data=plot_data, x='Source', y='EIG Values', palette='Set2')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Categorical Scatter Plot of EIG Values')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('EIG Values')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the average EIG values for both distributions\n",
    "avg_model_eig = model_eig_df[\"eig\"].mean()\n",
    "avg_human_eig = human_eig_df[\"calculated_eig\"].astype(float).mean()\n",
    "\n",
    "print(f\"Average EIG for eig_df: {avg_model_eig:.4f}\")\n",
    "print(f\"Average EIG for output_df: {avg_human_eig:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c946b50e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battleship-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
