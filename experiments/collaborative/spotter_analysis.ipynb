{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f139a8d",
   "metadata": {},
   "source": [
    "# Spotter Benchmark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.transforms as mtransforms\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from experiments.collaborative.analysis import (\n",
    "    load_dataset,\n",
    "    get_gold_answer_dataset,\n",
    "    MODEL_DISPLAY_NAMES,\n",
    "    get_spotter_type_short,\n",
    ")\n",
    "from battleship.run_spotter_benchmarks import rebuild_summary_from_results\n",
    "from battleship.utils import PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b9c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# set seaborn color palette\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b45d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "\n",
    "# Set the default font to DejaVu Sans\n",
    "# plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# Print current font family settings\n",
    "print(\"Current Font Settings:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Default font family: {plt.rcParams['font.family']}\")\n",
    "print(f\"Sans-serif fonts: {plt.rcParams['font.sans-serif']}\")\n",
    "print(f\"Serif fonts: {plt.rcParams['font.serif']}\")\n",
    "print(f\"Monospace fonts: {plt.rcParams['font.monospace']}\")\n",
    "print(f\"Cursive fonts: {plt.rcParams['font.cursive']}\")\n",
    "print(f\"Fantasy fonts: {plt.rcParams['font.fantasy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"battleship-final-data\"\n",
    "PATH_DATA = os.path.join(\"data\", EXPERIMENT_NAME)\n",
    "\n",
    "# PATH_EXPORT = os.path.join(PATH_DATA, \"export\")\n",
    "PATH_EXPORT = os.path.join(\n",
    "    PROJECT_ROOT, \"..\", \"battleship-iclr-2026\", \"iclr2026\", \"_figures_staging\"\n",
    ")  # Export directly into the paper draft\n",
    "\n",
    "df_gold = load_dataset(experiment_path=PATH_DATA, use_gold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e0dbf",
   "metadata": {},
   "source": [
    "## Human results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6003901",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels, human_labels = get_gold_answer_dataset(df_gold)\n",
    "print(len(gold_labels), len(human_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=gold_labels, y_pred=human_labels))\n",
    "\n",
    "human_accuracy_baseline = classification_report(y_true=gold_labels, y_pred=human_labels, output_dict=True)[\"accuracy\"]\n",
    "print(f\"Human accuracy baseline: {human_accuracy_baseline:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f483c",
   "metadata": {},
   "source": [
    "## Modeling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_IDS = [\n",
    "    \"run_2025_07_11_18_32_51\",\n",
    "    \"run_2025_08_22_09_53_20\", # GPT-5\n",
    "]\n",
    "\n",
    "results = [rebuild_summary_from_results(os.path.join(\"spotter_benchmarks\", run_id)) for run_id in RUN_IDS]\n",
    "df = pd.concat([pd.DataFrame(result) for result in results]).reset_index(drop=True)\n",
    "\n",
    "# Add display names and categorizations for analysis\n",
    "def add_display_fields(df):\n",
    "    \"\"\"Add display names and categorizations to the dataframe.\"\"\"\n",
    "    # Add spotter type categorization\n",
    "    df[\"spotter_type_short\"] = df.apply(\n",
    "        lambda row: get_spotter_type_short(row[\"spotter_type\"], row[\"use_cot\"]), axis=1\n",
    "    )\n",
    "    df[\"spotter_type_short\"] = pd.Categorical(\n",
    "        df[\"spotter_type_short\"],\n",
    "        categories=[\"Base\", \"CoT\", \"Code\", \"CoT + Code\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "    # Add model display name\n",
    "    df[\"llm_display_name\"] = df[\"llm\"].map(lambda x: MODEL_DISPLAY_NAMES.get(x, x))\n",
    "\n",
    "    df[\"llm_provider\"] = df[\"llm\"].map(lambda x: x.split(\"/\")[0] if \"/\" in x else None)\n",
    "\n",
    "    # Sort by order in MODEL_DISPLAY_NAMES using categorical\n",
    "    df[\"llm_display_name\"] = pd.Categorical(\n",
    "        df[\"llm_display_name\"], categories=[display_name for llm, display_name in MODEL_DISPLAY_NAMES.items() if llm in df[\"llm\"].unique()], ordered=True\n",
    "    )\n",
    "    df = df.sort_values(by=[\"llm_display_name\", \"spotter_type_short\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Process the dataframe\n",
    "df = add_display_fields(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44867f1",
   "metadata": {},
   "source": [
    "### Completion status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    count_df = df.groupby([\"llm\", \"spotter_type_short\"], observed=False).size().to_frame(name=\"count\")\n",
    "    display(count_df)\n",
    "\n",
    "    filtered_count_df = count_df[count_df[\"count\"] < 948]\n",
    "    if len(filtered_count_df) > 0:\n",
    "        print(f\"Incomplete models:\")\n",
    "        display(filtered_count_df)\n",
    "    else:\n",
    "        print(\"All models complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(filtered_count_df.reset_index().llm.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of completion status\n",
    "count_df = df.groupby([\"llm\", \"spotter_type_short\"], observed=False).size().to_frame(name=\"count\")\n",
    "\n",
    "# Create a pivot table for heatmap\n",
    "pivot_df = count_df.reset_index().pivot(index=\"llm\", columns=\"spotter_type_short\", values=\"count\")\n",
    "\n",
    "# Create completion status (1 for complete, 0 for incomplete)\n",
    "completion_df = (pivot_df == 948).astype(int)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.heatmap(\n",
    "    completion_df,\n",
    "    annot=pivot_df,  # Show actual counts as annotations\n",
    "    fmt='d',\n",
    "    cmap='RdYlGn',\n",
    "    cbar_kws={'label': 'Completion Status'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='white'\n",
    ")\n",
    "\n",
    "plt.title('Spotter Benchmark Completion Status\\n(Green = Complete [948], Red = Incomplete)', fontsize=14)\n",
    "plt.xlabel('Spotter Type', fontsize=12)\n",
    "plt.ylabel('LLM Model', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a2be7",
   "metadata": {},
   "source": [
    "### Answer value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of raw answer text\n",
    "df[\"answer_text\"].value_counts(dropna=False).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe and handle None values in answer_value\n",
    "df_plot = df.copy()\n",
    "df_plot[\"answer_value\"] = df_plot[\"answer_value\"].fillna(\"No Answer\")\n",
    "\n",
    "# Visualize the distribution of answer values by LLM and spotter type\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(\n",
    "    data=df_plot,\n",
    "    x=\"llm_display_name\",\n",
    "    hue=\"answer_value\",\n",
    "    order=df[\"llm_display_name\"].cat.categories,\n",
    "    palette={True: \"green\", False: \"red\", \"No Answer\": \"gray\"}\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Answer Values by LLM\")\n",
    "plt.xlabel(\"LLM Display Name\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90, ha=\"right\")\n",
    "plt.legend(title=\"Answer Value\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f233e5",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_legend = False\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x=\"is_correct\",\n",
    "        y=\"llm_display_name\",\n",
    "        hue=\"spotter_type_short\",\n",
    "        errorbar=(\"ci\", 95),\n",
    "        err_kws={\n",
    "            \"color\": \"gray\",\n",
    "            \"linewidth\": 1,\n",
    "        },\n",
    "        capsize=0.2,\n",
    "        legend=show_legend,\n",
    "    )\n",
    "\n",
    "    plt.axvline(\n",
    "        human_accuracy_baseline,\n",
    "        color=\"#4b4f73\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2.0,\n",
    "        label=\"Human Performance\",\n",
    "    )\n",
    "\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlabel(\"Accuracy\")\n",
    "\n",
    "    plt.xlim(0.5, 1.0)\n",
    "    # plt.axvline(\n",
    "    #     0.5,\n",
    "    #     color=\"#808080\",\n",
    "    #     linestyle=\"--\",\n",
    "    #     linewidth=2.0,\n",
    "    #     label=\"Random Performance\",\n",
    "    # )\n",
    "\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    if show_legend:\n",
    "        plt.legend(title=\"Spotter Models\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(PATH_EXPORT, \"spotter_accuracy_by_model.pdf\"),\n",
    "        bbox_inches=\"tight\",\n",
    "        dpi=300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddd8d1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Spotter Type Performance Across All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot showing mean accuracy with confidence intervals for each spotter type\n",
    "# Calculate mean accuracy for each spotter type across all models\n",
    "spotter_accuracy = df.groupby([\"spotter_type_short\", \"llm_display_name\"])[\"is_correct\"].mean().reset_index()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=spotter_accuracy,\n",
    "    x=\"spotter_type_short\",\n",
    "    y=\"is_correct\",\n",
    "    hue=\"spotter_type_short\",\n",
    "    errorbar=(\"ci\", 95),\n",
    "    err_kws={\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    capsize=0.1,\n",
    ")\n",
    "plt.axhline(\n",
    "    human_accuracy_baseline,\n",
    "    color=\"#4b4f73\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Human Performance\"\n",
    ")\n",
    "\n",
    "ax = plt.gca()\n",
    "trans = mtransforms.blended_transform_factory(ax.transAxes, ax.transData)\n",
    "\n",
    "plt.text(\n",
    "    s=\"Human Performance\",\n",
    "    x=0.5,\n",
    "    y=human_accuracy_baseline + 0.01,\n",
    "    fontsize=14,\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    "    transform=trans,\n",
    ")\n",
    "# plt.title(\"Mean Accuracy by Spotter Type\\n(Averaged Across All Models)\", fontsize=14)\n",
    "plt.xlabel(\"\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy\", fontsize=16)\n",
    "plt.ylim(0.5, 1.0)\n",
    "# plt.legend()\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# sns.despine()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(PATH_EXPORT, \"spotter_accuracy_overall.pdf\"), bbox_inches=\"tight\", dpi=300)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary Statistics by Spotter Type:\")\n",
    "print(\"=\" * 50)\n",
    "spotter_stats = df.groupby(\"spotter_type_short\")[\"is_correct\"].agg([\n",
    "    'count', 'mean', 'std', 'min', 'max'\n",
    "]).round(3)\n",
    "spotter_stats['mean_pct'] = (spotter_stats['mean'] * 100).round(1)\n",
    "print(spotter_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise statistical significance testing between spotter types\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Pairwise Statistical Comparisons (Mann-Whitney U Test):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get all spotter types\n",
    "spotter_types = df[\"spotter_type_short\"].cat.categories\n",
    "\n",
    "# Create a results table for p-values\n",
    "results_data = []\n",
    "\n",
    "# Perform pairwise comparisons\n",
    "for type1, type2 in combinations(spotter_types, 2):\n",
    "    # Get accuracy data for each spotter type\n",
    "    data1 = df[df[\"spotter_type_short\"] == type1][\"is_correct\"]\n",
    "    data2 = df[df[\"spotter_type_short\"] == type2][\"is_correct\"]\n",
    "\n",
    "    # Perform Mann-Whitney U test (non-parametric)\n",
    "    statistic, p_value = stats.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "\n",
    "    # Calculate means and effect size\n",
    "    mean1 = data1.mean()\n",
    "    mean2 = data2.mean()\n",
    "    mean_diff = mean2 - mean1\n",
    "\n",
    "    # Determine significance level\n",
    "    if p_value < 0.001:\n",
    "        significance = \"***\"\n",
    "        sig_level = \"p < 0.001\"\n",
    "    elif p_value < 0.01:\n",
    "        significance = \"**\"\n",
    "        sig_level = \"p < 0.01\"\n",
    "    elif p_value < 0.05:\n",
    "        significance = \"*\"\n",
    "        sig_level = \"p < 0.05\"\n",
    "    else:\n",
    "        significance = \"\"\n",
    "        sig_level = \"n.s.\"\n",
    "\n",
    "    # Store results\n",
    "    results_data.append({\n",
    "        'Comparison': f\"{type1} vs {type2}\",\n",
    "        'Mean_1': mean1,\n",
    "        'Mean_2': mean2,\n",
    "        'Difference': mean_diff,\n",
    "        'P_value': p_value,\n",
    "        'Significance': significance,\n",
    "        'Sig_Level': sig_level\n",
    "    })\n",
    "\n",
    "    # Print detailed results\n",
    "    print(f\"{type1} vs {type2}:\")\n",
    "    print(f\"  Mean accuracy: {mean1:.3f} vs {mean2:.3f} (diff: {mean_diff:+.3f})\")\n",
    "    print(f\"  Sample sizes: {len(data1)} vs {len(data2)}\")\n",
    "    print(f\"  p-value: {p_value:.4f} {significance} ({sig_level})\")\n",
    "    print()\n",
    "\n",
    "# Create a summary table\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\nSummary Table of Pairwise Comparisons:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.round(4).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices for significance analysis\n",
    "n_types = len(spotter_types)\n",
    "significance_matrix = np.full((n_types, n_types), \"\", dtype=object)\n",
    "mean_diff_matrix = np.zeros((n_types, n_types))\n",
    "\n",
    "# Fill the matrices\n",
    "for i, type1 in enumerate(spotter_types):\n",
    "    for j, type2 in enumerate(spotter_types):\n",
    "        if i != j:  # Don't compare a type with itself\n",
    "            # Find the comparison in our results\n",
    "            comparison1 = f\"{type1} vs {type2}\"\n",
    "            comparison2 = f\"{type2} vs {type1}\"\n",
    "\n",
    "            # Find the result (either direction)\n",
    "            result = results_df[\n",
    "                (results_df['Comparison'] == comparison1) |\n",
    "                (results_df['Comparison'] == comparison2)\n",
    "            ]\n",
    "\n",
    "            if not result.empty:\n",
    "                sig = result.iloc[0]['Significance']\n",
    "                significance_matrix[i, j] = sig\n",
    "\n",
    "                # Use the difference from the perspective of j vs i (column vs row)\n",
    "                if result.iloc[0]['Comparison'] == comparison2:\n",
    "                    mean_diff_matrix[i, j] = result.iloc[0]['Difference']\n",
    "                else:\n",
    "                    mean_diff_matrix[i, j] = -result.iloc[0]['Difference']\n",
    "\n",
    "# Reverse the order of spotter types (flip rows and columns)\n",
    "spotter_types_reversed = spotter_types[::-1]\n",
    "\n",
    "# Create reversed indices mapping\n",
    "reverse_idx = {i: n_types - 1 - i for i in range(n_types)}\n",
    "\n",
    "# Create new matrices with reversed order\n",
    "mean_diff_matrix_rev = np.zeros((n_types, n_types))\n",
    "significance_matrix_rev = np.full((n_types, n_types), \"\", dtype=object)\n",
    "\n",
    "for i in range(n_types):\n",
    "    for j in range(n_types):\n",
    "        rev_i = reverse_idx[i]\n",
    "        rev_j = reverse_idx[j]\n",
    "        mean_diff_matrix_rev[i, j] = mean_diff_matrix[rev_i, rev_j]\n",
    "        significance_matrix_rev[i, j] = significance_matrix[rev_i, rev_j]\n",
    "\n",
    "# Create annotations that combine significance and mean difference\n",
    "annotations = np.full((n_types, n_types), \"\", dtype=object)\n",
    "for i in range(n_types):\n",
    "    for j in range(n_types):\n",
    "        if i == j:\n",
    "            # Diagonal indicator - show the spotter type name\n",
    "            annotations[i, j] = f\"--\"\n",
    "        else:\n",
    "            sig = significance_matrix_rev[i, j]\n",
    "            diff = mean_diff_matrix_rev[i, j]\n",
    "            annotations[i, j] = f\"{diff:+.3f}\\n{sig}\" if sig else f\"{diff:+.3f}\"\n",
    "\n",
    "# Create a mask for the lower triangle (excluding diagonal to preserve it)\n",
    "mask = np.tril(np.ones_like(mean_diff_matrix_rev, dtype=bool), k=-1)\n",
    "\n",
    "# Create the figure with more space for the legend\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Create the upper-triangular heatmap\n",
    "sns.heatmap(\n",
    "    mean_diff_matrix_rev,\n",
    "    mask=mask,\n",
    "    annot=annotations,\n",
    "    fmt='',\n",
    "    xticklabels=spotter_types_reversed,\n",
    "    yticklabels=spotter_types_reversed,\n",
    "    cmap='RdBu',\n",
    "    center=0,\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Mean Accuracy Difference'},\n",
    "    square=True\n",
    ")\n",
    "\n",
    "ax.set_title(\"Spotter Accuracy Differences\", fontsize=16)\n",
    "ax.set_xlabel('', fontsize=12)\n",
    "ax.set_ylabel('', fontsize=12)\n",
    "\n",
    "# Add legend text\n",
    "legend_text = \"\"\"Statistical Significance:\n",
    "*** p < 0.001 (highly significant)\n",
    "**  p < 0.01 (very significant)\n",
    "*   p < 0.05 (significant)\n",
    "    p â‰¥ 0.05 (not significant)\"\"\"\n",
    "\n",
    "# Position the legend in the lower left corner of the plot\n",
    "ax.text(0.02, 0.02, legend_text, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray'), fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    os.path.join(PATH_EXPORT, \"spotter_accuracy_differences.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1823fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5429039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared helper to build cleaned human_df from df_gold\n",
    "from typing import Dict\n",
    "\n",
    "LABEL_MAP_DEFAULT: Dict[str, str] = {\n",
    "    \"Discourse\": \"gold_discourse\",\n",
    "    \"Stateful\": \"gold_stateful\",\n",
    "    \"Vague\": \"gold_vague\",\n",
    "    \"Ambiguous\": \"gold_ambiguous\",\n",
    "}\n",
    "\n",
    "def build_human_df(df_gold: pd.DataFrame, label_map: Dict[str, str] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a cleaned human answers dataframe with parsed gold/human answers and a\n",
    "    boolean flag `_has_any_label` indicating if any gold label is marked true.\n",
    "\n",
    "    Parameters:\n",
    "      - df_gold: raw dataset containing human answers and gold labels\n",
    "      - label_map: mapping from friendly label name to boolean column name\n",
    "                   (defaults to LABEL_MAP_DEFAULT)\n",
    "    \"\"\"\n",
    "    from battleship.agents import Answer\n",
    "\n",
    "    if label_map is None:\n",
    "        label_map = LABEL_MAP_DEFAULT\n",
    "    gold_label_cols = list(label_map.values())\n",
    "\n",
    "    human_df = df_gold.copy()\n",
    "    human_df = human_df[human_df[\"messageType\"] == \"answer\"].copy()\n",
    "    human_df = human_df[~pd.isna(human_df[\"gold_answer\"]) & ~pd.isna(human_df[\"messageText\"])].copy()\n",
    "\n",
    "    # Parse answers\n",
    "    human_df[\"gold_answer_value\"] = human_df[\"gold_answer\"].apply(Answer.parse)\n",
    "    human_df[\"human_answer_value\"] = human_df[\"messageText\"].apply(Answer.parse)\n",
    "    # Keep only rows where human parse succeeded\n",
    "    human_df = human_df[human_df[\"human_answer_value\"].isin([True, False])].copy()\n",
    "\n",
    "    # Determine rows that have any gold labels set\n",
    "    human_df[\"_has_any_label\"] = human_df[gold_label_cols].fillna(False).any(axis=1)\n",
    "    return human_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c46da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to build long-form dataset for question type analysis (incl. Simple/Complex)\n",
    "from typing import Tuple, List\n",
    "\n",
    "def build_combined_long_df(df: pd.DataFrame, df_gold: pd.DataFrame, NA_LABEL: str = \"Simple\") -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Build a combined long-form dataframe with per-question correctness for:\n",
    "      - Models (from df)\n",
    "      - Humans (from df_gold)\n",
    "    and gold-label buckets: Overall, Simple (N/A), Complex (any label True), and each specific label.\n",
    "\n",
    "    Returns:\n",
    "      combined_long_df, label_order, spotter_order\n",
    "    \"\"\"\n",
    "    LABEL_MAP = {\n",
    "        NA_LABEL: None,\n",
    "        \"Discourse\": \"gold_discourse\",\n",
    "        \"Stateful\": \"gold_stateful\",\n",
    "        \"Vague\": \"gold_vague\",\n",
    "        \"Ambiguous\": \"gold_ambiguous\",\n",
    "    }\n",
    "    gold_label_cols = [v for v in LABEL_MAP.values() if v is not None]\n",
    "\n",
    "    # -----------------------------\n",
    "    # Models\n",
    "    # -----------------------------\n",
    "    rows_with_any_label = df[gold_label_cols].fillna(False).any(axis=1)\n",
    "    model_records = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Overall always\n",
    "        model_records.append({\n",
    "            \"gold_label\": \"Overall\",\n",
    "            \"spotter_type_short\": row[\"spotter_type_short\"],\n",
    "            \"llm_display_name\": row[\"llm_display_name\"],\n",
    "            \"is_correct\": row[\"is_correct\"],\n",
    "        })\n",
    "        has_any = bool(rows_with_any_label.loc[idx])\n",
    "        if not has_any:\n",
    "            # Simple (N/A)\n",
    "            model_records.append({\n",
    "                \"gold_label\": NA_LABEL,\n",
    "                \"spotter_type_short\": row[\"spotter_type_short\"],\n",
    "                \"llm_display_name\": row[\"llm_display_name\"],\n",
    "                \"is_correct\": row[\"is_correct\"],\n",
    "            })\n",
    "        else:\n",
    "            # Complex aggregate bucket\n",
    "            model_records.append({\n",
    "                \"gold_label\": \"Complex\",\n",
    "                \"spotter_type_short\": row[\"spotter_type_short\"],\n",
    "                \"llm_display_name\": row[\"llm_display_name\"],\n",
    "                \"is_correct\": row[\"is_correct\"],\n",
    "            })\n",
    "            # Specific labels\n",
    "            for nice_name, col in LABEL_MAP.items():\n",
    "                if col is None:\n",
    "                    continue\n",
    "                val = row[col]\n",
    "                if pd.notna(val) and bool(val):\n",
    "                    model_records.append({\n",
    "                        \"gold_label\": nice_name,\n",
    "                        \"spotter_type_short\": row[\"spotter_type_short\"],\n",
    "                        \"llm_display_name\": row[\"llm_display_name\"],\n",
    "                        \"is_correct\": row[\"is_correct\"],\n",
    "                    })\n",
    "    model_long_df = pd.DataFrame.from_records(model_records)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Humans (via shared helper)\n",
    "    # -----------------------------\n",
    "    human_df = build_human_df(df_gold, {k: v for k, v in LABEL_MAP.items() if v is not None})\n",
    "    human_rows_with_any_label = human_df[\"_has_any_label\"]\n",
    "\n",
    "    human_records = []\n",
    "    for idx, row in human_df.iterrows():\n",
    "        is_correct = bool(row[\"human_answer_value\"] == row[\"gold_answer_value\"])\n",
    "        # Overall\n",
    "        human_records.append({\n",
    "            \"gold_label\": \"Overall\",\n",
    "            \"spotter_type_short\": \"Human\",\n",
    "            \"llm_display_name\": \"Human\",\n",
    "            \"is_correct\": is_correct,\n",
    "        })\n",
    "        has_any = bool(human_rows_with_any_label.loc[idx])\n",
    "        if not has_any:\n",
    "            # Simple\n",
    "            human_records.append({\n",
    "                \"gold_label\": NA_LABEL,\n",
    "                \"spotter_type_short\": \"Human\",\n",
    "                \"llm_display_name\": \"Human\",\n",
    "                \"is_correct\": is_correct,\n",
    "            })\n",
    "        else:\n",
    "            # Complex\n",
    "            human_records.append({\n",
    "                \"gold_label\": \"Complex\",\n",
    "                \"spotter_type_short\": \"Human\",\n",
    "                \"llm_display_name\": \"Human\",\n",
    "                \"is_correct\": is_correct,\n",
    "            })\n",
    "            for nice_name, col in LABEL_MAP.items():\n",
    "                if col is None:\n",
    "                    continue\n",
    "                val = row[col]\n",
    "                if pd.notna(val) and bool(val):\n",
    "                    human_records.append({\n",
    "                        \"gold_label\": nice_name,\n",
    "                        \"spotter_type_short\": \"Human\",\n",
    "                        \"llm_display_name\": \"Human\",\n",
    "                        \"is_correct\": is_correct,\n",
    "                    })\n",
    "    human_long_df = pd.DataFrame.from_records(human_records)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Combine and order\n",
    "    # -----------------------------\n",
    "    combined_long_df = pd.concat([model_long_df, human_long_df], ignore_index=True)\n",
    "\n",
    "    label_order = [\"Overall\", NA_LABEL, \"Complex\", \"Discourse\", \"Stateful\", \"Vague\", \"Ambiguous\"]\n",
    "    combined_long_df[\"gold_label\"] = pd.Categorical(combined_long_df[\"gold_label\"], categories=label_order, ordered=True)\n",
    "\n",
    "    existing_order = list(df[\"spotter_type_short\"].cat.categories)\n",
    "    spotter_order = existing_order + [\"Human\"]\n",
    "    combined_long_df[\"spotter_type_short\"] = pd.Categorical(combined_long_df[\"spotter_type_short\"], categories=spotter_order, ordered=True)\n",
    "\n",
    "    return combined_long_df, label_order, spotter_order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583fa852",
   "metadata": {},
   "source": [
    "### Performance by Gold Label (Across All Models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from battleship.agents import Answer\n",
    "\n",
    "NA_LABEL = \"Simple\"\n",
    "show_legend = False\n",
    "\n",
    "# Build combined dataset via helper (adds Complex bucket)\n",
    "combined_long_df, label_order, spotter_order = build_combined_long_df(df, df_gold, NA_LABEL=NA_LABEL)\n",
    "\n",
    "# Plot grouped bar chart with per-question 95% CIs (no per-model aggregation)\n",
    "plt.figure(figsize=(8, 6))\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    sns.barplot(\n",
    "        data=combined_long_df,\n",
    "        x=\"gold_label\",\n",
    "        y=\"is_correct\",\n",
    "        hue=\"spotter_type_short\",\n",
    "        errorbar=(\"ci\", 95),\n",
    "        err_kws={\"linewidth\": 1.5},\n",
    "        capsize=0.1,\n",
    "        legend=show_legend,\n",
    "    )\n",
    "\n",
    "# plt.title(\"Accuracy by Question Type (Per-question 95% CI, incl. Human)\", fontsize=14)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(rotation=0)\n",
    "if show_legend:\n",
    "    plt.legend(title=\"Spotter Models\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\n",
    "    os.path.join(PATH_EXPORT, \"spotter_accuracy_by_gold_label.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n",
    "\n",
    "# Summary table for quick inspection (kept for debugging)\n",
    "summary_table = (\n",
    "    combined_long_df.groupby([\n",
    "        \"gold_label\", \"spotter_type_short\"\n",
    "    ], observed=False)[\"is_correct\"].agg([\"count\", \"mean\", \"std\"]).round(3)\n",
    ")\n",
    "summary_table[\"mean_pct\"] = (summary_table[\"mean\"] * 100).round(1)\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendix: Mean accuracy by question type with MultiIndex rows (Provider, LLM, Spotter Type)\n",
    "# Supersedes prior master_table\n",
    "# Additions:\n",
    "# - Include llm_provider as top-level index\n",
    "# - Include Valid column (prop of non-missing answer_text)\n",
    "# - Rename gold_label axis to 'Question Type' to avoid LaTeX issues\n",
    "# - Preserve original LLM ordering\n",
    "# - Column order places Complex immediately after Simple\n",
    "\n",
    "# Build combined dataset (ensures Complex bucket exists and consistent ordering)\n",
    "combined_long_df, label_order, spotter_order = build_combined_long_df(df, df_gold, NA_LABEL=\"Simple\")\n",
    "\n",
    "# Reorder label_order to ensure Complex is right after Simple\n",
    "if \"Simple\" in label_order and \"Complex\" in label_order:\n",
    "    lbls = [l for l in label_order if l not in (\"Simple\", \"Complex\")]\n",
    "    label_order = [\"Overall\", \"Simple\", \"Complex\"] + [l for l in lbls if l not in (\"Overall\",)]\n",
    "\n",
    "# 1. Map providers to combined_long_df (Human gets its own provider label)\n",
    "provider_map = (\n",
    "    df.drop_duplicates(subset=[\"llm_display_name\"])\n",
    "      .set_index(\"llm_display_name\")[\"llm_provider\"].to_dict()\n",
    ")\n",
    "combined_long_df = combined_long_df.copy()\n",
    "combined_long_df[\"llm_provider\"] = combined_long_df[\"llm_display_name\"].map(provider_map)\n",
    "combined_long_df.loc[combined_long_df[\"llm_display_name\"] == \"Human\", \"llm_provider\"] = \"Human\"\n",
    "\n",
    "# 2. Build accuracy pivot: mean correctness per (provider, llm, spotter type) across question types\n",
    "appendix_accuracy_table = (\n",
    "    combined_long_df\n",
    "    .groupby([\"llm_provider\", \"llm_display_name\", \"spotter_type_short\", \"gold_label\"], observed=True)[\"is_correct\"]\n",
    "    .mean()\n",
    "    .unstack(\"gold_label\")\n",
    ")\n",
    "\n",
    "# 3. Ensure column order matches adjusted label_order and that Complex follows Simple\n",
    "appendix_accuracy_table = appendix_accuracy_table.reindex(columns=[c for c in label_order if c in appendix_accuracy_table.columns])\n",
    "\n",
    "# 4. Compute Valid metric from original df (models). Human assumed fully valid (1.0)\n",
    "valid_series = (\n",
    "    df.groupby([\"llm_provider\", \"llm_display_name\", \"spotter_type_short\"], observed=True)[\"answer_text\"]\n",
    "      .apply(lambda x: 1 - x.isna().mean())\n",
    ")\n",
    "# Append Valid column (align on index); fill Human rows with 1.0\n",
    "appendix_accuracy_table[\"Valid\"] = valid_series\n",
    "appendix_accuracy_table[\"Valid\"].fillna(1.0, inplace=True)\n",
    "\n",
    "# 5. Round all numeric values to 3 decimals\n",
    "appendix_accuracy_table = appendix_accuracy_table.round(3)\n",
    "\n",
    "# 6. Set index level names & rename columns axis\n",
    "appendix_accuracy_table.index = appendix_accuracy_table.index.set_names([\"Provider\", \"LLM\", \"Spotter Type\"])\n",
    "appendix_accuracy_table.columns.name = \"Question Type\"\n",
    "\n",
    "# 7. Sort index to respect categorical LLM ordering (already preserved) and spotter type order\n",
    "appendix_accuracy_table = appendix_accuracy_table.sort_index(level=[\"Provider\", \"LLM\", \"Spotter Type\"], sort_remaining=False)\n",
    "\n",
    "print(\"Appendix Accuracy Table (mean accuracy by Question Type with Valid):\")\n",
    "print(appendix_accuracy_table)\n",
    "\n",
    "# 8. Export to LaTeX (basic)\n",
    "appendix_latex_path = os.path.join(PATH_EXPORT, \"spotter_master_table_basic.tex\")\n",
    "appendix_accuracy_table.to_latex(\n",
    "    appendix_latex_path,\n",
    "    float_format=lambda x: f\"{x:.3f}\",\n",
    "    multirow=True,\n",
    "    # escape=True,\n",
    ")\n",
    "print(f\"LaTeX table written to: {appendix_latex_path}\")\n",
    "\n",
    "##########################################################################\n",
    "# Custom LaTeX export with provider as group header row\n",
    "##########################################################################\n",
    "def _latex_escape(text: str) -> str:\n",
    "    repl = {\n",
    "        \"\\\\\": r\"\\textbackslash{}\",\n",
    "        \"&\": r\"\\&\",\n",
    "        \"%\": r\"\\%\",\n",
    "        \"$\": r\"\\$\",\n",
    "        \"#\": r\"\\#\",\n",
    "        \"_\": r\"\\_\",\n",
    "        \"{\": r\"\\{\",\n",
    "        \"}\": r\"\\}\",\n",
    "        \"~\": r\"\\textasciitilde{}\",\n",
    "        \"^\": r\"\\textasciicircum{}\",\n",
    "    }\n",
    "    out = str(text)\n",
    "    for k, v in repl.items():\n",
    "        out = out.replace(k, v)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Build LaTeX with provider as pseudo-row\n",
    "provider_grouped_path = os.path.join(PATH_EXPORT, \"spotter_master_table.tex\")\n",
    "\n",
    "row_end = \"\\\\\\\\\"  # literal \\\\ in output\n",
    "\n",
    "# Pretty names for providers\n",
    "PROVIDER_DISPLAY = {\n",
    "    \"openai\": \"OpenAI\",\n",
    "    \"anthropic\": \"Anthropic\",\n",
    "    \"google\": \"Google\",\n",
    "    \"meta-llama\": \"Meta Llama\",\n",
    "    \"Human\": \"Human\",\n",
    "}\n",
    "\n",
    "df_exp = appendix_accuracy_table.copy()\n",
    "metric_cols = list(df_exp.columns)\n",
    "num_metrics = len(metric_cols)\n",
    "# We'll print columns: LLM, Spotter Type, then metric columns\n",
    "ncols_total = 2 + num_metrics\n",
    "\n",
    "lines = []\n",
    "colspec = \"ll\" + (\"r\" * num_metrics)\n",
    "# Note: Requires \\usepackage[table]{xcolor}\n",
    "lines.append(f\"\\\\begin{{tabular}}{{{colspec}}}\")\n",
    "lines.append(\"\\\\toprule\")\n",
    "# Header row\n",
    "header = [\"LLM\", \"Spotter Type\"] + [_latex_escape(c) for c in metric_cols]\n",
    "lines.append(\" & \".join(header) + f\" {row_end}\")\n",
    "lines.append(\"\\\\midrule\")\n",
    "\n",
    "# Provider order as they appear in the index\n",
    "provider_order = list(df_exp.index.get_level_values(\"Provider\").unique())\n",
    "\n",
    "# Iterate by provider in order\n",
    "for p_idx, provider in enumerate(provider_order):\n",
    "    df_grp = df_exp.xs(provider, level=\"Provider\", drop_level=False)\n",
    "    prov_disp = PROVIDER_DISPLAY.get(provider, provider)\n",
    "    prov = _latex_escape(prov_disp)\n",
    "    # Subtle full-width band for provider header row\n",
    "    lines.append(\n",
    "        f\"\\\\rowcolor{{gray!40}} \\\\multicolumn{{{ncols_total}}}{{l}}{{\\\\textbf{{{prov}}}}} {row_end}\"\n",
    "    )\n",
    "\n",
    "    # Drop Provider level for iteration over (LLM, Spotter)\n",
    "    df_sub = df_grp.droplevel(\"Provider\")\n",
    "\n",
    "    # Alternate shading per LLM block\n",
    "    current_llm = None\n",
    "    shade = True  # reset per provider\n",
    "    for (llm, spotter), row in df_sub.iterrows():\n",
    "        if llm != current_llm:\n",
    "            current_llm = llm\n",
    "            shade = not shade\n",
    "        prefix = \"\\\\rowcolor{gray!10} \" if shade else \"\"\n",
    "        llm_s = (\n",
    "            _latex_escape(llm) if shade and llm == current_llm else _latex_escape(llm)\n",
    "        )\n",
    "        # Avoid repeating LLM name in subsequent rows of same block\n",
    "        llm_s = (\n",
    "            llm_s\n",
    "            if row.name[0] != current_llm or spotter == df_sub.loc[current_llm].index[0]\n",
    "            else \"\"\n",
    "        )\n",
    "        spot_s = _latex_escape(str(spotter))\n",
    "        vals = [f\"{v:.3f}\" if pd.notna(v) else \"\" for v in row.values]\n",
    "        lines.append(prefix + \" & \".join([llm_s, spot_s] + vals) + f\" {row_end}\")\n",
    "\n",
    "    # # Separate provider groups with a midrule except after the last group\n",
    "    # if p_idx < len(provider_order) - 1:\n",
    "    #     lines.append(\"\\\\midrule\")\n",
    "\n",
    "lines.append(\"\\\\bottomrule\")\n",
    "lines.append(\"\\\\end{tabular}\")\n",
    "\n",
    "with open(provider_grouped_path, \"w\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(f\"LaTeX table with provider headers written to: {provider_grouped_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold label prevalence (share of human answers with each gold label)\n",
    "# Note: Multilabel, so percentages can sum to >100%. Includes the N/A bucket for none.\n",
    "\n",
    "# Config\n",
    "NA_LABEL = \"Simple\"\n",
    "LABEL_MAP = {\n",
    "    \"Discourse\": \"gold_discourse\",\n",
    "    \"Stateful\": \"gold_stateful\",\n",
    "    \"Vague\": \"gold_vague\",\n",
    "    \"Ambiguous\": \"gold_ambiguous\",\n",
    "}\n",
    "\n",
    "gold_label_cols = list(LABEL_MAP.values())\n",
    "\n",
    "# Build human_df via shared helper (self-contained)\n",
    "human_df = build_human_df(df_gold, LABEL_MAP)\n",
    "\n",
    "label_prevalence_records = []\n",
    "\n",
    "total_answers = len(human_df)\n",
    "any_label_mask = human_df[\"_has_any_label\"]\n",
    "\n",
    "# Overall (all questions)\n",
    "overall_count = int(total_answers)\n",
    "label_prevalence_records.append(\n",
    "    {\n",
    "        \"gold_label\": \"Overall\",\n",
    "        \"count\": overall_count,\n",
    "        \"percent\": 100.00,\n",
    "    }\n",
    ")\n",
    "\n",
    "# N/A first (Simple)\n",
    "na_count = int((~any_label_mask).sum())\n",
    "label_prevalence_records.append(\n",
    "    {\n",
    "        \"gold_label\": NA_LABEL,\n",
    "        \"count\": na_count,\n",
    "        \"percent\": round(100.0 * na_count / total_answers, 1) if total_answers else 0.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Other labels\n",
    "for nice_name, col in LABEL_MAP.items():\n",
    "    count = int(human_df[col].fillna(False).sum())\n",
    "    label_prevalence_records.append(\n",
    "        {\n",
    "            \"gold_label\": nice_name,\n",
    "            \"count\": count,\n",
    "            \"percent\": round(100.0 * count / total_answers, 1) if total_answers else 0.0,\n",
    "        }\n",
    "    )\n",
    "\n",
    "prevalence_df = pd.DataFrame(label_prevalence_records)\n",
    "# Order using desired label order\n",
    "order = [lbl for lbl in [\"Overall\", NA_LABEL, \"Discourse\", \"Stateful\", \"Vague\", \"Ambiguous\"] if lbl in prevalence_df[\"gold_label\"].unique()]\n",
    "prevalence_df[\"gold_label\"] = pd.Categorical(prevalence_df[\"gold_label\"], categories=order, ordered=True)\n",
    "prevalence_df = prevalence_df.sort_values(\"gold_label\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nGold Label Prevalence (human data):\")\n",
    "print(prevalence_df.to_string(index=False))\n",
    "\n",
    "# Save prevalence to LaTeX with 1 decimal precision\n",
    "latex_str = prevalence_df.to_latex(index=False, float_format=lambda x: f\"{x:.1f}\")\n",
    "with open(os.path.join(PATH_EXPORT, \"gold_label_prevalence.tex\"), \"w\") as f:\n",
    "    f.write(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f47d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_LABEL = \"Simple\"\n",
    "COMPLEX_LABEL = \"Complex\"\n",
    "show_legend = False\n",
    "\n",
    "# Build combined dataset (ensures Complex bucket exists and consistent ordering)\n",
    "combined_long_df, label_order, _ = build_combined_long_df(\n",
    "    df, df_gold, NA_LABEL=SIMPLE_LABEL\n",
    ")\n",
    "\n",
    "# Collapse to Simple vs Complex only\n",
    "collapsed_long_df = combined_long_df[\n",
    "    combined_long_df[\"gold_label\"].isin([SIMPLE_LABEL, COMPLEX_LABEL])\n",
    "].copy()\n",
    "collapsed_long_df.rename(columns={\"gold_label\": \"difficulty\"}, inplace=True)\n",
    "\n",
    "# Order categories\n",
    "difficulty_order = [SIMPLE_LABEL, COMPLEX_LABEL]\n",
    "collapsed_long_df[\"difficulty\"] = pd.Categorical(\n",
    "    collapsed_long_df[\"difficulty\"], categories=difficulty_order, ordered=True\n",
    ")\n",
    "\n",
    "existing_order = list(df[\"spotter_type_short\"].cat.categories)\n",
    "spotter_order = existing_order + [\"Human\"]\n",
    "collapsed_long_df[\"spotter_type_short\"] = pd.Categorical(\n",
    "    collapsed_long_df[\"spotter_type_short\"], categories=spotter_order, ordered=True\n",
    ")\n",
    "\n",
    "# Point plot with tuned styling\n",
    "plt.figure(figsize=(4, 6))\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = sns.pointplot(\n",
    "        data=collapsed_long_df,\n",
    "        x=\"difficulty\",\n",
    "        y=\"is_correct\",\n",
    "        hue=\"spotter_type_short\",\n",
    "        errorbar=(\"ci\", 95),\n",
    "        err_kws={\"linewidth\": 1.5},\n",
    "        linewidth=3,\n",
    "        linestyle=\"-\",\n",
    "        markers=\"o\",\n",
    "        markersize=5,\n",
    "        capsize=0.1,\n",
    "        legend=show_legend,\n",
    "    )\n",
    "\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "if show_legend:\n",
    "    ax.legend(title=\"Spotter Models\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\n",
    "    os.path.join(PATH_EXPORT, \"spotter_accuracy_simple_vs_complex.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n",
    "\n",
    "# Print a compact summary table\n",
    "summary_table = (\n",
    "    collapsed_long_df\n",
    "    .groupby([\"difficulty\", \"spotter_type_short\"], observed=False)[\"is_correct\"]\n",
    "    .agg([\"count\", \"mean\", \"std\"]).round(3)\n",
    ")\n",
    "summary_table[\"mean_pct\"] = (summary_table[\"mean\"] * 100).round(1)\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05e836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battleship-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
