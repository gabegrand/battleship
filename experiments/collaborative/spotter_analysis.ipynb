{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f139a8d",
   "metadata": {},
   "source": [
    "# Spotter Benchmark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from experiments.collaborative.analysis import (\n",
    "    load_dataset,\n",
    "    get_gold_answer_dataset,\n",
    "    MODEL_DISPLAY_NAMES,\n",
    "    get_spotter_type_short,\n",
    ")\n",
    "from battleship.run_spotter_benchmarks import rebuild_summary_from_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b9c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# set seaborn color palette\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# set seaborn style\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b45d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "\n",
    "# Set the default font to DejaVu Sans\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# Print current font family settings\n",
    "print(\"Current Font Settings:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Default font family: {plt.rcParams['font.family']}\")\n",
    "print(f\"Sans-serif fonts: {plt.rcParams['font.sans-serif']}\")\n",
    "print(f\"Serif fonts: {plt.rcParams['font.serif']}\")\n",
    "print(f\"Monospace fonts: {plt.rcParams['font.monospace']}\")\n",
    "print(f\"Cursive fonts: {plt.rcParams['font.cursive']}\")\n",
    "print(f\"Fantasy fonts: {plt.rcParams['font.fantasy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"battleship-final-data\"\n",
    "PATH_DATA = os.path.join(\"data\", EXPERIMENT_NAME)\n",
    "PATH_EXPORT = os.path.join(PATH_DATA, \"export\")\n",
    "\n",
    "df_gold = load_dataset(experiment_path=PATH_DATA, use_gold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e0dbf",
   "metadata": {},
   "source": [
    "## Human results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6003901",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels, human_labels = get_gold_answer_dataset(df_gold)\n",
    "print(len(gold_labels), len(human_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=gold_labels, y_pred=human_labels))\n",
    "\n",
    "human_accuracy_baseline = classification_report(y_true=gold_labels, y_pred=human_labels, output_dict=True)[\"accuracy\"]\n",
    "print(f\"Human accuracy baseline: {human_accuracy_baseline:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f483c",
   "metadata": {},
   "source": [
    "## Modeling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = \"run_2025_07_11_18_32_51\"\n",
    "\n",
    "results = rebuild_summary_from_results(os.path.join(\"spotter_benchmarks\", RUN_ID))\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Add display names and categorizations for analysis\n",
    "def add_display_fields(df):\n",
    "    \"\"\"Add display names and categorizations to the dataframe.\"\"\"\n",
    "    # Add spotter type categorization\n",
    "    df[\"spotter_type_short\"] = df.apply(\n",
    "        lambda row: get_spotter_type_short(row[\"spotter_type\"], row[\"use_cot\"]), axis=1\n",
    "    )\n",
    "    df[\"spotter_type_short\"] = pd.Categorical(\n",
    "        df[\"spotter_type_short\"],\n",
    "        categories=[\"Base\", \"+ CoT\", \"+ Code\", \"+ CoT + Code\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "    # Add model display name\n",
    "    df[\"llm_display_name\"] = df[\"llm\"].map(lambda x: MODEL_DISPLAY_NAMES.get(x, x))\n",
    "\n",
    "    # Sort by order in MODEL_DISPLAY_NAMES using categorical\n",
    "    df[\"llm_display_name\"] = pd.Categorical(\n",
    "        df[\"llm_display_name\"], categories=MODEL_DISPLAY_NAMES.values(), ordered=True\n",
    "    )\n",
    "    df = df.sort_values(by=[\"llm_display_name\", \"spotter_type_short\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Process the dataframe\n",
    "df = add_display_fields(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44867f1",
   "metadata": {},
   "source": [
    "### Completion status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    count_df = df.groupby([\"llm\", \"spotter_type_short\"], observed=False).size().to_frame(name=\"count\")\n",
    "    display(count_df)\n",
    "\n",
    "    filtered_count_df = count_df[count_df[\"count\"] < 948]\n",
    "    if len(filtered_count_df) > 0:\n",
    "        print(f\"Incomplete models:\")\n",
    "        display(filtered_count_df)\n",
    "    else:\n",
    "        print(\"All models complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(filtered_count_df.reset_index().llm.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of completion status\n",
    "count_df = df.groupby([\"llm\", \"spotter_type_short\"], observed=False).size().to_frame(name=\"count\")\n",
    "\n",
    "# Create a pivot table for heatmap\n",
    "pivot_df = count_df.reset_index().pivot(index=\"llm\", columns=\"spotter_type_short\", values=\"count\")\n",
    "\n",
    "# Create completion status (1 for complete, 0 for incomplete)\n",
    "completion_df = (pivot_df == 948).astype(int)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.heatmap(\n",
    "    completion_df,\n",
    "    annot=pivot_df,  # Show actual counts as annotations\n",
    "    fmt='d',\n",
    "    cmap='RdYlGn',\n",
    "    cbar_kws={'label': 'Completion Status'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='white'\n",
    ")\n",
    "\n",
    "plt.title('Spotter Benchmark Completion Status\\n(Green = Complete [948], Red = Incomplete)', fontsize=14)\n",
    "plt.xlabel('Spotter Type', fontsize=12)\n",
    "plt.ylabel('LLM Model', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a2be7",
   "metadata": {},
   "source": [
    "### Answer value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of raw answer text\n",
    "df[\"answer_text\"].value_counts(dropna=False).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe and handle None values in answer_value\n",
    "df_plot = df.copy()\n",
    "df_plot[\"answer_value\"] = df_plot[\"answer_value\"].fillna(\"No Answer\")\n",
    "\n",
    "# Visualize the distribution of answer values by LLM and spotter type\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(\n",
    "    data=df_plot,\n",
    "    x=\"llm_display_name\",\n",
    "    hue=\"answer_value\",\n",
    "    order=df[\"llm_display_name\"].cat.categories,\n",
    "    palette={True: \"green\", False: \"red\", \"No Answer\": \"gray\"}\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Answer Values by LLM\")\n",
    "plt.xlabel(\"LLM Display Name\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90, ha=\"right\")\n",
    "plt.legend(title=\"Answer Value\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f233e5",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.plotting_context(context=\"talk\"), sns.axes_style(\"whitegrid\"):\n",
    "\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x=\"is_correct\",\n",
    "        y=\"llm_display_name\",\n",
    "        hue=\"spotter_type_short\",\n",
    "        errorbar=(\"ci\", 95),\n",
    "        err_kws={\n",
    "            \"color\": \"gray\",\n",
    "            \"linewidth\": 1,\n",
    "        },\n",
    "        capsize=0.2,\n",
    "    )\n",
    "\n",
    "    plt.axvline(\n",
    "        human_accuracy_baseline,\n",
    "        color=\"#4b4f73\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2.0,\n",
    "        label=\"Human Performance\",\n",
    "    )\n",
    "\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlabel(\"Gold Answer Accuracy\")\n",
    "\n",
    "    plt.xlim(0.0, 1.0)\n",
    "\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    plt.legend(title=\"Spotter Models\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    # plt.savefig(\n",
    "    #     os.path.join(PATH_EXPORT, \"spotter_accuracy.pdf\"),\n",
    "    #     bbox_inches=\"tight\",\n",
    "    #     dpi=300,\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddd8d1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Spotter Type Performance Across All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot showing mean accuracy with confidence intervals for each spotter type\n",
    "# Calculate mean accuracy for each spotter type across all models\n",
    "spotter_accuracy = df.groupby([\"spotter_type_short\", \"llm_display_name\"])[\"is_correct\"].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=spotter_accuracy,\n",
    "    x=\"spotter_type_short\",\n",
    "    y=\"is_correct\",\n",
    "    hue=\"spotter_type_short\",\n",
    "    errorbar=(\"ci\", 95),\n",
    "    err_kws={\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    capsize=0.1,\n",
    ")\n",
    "plt.axhline(\n",
    "    human_accuracy_baseline,\n",
    "    color=\"#4b4f73\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Human Performance\"\n",
    ")\n",
    "plt.text(\n",
    "    s=\"Human Performance\",\n",
    "    x=0.0,\n",
    "    y=human_accuracy_baseline + 0.01,\n",
    "    fontsize=12,\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    ")\n",
    "plt.title(\"Mean Accuracy by Spotter Type\\n(Averaged Across All Models)\", fontsize=14)\n",
    "plt.xlabel(\"Spotter Type\", fontsize=16)\n",
    "plt.ylabel(\"Mean Accuracy\", fontsize=16)\n",
    "plt.ylim(0, 1.0)\n",
    "# plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary Statistics by Spotter Type:\")\n",
    "print(\"=\" * 50)\n",
    "spotter_stats = df.groupby(\"spotter_type_short\")[\"is_correct\"].agg([\n",
    "    'count', 'mean', 'std', 'min', 'max'\n",
    "]).round(3)\n",
    "spotter_stats['mean_pct'] = (spotter_stats['mean'] * 100).round(1)\n",
    "print(spotter_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise statistical significance testing between spotter types\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Pairwise Statistical Comparisons (Mann-Whitney U Test):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get all spotter types\n",
    "spotter_types = df[\"spotter_type_short\"].cat.categories\n",
    "\n",
    "# Create a results table for p-values\n",
    "results_data = []\n",
    "\n",
    "# Perform pairwise comparisons\n",
    "for type1, type2 in combinations(spotter_types, 2):\n",
    "    # Get accuracy data for each spotter type\n",
    "    data1 = df[df[\"spotter_type_short\"] == type1][\"is_correct\"]\n",
    "    data2 = df[df[\"spotter_type_short\"] == type2][\"is_correct\"]\n",
    "\n",
    "    # Perform Mann-Whitney U test (non-parametric)\n",
    "    statistic, p_value = stats.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "\n",
    "    # Calculate means and effect size\n",
    "    mean1 = data1.mean()\n",
    "    mean2 = data2.mean()\n",
    "    mean_diff = mean2 - mean1\n",
    "\n",
    "    # Determine significance level\n",
    "    if p_value < 0.001:\n",
    "        significance = \"***\"\n",
    "        sig_level = \"p < 0.001\"\n",
    "    elif p_value < 0.01:\n",
    "        significance = \"**\"\n",
    "        sig_level = \"p < 0.01\"\n",
    "    elif p_value < 0.05:\n",
    "        significance = \"*\"\n",
    "        sig_level = \"p < 0.05\"\n",
    "    else:\n",
    "        significance = \"\"\n",
    "        sig_level = \"n.s.\"\n",
    "\n",
    "    # Store results\n",
    "    results_data.append({\n",
    "        'Comparison': f\"{type1} vs {type2}\",\n",
    "        'Mean_1': mean1,\n",
    "        'Mean_2': mean2,\n",
    "        'Difference': mean_diff,\n",
    "        'P_value': p_value,\n",
    "        'Significance': significance,\n",
    "        'Sig_Level': sig_level\n",
    "    })\n",
    "\n",
    "    # Print detailed results\n",
    "    print(f\"{type1} vs {type2}:\")\n",
    "    print(f\"  Mean accuracy: {mean1:.3f} vs {mean2:.3f} (diff: {mean_diff:+.3f})\")\n",
    "    print(f\"  Sample sizes: {len(data1)} vs {len(data2)}\")\n",
    "    print(f\"  p-value: {p_value:.4f} {significance} ({sig_level})\")\n",
    "    print()\n",
    "\n",
    "# Create a summary table\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\nSummary Table of Pairwise Comparisons:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.round(4).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices for significance analysis\n",
    "n_types = len(spotter_types)\n",
    "significance_matrix = np.full((n_types, n_types), \"\", dtype=object)\n",
    "mean_diff_matrix = np.zeros((n_types, n_types))\n",
    "\n",
    "# Fill the matrices\n",
    "for i, type1 in enumerate(spotter_types):\n",
    "    for j, type2 in enumerate(spotter_types):\n",
    "        if i != j:  # Don't compare a type with itself\n",
    "            # Find the comparison in our results\n",
    "            comparison1 = f\"{type1} vs {type2}\"\n",
    "            comparison2 = f\"{type2} vs {type1}\"\n",
    "\n",
    "            # Find the result (either direction)\n",
    "            result = results_df[\n",
    "                (results_df['Comparison'] == comparison1) |\n",
    "                (results_df['Comparison'] == comparison2)\n",
    "            ]\n",
    "\n",
    "            if not result.empty:\n",
    "                sig = result.iloc[0]['Significance']\n",
    "                significance_matrix[i, j] = sig\n",
    "\n",
    "                # Use the difference from the perspective of j vs i (column vs row)\n",
    "                if result.iloc[0]['Comparison'] == comparison2:\n",
    "                    mean_diff_matrix[i, j] = result.iloc[0]['Difference']\n",
    "                else:\n",
    "                    mean_diff_matrix[i, j] = -result.iloc[0]['Difference']\n",
    "\n",
    "# Reverse the order of spotter types (flip rows and columns)\n",
    "spotter_types_reversed = spotter_types[::-1]\n",
    "\n",
    "# Create reversed indices mapping\n",
    "reverse_idx = {i: n_types - 1 - i for i in range(n_types)}\n",
    "\n",
    "# Create new matrices with reversed order\n",
    "mean_diff_matrix_rev = np.zeros((n_types, n_types))\n",
    "significance_matrix_rev = np.full((n_types, n_types), \"\", dtype=object)\n",
    "\n",
    "for i in range(n_types):\n",
    "    for j in range(n_types):\n",
    "        rev_i = reverse_idx[i]\n",
    "        rev_j = reverse_idx[j]\n",
    "        mean_diff_matrix_rev[i, j] = mean_diff_matrix[rev_i, rev_j]\n",
    "        significance_matrix_rev[i, j] = significance_matrix[rev_i, rev_j]\n",
    "\n",
    "# Create annotations that combine significance and mean difference\n",
    "annotations = np.full((n_types, n_types), \"\", dtype=object)\n",
    "for i in range(n_types):\n",
    "    for j in range(n_types):\n",
    "        if i == j:\n",
    "            # Diagonal indicator - show the spotter type name\n",
    "            annotations[i, j] = f\"--\"\n",
    "        else:\n",
    "            sig = significance_matrix_rev[i, j]\n",
    "            diff = mean_diff_matrix_rev[i, j]\n",
    "            annotations[i, j] = f\"{diff:+.3f}\\n{sig}\" if sig else f\"{diff:+.3f}\"\n",
    "\n",
    "# Create a mask for the lower triangle (excluding diagonal to preserve it)\n",
    "mask = np.tril(np.ones_like(mean_diff_matrix_rev, dtype=bool), k=-1)\n",
    "\n",
    "# Create the figure with more space for the legend\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Create the upper-triangular heatmap\n",
    "sns.heatmap(\n",
    "    mean_diff_matrix_rev,\n",
    "    mask=mask,\n",
    "    annot=annotations,\n",
    "    fmt='',\n",
    "    xticklabels=spotter_types_reversed,\n",
    "    yticklabels=spotter_types_reversed,\n",
    "    cmap='RdBu',\n",
    "    center=0,\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Mean Accuracy Difference'},\n",
    "    square=True\n",
    ")\n",
    "\n",
    "ax.set_title(\"Spotter Accuracy Differences\", fontsize=16)\n",
    "ax.set_xlabel('', fontsize=12)\n",
    "ax.set_ylabel('', fontsize=12)\n",
    "\n",
    "# Add legend text\n",
    "legend_text = \"\"\"Statistical Significance:\n",
    "*** p < 0.001 (highly significant)\n",
    "**  p < 0.01 (very significant)\n",
    "*   p < 0.05 (significant)\n",
    "    p ≥ 0.05 (not significant)\"\"\"\n",
    "\n",
    "# Position the legend in the lower left corner of the plot\n",
    "ax.text(0.02, 0.02, legend_text, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray'), fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1823fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c46da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
