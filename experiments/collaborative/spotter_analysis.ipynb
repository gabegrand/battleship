{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f139a8d",
   "metadata": {},
   "source": [
    "# Spotter Benchmark Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef51be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.transforms as mtransforms\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from experiments.collaborative.analysis import (\n",
    "    load_dataset,\n",
    "    get_gold_answer_dataset,\n",
    "    MODEL_DISPLAY_NAMES,\n",
    "    get_spotter_type_short,\n",
    ")\n",
    "from battleship.run_spotter_benchmarks import rebuild_summary_from_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b9c1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# set seaborn color palette\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b45d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "\n",
    "# Set the default font to DejaVu Sans\n",
    "# plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# Print current font family settings\n",
    "print(\"Current Font Settings:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Default font family: {plt.rcParams['font.family']}\")\n",
    "print(f\"Sans-serif fonts: {plt.rcParams['font.sans-serif']}\")\n",
    "print(f\"Serif fonts: {plt.rcParams['font.serif']}\")\n",
    "print(f\"Monospace fonts: {plt.rcParams['font.monospace']}\")\n",
    "print(f\"Cursive fonts: {plt.rcParams['font.cursive']}\")\n",
    "print(f\"Fantasy fonts: {plt.rcParams['font.fantasy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"battleship-final-data\"\n",
    "PATH_DATA = os.path.join(\"data\", EXPERIMENT_NAME)\n",
    "PATH_EXPORT = os.path.join(PATH_DATA, \"export\")\n",
    "\n",
    "df_gold = load_dataset(experiment_path=PATH_DATA, use_gold=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7e0dbf",
   "metadata": {},
   "source": [
    "## Human results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6003901",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels, human_labels = get_gold_answer_dataset(df_gold)\n",
    "print(len(gold_labels), len(human_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=gold_labels, y_pred=human_labels))\n",
    "\n",
    "human_accuracy_baseline = classification_report(y_true=gold_labels, y_pred=human_labels, output_dict=True)[\"accuracy\"]\n",
    "print(f\"Human accuracy baseline: {human_accuracy_baseline:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f483c",
   "metadata": {},
   "source": [
    "## Modeling results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5223a81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_IDS = [\n",
    "    \"run_2025_07_11_18_32_51\",\n",
    "    \"run_2025_08_22_09_53_20\", # GPT-5\n",
    "]\n",
    "\n",
    "results = [rebuild_summary_from_results(os.path.join(\"spotter_benchmarks\", run_id)) for run_id in RUN_IDS]\n",
    "df = pd.concat([pd.DataFrame(result) for result in results]).reset_index(drop=True)\n",
    "\n",
    "# Add display names and categorizations for analysis\n",
    "def add_display_fields(df):\n",
    "    \"\"\"Add display names and categorizations to the dataframe.\"\"\"\n",
    "    # Add spotter type categorization\n",
    "    df[\"spotter_type_short\"] = df.apply(\n",
    "        lambda row: get_spotter_type_short(row[\"spotter_type\"], row[\"use_cot\"]), axis=1\n",
    "    )\n",
    "    df[\"spotter_type_short\"] = pd.Categorical(\n",
    "        df[\"spotter_type_short\"],\n",
    "        categories=[\"Base\", \"CoT\", \"Code\", \"CoT + Code\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "\n",
    "    # Add model display name\n",
    "    df[\"llm_display_name\"] = df[\"llm\"].map(lambda x: MODEL_DISPLAY_NAMES.get(x, x))\n",
    "\n",
    "    # Sort by order in MODEL_DISPLAY_NAMES using categorical\n",
    "    df[\"llm_display_name\"] = pd.Categorical(\n",
    "        df[\"llm_display_name\"], categories=[display_name for llm, display_name in MODEL_DISPLAY_NAMES.items() if llm in df[\"llm\"].unique()], ordered=True\n",
    "    )\n",
    "    df = df.sort_values(by=[\"llm_display_name\", \"spotter_type_short\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Process the dataframe\n",
    "df = add_display_fields(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44867f1",
   "metadata": {},
   "source": [
    "### Completion status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fa3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    count_df = df.groupby([\"llm\", \"spotter_type_short\"], observed=False).size().to_frame(name=\"count\")\n",
    "    display(count_df)\n",
    "\n",
    "    filtered_count_df = count_df[count_df[\"count\"] < 948]\n",
    "    if len(filtered_count_df) > 0:\n",
    "        print(f\"Incomplete models:\")\n",
    "        display(filtered_count_df)\n",
    "    else:\n",
    "        print(\"All models complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(filtered_count_df.reset_index().llm.unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visualization of completion status\n",
    "count_df = df.groupby([\"llm\", \"spotter_type_short\"], observed=False).size().to_frame(name=\"count\")\n",
    "\n",
    "# Create a pivot table for heatmap\n",
    "pivot_df = count_df.reset_index().pivot(index=\"llm\", columns=\"spotter_type_short\", values=\"count\")\n",
    "\n",
    "# Create completion status (1 for complete, 0 for incomplete)\n",
    "completion_df = (pivot_df == 948).astype(int)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(8, 12))\n",
    "sns.heatmap(\n",
    "    completion_df,\n",
    "    annot=pivot_df,  # Show actual counts as annotations\n",
    "    fmt='d',\n",
    "    cmap='RdYlGn',\n",
    "    cbar_kws={'label': 'Completion Status'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='white'\n",
    ")\n",
    "\n",
    "plt.title('Spotter Benchmark Completion Status\\n(Green = Complete [948], Red = Incomplete)', fontsize=14)\n",
    "plt.xlabel('Spotter Type', fontsize=12)\n",
    "plt.ylabel('LLM Model', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43a2be7",
   "metadata": {},
   "source": [
    "### Answer value distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of raw answer text\n",
    "df[\"answer_text\"].value_counts(dropna=False).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe and handle None values in answer_value\n",
    "df_plot = df.copy()\n",
    "df_plot[\"answer_value\"] = df_plot[\"answer_value\"].fillna(\"No Answer\")\n",
    "\n",
    "# Visualize the distribution of answer values by LLM and spotter type\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(\n",
    "    data=df_plot,\n",
    "    x=\"llm_display_name\",\n",
    "    hue=\"answer_value\",\n",
    "    order=df[\"llm_display_name\"].cat.categories,\n",
    "    palette={True: \"green\", False: \"red\", \"No Answer\": \"gray\"}\n",
    ")\n",
    "\n",
    "plt.title(\"Distribution of Answer Values by LLM\")\n",
    "plt.xlabel(\"LLM Display Name\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=90, ha=\"right\")\n",
    "plt.legend(title=\"Answer Value\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f233e5",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeb26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_legend = False\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.barplot(\n",
    "        data=df,\n",
    "        x=\"is_correct\",\n",
    "        y=\"llm_display_name\",\n",
    "        hue=\"spotter_type_short\",\n",
    "        errorbar=(\"ci\", 95),\n",
    "        err_kws={\n",
    "            \"color\": \"gray\",\n",
    "            \"linewidth\": 1,\n",
    "        },\n",
    "        capsize=0.2,\n",
    "        legend=show_legend,\n",
    "    )\n",
    "\n",
    "    plt.axvline(\n",
    "        human_accuracy_baseline,\n",
    "        color=\"#4b4f73\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2.0,\n",
    "        label=\"Human Performance\",\n",
    "    )\n",
    "\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlabel(\"Accuracy\")\n",
    "\n",
    "    plt.xlim(0.5, 1.0)\n",
    "    # plt.axvline(\n",
    "    #     0.5,\n",
    "    #     color=\"#808080\",\n",
    "    #     linestyle=\"--\",\n",
    "    #     linewidth=2.0,\n",
    "    #     label=\"Random Performance\",\n",
    "    # )\n",
    "\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    if show_legend:\n",
    "        plt.legend(title=\"Spotter Models\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(PATH_EXPORT, \"spotter_accuracy_by_model.pdf\"),\n",
    "        bbox_inches=\"tight\",\n",
    "        dpi=300,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddd8d1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Spotter Type Performance Across All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot showing mean accuracy with confidence intervals for each spotter type\n",
    "# Calculate mean accuracy for each spotter type across all models\n",
    "spotter_accuracy = df.groupby([\"spotter_type_short\", \"llm_display_name\"])[\"is_correct\"].mean().reset_index()\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=spotter_accuracy,\n",
    "    x=\"spotter_type_short\",\n",
    "    y=\"is_correct\",\n",
    "    hue=\"spotter_type_short\",\n",
    "    errorbar=(\"ci\", 95),\n",
    "    err_kws={\n",
    "        \"linewidth\": 2,\n",
    "    },\n",
    "    capsize=0.1,\n",
    ")\n",
    "plt.axhline(\n",
    "    human_accuracy_baseline,\n",
    "    color=\"#4b4f73\",\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2.0,\n",
    "    label=\"Human Performance\"\n",
    ")\n",
    "\n",
    "ax = plt.gca()\n",
    "trans = mtransforms.blended_transform_factory(ax.transAxes, ax.transData)\n",
    "\n",
    "plt.text(\n",
    "    s=\"Human Performance\",\n",
    "    x=0.5,\n",
    "    y=human_accuracy_baseline + 0.01,\n",
    "    fontsize=14,\n",
    "    ha=\"center\",\n",
    "    va=\"bottom\",\n",
    "    transform=trans,\n",
    ")\n",
    "# plt.title(\"Mean Accuracy by Spotter Type\\n(Averaged Across All Models)\", fontsize=14)\n",
    "plt.xlabel(\"\", fontsize=16)\n",
    "plt.ylabel(\"Accuracy\", fontsize=16)\n",
    "plt.ylim(0.5, 1.0)\n",
    "# plt.legend()\n",
    "# plt.xticks(rotation=45)\n",
    "\n",
    "# sns.despine()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(PATH_EXPORT, \"spotter_accuracy_overall.pdf\"), bbox_inches=\"tight\", dpi=300)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Summary Statistics by Spotter Type:\")\n",
    "print(\"=\" * 50)\n",
    "spotter_stats = df.groupby(\"spotter_type_short\")[\"is_correct\"].agg([\n",
    "    'count', 'mean', 'std', 'min', 'max'\n",
    "]).round(3)\n",
    "spotter_stats['mean_pct'] = (spotter_stats['mean'] * 100).round(1)\n",
    "print(spotter_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise statistical significance testing between spotter types\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Pairwise Statistical Comparisons (Mann-Whitney U Test):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get all spotter types\n",
    "spotter_types = df[\"spotter_type_short\"].cat.categories\n",
    "\n",
    "# Create a results table for p-values\n",
    "results_data = []\n",
    "\n",
    "# Perform pairwise comparisons\n",
    "for type1, type2 in combinations(spotter_types, 2):\n",
    "    # Get accuracy data for each spotter type\n",
    "    data1 = df[df[\"spotter_type_short\"] == type1][\"is_correct\"]\n",
    "    data2 = df[df[\"spotter_type_short\"] == type2][\"is_correct\"]\n",
    "\n",
    "    # Perform Mann-Whitney U test (non-parametric)\n",
    "    statistic, p_value = stats.mannwhitneyu(data1, data2, alternative='two-sided')\n",
    "\n",
    "    # Calculate means and effect size\n",
    "    mean1 = data1.mean()\n",
    "    mean2 = data2.mean()\n",
    "    mean_diff = mean2 - mean1\n",
    "\n",
    "    # Determine significance level\n",
    "    if p_value < 0.001:\n",
    "        significance = \"***\"\n",
    "        sig_level = \"p < 0.001\"\n",
    "    elif p_value < 0.01:\n",
    "        significance = \"**\"\n",
    "        sig_level = \"p < 0.01\"\n",
    "    elif p_value < 0.05:\n",
    "        significance = \"*\"\n",
    "        sig_level = \"p < 0.05\"\n",
    "    else:\n",
    "        significance = \"\"\n",
    "        sig_level = \"n.s.\"\n",
    "\n",
    "    # Store results\n",
    "    results_data.append({\n",
    "        'Comparison': f\"{type1} vs {type2}\",\n",
    "        'Mean_1': mean1,\n",
    "        'Mean_2': mean2,\n",
    "        'Difference': mean_diff,\n",
    "        'P_value': p_value,\n",
    "        'Significance': significance,\n",
    "        'Sig_Level': sig_level\n",
    "    })\n",
    "\n",
    "    # Print detailed results\n",
    "    print(f\"{type1} vs {type2}:\")\n",
    "    print(f\"  Mean accuracy: {mean1:.3f} vs {mean2:.3f} (diff: {mean_diff:+.3f})\")\n",
    "    print(f\"  Sample sizes: {len(data1)} vs {len(data2)}\")\n",
    "    print(f\"  p-value: {p_value:.4f} {significance} ({sig_level})\")\n",
    "    print()\n",
    "\n",
    "# Create a summary table\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\nSummary Table of Pairwise Comparisons:\")\n",
    "print(\"=\" * 60)\n",
    "print(results_df.round(4).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrices for significance analysis\n",
    "n_types = len(spotter_types)\n",
    "significance_matrix = np.full((n_types, n_types), \"\", dtype=object)\n",
    "mean_diff_matrix = np.zeros((n_types, n_types))\n",
    "\n",
    "# Fill the matrices\n",
    "for i, type1 in enumerate(spotter_types):\n",
    "    for j, type2 in enumerate(spotter_types):\n",
    "        if i != j:  # Don't compare a type with itself\n",
    "            # Find the comparison in our results\n",
    "            comparison1 = f\"{type1} vs {type2}\"\n",
    "            comparison2 = f\"{type2} vs {type1}\"\n",
    "\n",
    "            # Find the result (either direction)\n",
    "            result = results_df[\n",
    "                (results_df['Comparison'] == comparison1) |\n",
    "                (results_df['Comparison'] == comparison2)\n",
    "            ]\n",
    "\n",
    "            if not result.empty:\n",
    "                sig = result.iloc[0]['Significance']\n",
    "                significance_matrix[i, j] = sig\n",
    "\n",
    "                # Use the difference from the perspective of j vs i (column vs row)\n",
    "                if result.iloc[0]['Comparison'] == comparison2:\n",
    "                    mean_diff_matrix[i, j] = result.iloc[0]['Difference']\n",
    "                else:\n",
    "                    mean_diff_matrix[i, j] = -result.iloc[0]['Difference']\n",
    "\n",
    "# Reverse the order of spotter types (flip rows and columns)\n",
    "spotter_types_reversed = spotter_types[::-1]\n",
    "\n",
    "# Create reversed indices mapping\n",
    "reverse_idx = {i: n_types - 1 - i for i in range(n_types)}\n",
    "\n",
    "# Create new matrices with reversed order\n",
    "mean_diff_matrix_rev = np.zeros((n_types, n_types))\n",
    "significance_matrix_rev = np.full((n_types, n_types), \"\", dtype=object)\n",
    "\n",
    "for i in range(n_types):\n",
    "    for j in range(n_types):\n",
    "        rev_i = reverse_idx[i]\n",
    "        rev_j = reverse_idx[j]\n",
    "        mean_diff_matrix_rev[i, j] = mean_diff_matrix[rev_i, rev_j]\n",
    "        significance_matrix_rev[i, j] = significance_matrix[rev_i, rev_j]\n",
    "\n",
    "# Create annotations that combine significance and mean difference\n",
    "annotations = np.full((n_types, n_types), \"\", dtype=object)\n",
    "for i in range(n_types):\n",
    "    for j in range(n_types):\n",
    "        if i == j:\n",
    "            # Diagonal indicator - show the spotter type name\n",
    "            annotations[i, j] = f\"--\"\n",
    "        else:\n",
    "            sig = significance_matrix_rev[i, j]\n",
    "            diff = mean_diff_matrix_rev[i, j]\n",
    "            annotations[i, j] = f\"{diff:+.3f}\\n{sig}\" if sig else f\"{diff:+.3f}\"\n",
    "\n",
    "# Create a mask for the lower triangle (excluding diagonal to preserve it)\n",
    "mask = np.tril(np.ones_like(mean_diff_matrix_rev, dtype=bool), k=-1)\n",
    "\n",
    "# Create the figure with more space for the legend\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Create the upper-triangular heatmap\n",
    "sns.heatmap(\n",
    "    mean_diff_matrix_rev,\n",
    "    mask=mask,\n",
    "    annot=annotations,\n",
    "    fmt='',\n",
    "    xticklabels=spotter_types_reversed,\n",
    "    yticklabels=spotter_types_reversed,\n",
    "    cmap='RdBu',\n",
    "    center=0,\n",
    "    ax=ax,\n",
    "    cbar_kws={'label': 'Mean Accuracy Difference'},\n",
    "    square=True\n",
    ")\n",
    "\n",
    "ax.set_title(\"Spotter Accuracy Differences\", fontsize=16)\n",
    "ax.set_xlabel('', fontsize=12)\n",
    "ax.set_ylabel('', fontsize=12)\n",
    "\n",
    "# Add legend text\n",
    "legend_text = \"\"\"Statistical Significance:\n",
    "*** p < 0.001 (highly significant)\n",
    "**  p < 0.01 (very significant)\n",
    "*   p < 0.05 (significant)\n",
    "    p ≥ 0.05 (not significant)\"\"\"\n",
    "\n",
    "# Position the legend in the lower left corner of the plot\n",
    "ax.text(0.02, 0.02, legend_text, transform=ax.transAxes, fontsize=12,\n",
    "        verticalalignment='bottom', bbox=dict(facecolor='white', alpha=0.9, edgecolor='gray'), fontfamily='monospace')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\n",
    "    os.path.join(PATH_EXPORT, \"spotter_accuracy_differences.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1823fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c46da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "583fa852",
   "metadata": {},
   "source": [
    "### Performance by Gold Label (Across All Models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from battleship.agents import Answer\n",
    "\n",
    "NA_LABEL = \"Simple\"\n",
    "show_legend = False\n",
    "\n",
    "# Build multilabel subsets for gold categories, including an Overall and N/A bucket\n",
    "LABEL_MAP = {\n",
    "    NA_LABEL: None,\n",
    "    \"Discourse\": \"gold_discourse\",\n",
    "    \"Stateful\": \"gold_stateful\",\n",
    "    \"Vague\": \"gold_vague\",\n",
    "    \"Ambiguous\": \"gold_ambiguous\",\n",
    "}\n",
    "\n",
    "# Determine rows that have no gold labels set (all False or missing)\n",
    "gold_label_cols = [v for v in LABEL_MAP.values() if v is not None]\n",
    "rows_with_any_label = df[gold_label_cols].fillna(False).any(axis=1)\n",
    "\n",
    "# Create a long-form dataframe for model results\n",
    "model_records = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Overall bucket (always include)\n",
    "    model_records.append({\n",
    "        \"gold_label\": \"Overall\",\n",
    "        \"spotter_type_short\": row[\"spotter_type_short\"],\n",
    "        \"llm_display_name\": row[\"llm_display_name\"],\n",
    "        \"is_correct\": row[\"is_correct\"],\n",
    "    })\n",
    "\n",
    "    has_any = bool(rows_with_any_label.loc[idx])\n",
    "    if not has_any:\n",
    "        # N/A bucket\n",
    "        model_records.append({\n",
    "            \"gold_label\": NA_LABEL,\n",
    "            \"spotter_type_short\": row[\"spotter_type_short\"],\n",
    "            \"llm_display_name\": row[\"llm_display_name\"],\n",
    "            \"is_correct\": row[\"is_correct\"],\n",
    "        })\n",
    "    else:\n",
    "        for nice_name, col in LABEL_MAP.items():\n",
    "            if col is None:\n",
    "                continue\n",
    "            val = row[col]\n",
    "            if pd.notna(val) and bool(val):\n",
    "                model_records.append({\n",
    "                    \"gold_label\": nice_name,\n",
    "                    \"spotter_type_short\": row[\"spotter_type_short\"],\n",
    "                    \"llm_display_name\": row[\"llm_display_name\"],\n",
    "                    \"is_correct\": row[\"is_correct\"],\n",
    "                })\n",
    "\n",
    "model_long_df = pd.DataFrame.from_records(model_records)\n",
    "\n",
    "# Build a comparable human dataset per subset (treat humans as another spotter model)\n",
    "# Use df_gold to compute per-question human correctness and gold labels\n",
    "human_df = df_gold.copy()\n",
    "human_df = human_df[human_df[\"messageType\"] == \"answer\"].copy()\n",
    "human_df = human_df[~pd.isna(human_df[\"gold_answer\"]) & ~pd.isna(human_df[\"messageText\"])].copy()\n",
    "\n",
    "# Parse answers\n",
    "human_df[\"gold_answer_value\"] = human_df[\"gold_answer\"].apply(Answer.parse)\n",
    "human_df[\"human_answer_value\"] = human_df[\"messageText\"].apply(Answer.parse)\n",
    "# Drop rows where human parse failed\n",
    "human_df = human_df[human_df[\"human_answer_value\"].isin([True, False])]\n",
    "\n",
    "# Determine rows that have no gold labels set\n",
    "human_rows_with_any_label = human_df[gold_label_cols].fillna(False).any(axis=1)\n",
    "\n",
    "human_records = []\n",
    "for idx, row in human_df.iterrows():\n",
    "    is_correct = bool(row[\"human_answer_value\"] == row[\"gold_answer_value\"])\n",
    "\n",
    "    # Overall bucket\n",
    "    human_records.append({\n",
    "        \"gold_label\": \"Overall\",\n",
    "        \"spotter_type_short\": \"Human\",\n",
    "        \"llm_display_name\": \"Human\",\n",
    "        \"is_correct\": is_correct,\n",
    "    })\n",
    "\n",
    "    has_any = bool(human_rows_with_any_label.loc[idx])\n",
    "    if not has_any:\n",
    "        human_records.append({\n",
    "            \"gold_label\": NA_LABEL,\n",
    "            \"spotter_type_short\": \"Human\",\n",
    "            \"llm_display_name\": \"Human\",\n",
    "            \"is_correct\": is_correct,\n",
    "        })\n",
    "    else:\n",
    "        for nice_name, col in LABEL_MAP.items():\n",
    "            if col is None:\n",
    "                continue\n",
    "            val = row[col]\n",
    "            if pd.notna(val) and bool(val):\n",
    "                human_records.append({\n",
    "                    \"gold_label\": nice_name,\n",
    "                    \"spotter_type_short\": \"Human\",\n",
    "                    \"llm_display_name\": \"Human\",\n",
    "                    \"is_correct\": is_correct,\n",
    "                })\n",
    "\n",
    "human_long_df = pd.DataFrame.from_records(human_records)\n",
    "\n",
    "# Combine model and human long-form data\n",
    "combined_long_df = pd.concat([model_long_df, human_long_df], ignore_index=True)\n",
    "\n",
    "# Categorical ordering\n",
    "label_order = [\"Overall\", NA_LABEL, \"Discourse\", \"Stateful\", \"Vague\", \"Ambiguous\"]\n",
    "combined_long_df[\"gold_label\"] = pd.Categorical(combined_long_df[\"gold_label\"], categories=label_order, ordered=True)\n",
    "\n",
    "# Ensure spotter type order includes Human first, then the model types in existing order\n",
    "existing_order = list(df[\"spotter_type_short\"].cat.categories)\n",
    "spotter_order = existing_order + [\"Human\"]\n",
    "combined_long_df[\"spotter_type_short\"] = pd.Categorical(combined_long_df[\"spotter_type_short\"], categories=spotter_order, ordered=True)\n",
    "\n",
    "# Plot grouped bar chart with per-question 95% CIs (no per-model aggregation)\n",
    "plt.figure(figsize=(8, 6))\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    sns.barplot(\n",
    "        data=combined_long_df,\n",
    "        x=\"gold_label\",\n",
    "        y=\"is_correct\",\n",
    "        hue=\"spotter_type_short\",\n",
    "        errorbar=(\"ci\", 95),\n",
    "        err_kws={\"linewidth\": 1.5},\n",
    "        capsize=0.1,\n",
    "        legend=show_legend,\n",
    "    )\n",
    "\n",
    "# plt.title(\"Accuracy by Gold Label (Per-question 95% CI, incl. Human)\", fontsize=14)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(rotation=0)\n",
    "if show_legend:\n",
    "    plt.legend(title=\"Spotter Models\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\n",
    "    os.path.join(PATH_EXPORT, \"spotter_accuracy_by_gold_label.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n",
    "\n",
    "# Print summary table for quick inspection\n",
    "summary_table = (\n",
    "    combined_long_df\n",
    "    .groupby([\"gold_label\", \"spotter_type_short\"], observed=False)[\"is_correct\"]\n",
    "    .agg([\"count\", \"mean\", \"std\"]).round(3)\n",
    ")\n",
    "summary_table[\"mean_pct\"] = (summary_table[\"mean\"] * 100).round(1)\n",
    "print(summary_table)\n",
    "\n",
    "# Gold label prevalence (share of human answers with each gold label)\n",
    "# Note: Multilabel, so percentages can sum to >100%. Includes the N/A bucket for none.\n",
    "label_prevalence_records = []\n",
    "\n",
    "total_answers = len(human_df)\n",
    "any_label_mask = human_rows_with_any_label\n",
    "\n",
    "# Overall (all questions)\n",
    "overall_count = int(total_answers)\n",
    "label_prevalence_records.append({\n",
    "    \"gold_label\": \"Overall\",\n",
    "    \"count\": overall_count,\n",
    "    \"percent\": 100.00,\n",
    "})\n",
    "\n",
    "# N/A first\n",
    "na_count = int((~any_label_mask).sum())\n",
    "label_prevalence_records.append({\n",
    "    \"gold_label\": NA_LABEL,\n",
    "    \"count\": na_count,\n",
    "    \"percent\": round(100.0 * na_count / total_answers, 1),\n",
    "})\n",
    "\n",
    "# Other labels\n",
    "for nice_name, col in LABEL_MAP.items():\n",
    "    if col is None:\n",
    "        continue\n",
    "    count = int(human_df[col].fillna(False).sum())\n",
    "    label_prevalence_records.append({\n",
    "        \"gold_label\": nice_name,\n",
    "        \"count\": count,\n",
    "        \"percent\": round(100.0 * count / total_answers, 1),\n",
    "    })\n",
    "\n",
    "prevalence_df = pd.DataFrame(label_prevalence_records)\n",
    "# Order using existing label_order (skipping Overall if present)\n",
    "order = [lbl for lbl in [\"Overall\", NA_LABEL, \"Discourse\", \"Stateful\", \"Vague\", \"Ambiguous\"] if lbl in prevalence_df[\"gold_label\"].unique()]\n",
    "prevalence_df[\"gold_label\"] = pd.Categorical(prevalence_df[\"gold_label\"], categories=order, ordered=True)\n",
    "prevalence_df = prevalence_df.sort_values(\"gold_label\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\nGold Label Prevalence (human data):\")\n",
    "print(prevalence_df.to_string(index=False))\n",
    "\n",
    "# Save prevalence to LaTeX with 2 decimal precision (preserving trailing zeroes)\n",
    "latex_str = prevalence_df.to_latex(index=False, float_format=lambda x: f\"{x:.1f}\")\n",
    "with open(os.path.join(PATH_EXPORT, \"gold_label_prevalence.tex\"), \"w\") as f:\n",
    "    f.write(latex_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da94b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f47d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from battleship.agents import Answer\n",
    "\n",
    "SIMPLE_LABEL = \"Simple\"\n",
    "COMPLEX_LABEL = \"Complex\"\n",
    "show_legend = False\n",
    "\n",
    "# Columns that mark complex gold labels\n",
    "LABEL_MAP = {\n",
    "    SIMPLE_LABEL: None,\n",
    "    \"Discourse\": \"gold_discourse\",\n",
    "    \"Stateful\": \"gold_stateful\",\n",
    "    \"Vague\": \"gold_vague\",\n",
    "    \"Ambiguous\": \"gold_ambiguous\",\n",
    "}\n",
    "\n",
    "gold_label_cols = [v for v in LABEL_MAP.values() if v is not None]\n",
    "\n",
    "# -----------------------------\n",
    "# Build long-form collapsed dataset for models\n",
    "# -----------------------------\n",
    "model_records = []\n",
    "any_complex = df[gold_label_cols].fillna(False).any(axis=1)\n",
    "for idx, row in df.iterrows():\n",
    "    difficulty = COMPLEX_LABEL if bool(any_complex.loc[idx]) else SIMPLE_LABEL\n",
    "    model_records.append({\n",
    "        \"difficulty\": difficulty,\n",
    "        \"spotter_type_short\": row[\"spotter_type_short\"],\n",
    "        \"llm_display_name\": row[\"llm_display_name\"],\n",
    "        \"is_correct\": row[\"is_correct\"],\n",
    "    })\n",
    "\n",
    "model_long_df = pd.DataFrame.from_records(model_records)\n",
    "\n",
    "# -----------------------------\n",
    "# Build comparable collapsed dataset for humans\n",
    "# -----------------------------\n",
    "human_df = df_gold.copy()\n",
    "human_df = human_df[human_df[\"messageType\"] == \"answer\"].copy()\n",
    "human_df = human_df[~pd.isna(human_df[\"gold_answer\"]) & ~pd.isna(human_df[\"messageText\"])].copy()\n",
    "\n",
    "# Parse answers\n",
    "human_df[\"gold_answer_value\"] = human_df[\"gold_answer\"].apply(Answer.parse)\n",
    "human_df[\"human_answer_value\"] = human_df[\"messageText\"].apply(Answer.parse)\n",
    "# Keep only rows where human parse succeeded\n",
    "human_df = human_df[human_df[\"human_answer_value\"].isin([True, False])]\n",
    "\n",
    "human_any_complex = human_df[gold_label_cols].fillna(False).any(axis=1)\n",
    "\n",
    "human_records = []\n",
    "for idx, row in human_df.iterrows():\n",
    "    is_correct = bool(row[\"human_answer_value\"] == row[\"gold_answer_value\"])\n",
    "    difficulty = COMPLEX_LABEL if bool(human_any_complex.loc[idx]) else SIMPLE_LABEL\n",
    "    human_records.append({\n",
    "        \"difficulty\": difficulty,\n",
    "        \"spotter_type_short\": \"Human\",\n",
    "        \"llm_display_name\": \"Human\",\n",
    "        \"is_correct\": is_correct,\n",
    "    })\n",
    "\n",
    "human_long_df = pd.DataFrame.from_records(human_records)\n",
    "\n",
    "# -----------------------------\n",
    "# Combine and plot\n",
    "# -----------------------------\n",
    "collapsed_long_df = pd.concat([model_long_df, human_long_df], ignore_index=True)\n",
    "\n",
    "# Order categories\n",
    "difficulty_order = [SIMPLE_LABEL, COMPLEX_LABEL]\n",
    "collapsed_long_df[\"difficulty\"] = pd.Categorical(collapsed_long_df[\"difficulty\"], categories=difficulty_order, ordered=True)\n",
    "\n",
    "existing_order = list(df[\"spotter_type_short\"].cat.categories)\n",
    "spotter_order = existing_order + [\"Human\"]\n",
    "collapsed_long_df[\"spotter_type_short\"] = pd.Categorical(collapsed_long_df[\"spotter_type_short\"], categories=spotter_order, ordered=True)\n",
    "\n",
    "# Point plot with tuned styling\n",
    "plt.figure(figsize=(4, 6))\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    ax = sns.pointplot(\n",
    "        data=collapsed_long_df,\n",
    "        x=\"difficulty\",\n",
    "        y=\"is_correct\",\n",
    "        hue=\"spotter_type_short\",\n",
    "        errorbar=(\"ci\", 95),\n",
    "        err_kws={\"linewidth\": 1.5},\n",
    "        linewidth=3,\n",
    "        linestyle=\"-\",\n",
    "        markers=\"o\",\n",
    "        markersize=5,\n",
    "        capsize=0.1,\n",
    "        legend=show_legend,\n",
    "    )\n",
    "\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels([SIMPLE_LABEL, COMPLEX_LABEL])\n",
    "\n",
    "if show_legend:\n",
    "    ax.legend(title=\"Spotter Models\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "# sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\n",
    "    os.path.join(PATH_EXPORT, \"spotter_accuracy_simple_vs_complex.pdf\"),\n",
    "    bbox_inches=\"tight\",\n",
    "    dpi=300,\n",
    ")\n",
    "\n",
    "# Print a compact summary table\n",
    "summary_table = (\n",
    "    collapsed_long_df\n",
    "    .groupby([\"difficulty\", \"spotter_type_short\"], observed=False)[\"is_correct\"]\n",
    "    .agg([\"count\", \"mean\", \"std\"]).round(3)\n",
    ")\n",
    "summary_table[\"mean_pct\"] = (summary_table[\"mean\"] * 100).round(1)\n",
    "print(summary_table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05e836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
