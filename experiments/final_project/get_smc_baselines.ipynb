{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from hfppl.llms import CachedCausalLM\n",
    "from hfppl.inference import smc_standard\n",
    "\n",
    "from battleship.v1.board import Board\n",
    "from battleship.scoring import compute_score\n",
    "from battleship.models import QuestionGenerationModel, SingleStepQuestionGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HF_AUTH_TOKEN from .hf_auth_token\n",
    "with open(os.path.join(\"../../\", \".hf_auth_token\"), \"r\") as f:\n",
    "    os.environ[\"HF_AUTH_TOKEN\"] = f.read().strip()\n",
    "\n",
    "HF_AUTH_TOKEN = os.environ[\"HF_AUTH_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/battleship/.venv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/battleship/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c622077451134b1c9d296a013e598625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the HuggingFace model\n",
    "lm = CachedCausalLM.from_pretrained(\"codellama/CodeLlama-13b-hf\", auth_token=HF_AUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../battleship/prompts/examples.csv\")\n",
    "\n",
    "def format_example(user_input: str, response: str = None):\n",
    "    return f\"User: {user_input}\\n\" f\"Assistant:{' ' + response if response else ''}\"\n",
    "\n",
    "def make_question_prompt(df, board=None, instructions=None):\n",
    "    prompt = \"\"\n",
    "    if instructions != None:\n",
    "        prompt += f\"Instructions:\\n{instructions}\\n\"\n",
    "    if board != None:\n",
    "        prompt += \"Board:\\n\" + board.to_textual_description() + \"\\n\"\n",
    "    prompt += \"Questions:\\n\" + \"\\n\".join(df.question) + \"\\n\"\n",
    "    return prompt\n",
    "\n",
    "def make_question_to_code_prompt(df):\n",
    "    prompt = \"\\n\".join([format_example(q, r) for q, r in zip(df.question, df.code)]) + \"\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"User input will be a series of sentences representing a board from Battleship, the board game, that you should aim to win. Tiles in the board can either be 'Water' tiles, 'Blue Ship' tiles, 'Red Ship' tiles, and 'Purple Ship' tiles (there are only these three battleships). Some tiles may also be 'Hidden' tiles, meaning they could be any of the others but have not been revealed yet. The user will denote coordinates as follows: columns are numbered from 1 onwards, where column 1 is the leftmost column, and rows are given a letter from A onwards where row A is the topmost row (so the cell at the second row and second column is B2). Your role is to ask the most informative possible question from the context given: strictly output the question only, and make sure the questions are relevant to the context: 'Which cells should I target to sink the battleships with the least number of moves?' is not a relevant question because it is the general goal of Battleship. Questions also need to be answerable with yes or no, no other questions will be considered in scope.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single board evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def single_smc_baseline(board_id,particle_num,instructions):\n",
    "    board = Board.from_trial_id(board_id)\n",
    "    board.to_textual_description()\n",
    "\n",
    "    model = QuestionGenerationModel(\n",
    "        lm=lm,\n",
    "        board=board,\n",
    "        question_prompt=make_question_prompt(df),\n",
    "        translation_prompt=make_question_to_code_prompt(df),\n",
    "    )\n",
    "\n",
    "    model_combined = QuestionGenerationModel(\n",
    "        lm=lm,\n",
    "        board=board,\n",
    "        question_prompt=make_question_prompt(df,board=board,instructions=instructions),\n",
    "        translation_prompt=make_question_to_code_prompt(df),\n",
    "    )\n",
    "\n",
    "    particles = await smc_standard(model, n_particles=particle_num)\n",
    "    print(\"Done with standard model...\")\n",
    "    particles_c = await smc_standard(model_combined, n_particles=particle_num)\n",
    "    print(\"Done with combined model...\")\n",
    "    return [particles,particles_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = []\n",
    "particle_types = [particles,particles_c]\n",
    "\n",
    "for particle_type in particle_types:\n",
    "    for i, p in enumerate(particle_type):\n",
    "        df_p = pd.DataFrame(p.get_final_results())\n",
    "        df_p[\"particle\"] = i\n",
    "        df_results.append(df_p)\n",
    "    df_results = pd.concat(df_results).reset_index(drop=True)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in particles:\n",
    "    print(f\"Question: {str(p.context)}\")\n",
    "    print(f\"|- Program: {p.result['translation']}\")\n",
    "    print(f\"|- EIG: {compute_score(board=board, program=p.result['translation'])}\")\n",
    "    print(f\"|- Particle weight: {p.weight:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple board evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_smc_baseline(n_particles=5, trial_ids=range(1, 19), model_types=[\"REGULAR\", \"COMBINED\"], model_cls = QuestionGenerationModel, results_file = \"hfppl_results.csv\", verbose = False):\n",
    "    results_all = []\n",
    "    for trial_id in trial_ids:\n",
    "        for model_type in model_types:\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"Trial {trial_id}\")\n",
    "            print(f\"Model type: {model_type}\")\n",
    "            print(\"-\" * 80)\n",
    "            board = Board.from_trial_id(trial_id)\n",
    "            instructions_used = None if model_type == \"REGULAR\" else instructions\n",
    "            model = model_cls(\n",
    "                    lm=lm,\n",
    "                    board=board,\n",
    "                    question_prompt=make_question_prompt(df, board=board, instructions=instructions_used),\n",
    "                    translation_prompt=make_question_to_code_prompt(df),\n",
    "                    verbose=verbose,\n",
    "                )\n",
    "            particles = await smc_standard(model, n_particles=n_particles)\n",
    "            results_trial = []\n",
    "            for i, p in enumerate(particles):\n",
    "                df_p = pd.DataFrame(p.get_final_results())\n",
    "                df_p[\"particle\"] = i\n",
    "                df_p[\"model_type\"] = model_type\n",
    "                results_trial.append(df_p)\n",
    "            df_trial = pd.concat(results_trial).reset_index(drop=True)\n",
    "            df_trial[\"trial_id\"] = trial_id\n",
    "            results_all.append(df_trial)\n",
    "            df_results = pd.concat(results_all).reset_index(drop=True)\n",
    "            df_results.to_csv(results_file, index=False)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIAL_IDS = range(1, 19)\n",
    "N_PARTICLES = 5\n",
    "\n",
    "await run_smc_baseline(n_particles=N_PARTICLES, trial_ids=TRIAL_IDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-step SMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Trial 1\n",
      "Model type: COMBINED\n",
      "--------------------------------------------------------------------------------\n",
      "Does\n",
      "The\n",
      "Does\n",
      "Does\n",
      "Wh\n",
      "Wh\n",
      "Wh\n",
      "How\n",
      "What\n",
      "What\n",
      "Is\n",
      "Is\n",
      "How\n",
      "Is\n",
      "What\n",
      "Can\n",
      "How\n",
      "Are\n",
      "Wh\n",
      "Are\n",
      "Are\n",
      "Are\n",
      "How\n",
      "Wh\n",
      "Does\n",
      "What\n",
      "Does the\n",
      "How\n",
      "\n",
      "\n",
      "Where\n",
      "Input\n",
      "How\n",
      "Do\n",
      "Qu\n",
      "Is\n",
      "How\n",
      "Are\n",
      "Test\n",
      "Al\n",
      "is\n",
      "Qu\n",
      "How\n",
      "What\n",
      "How\n",
      "\n",
      "\n",
      "The other\n",
      "What\n",
      "Are\n",
      "Are\n",
      "Is\n",
      "What\n",
      "How\n",
      "How\n",
      "What\n",
      "What\n",
      "How\n",
      "Wh\n",
      "How\n",
      "\n",
      "\n",
      "Does the\n",
      "How\n",
      "What\n",
      "How\n",
      "Does the\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacty of 22.19 GiB of which 835.50 MiB is free. Including non-PyTorch memory, this process has 21.37 GiB memory in use. Of the allocated memory 19.53 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# TRIAL_IDS = [13]\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m N_PARTICLES \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m df_results \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m run_smc_baseline(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     n_particles\u001b[39m=\u001b[39mN_PARTICLES,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     trial_ids\u001b[39m=\u001b[39mTRIAL_IDS,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     model_types\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCOMBINED\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     model_cls\u001b[39m=\u001b[39mSingleStepQuestionGenerationModel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     results_file\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhfppl_results_single_step.csv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "\u001b[1;32m/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m instructions_used \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m model_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mREGULAR\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m instructions\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m model \u001b[39m=\u001b[39m model_cls(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         lm\u001b[39m=\u001b[39mlm,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         board\u001b[39m=\u001b[39mboard,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         question_prompt\u001b[39m=\u001b[39mmake_question_prompt(df, board\u001b[39m=\u001b[39mboard, instructions\u001b[39m=\u001b[39minstructions_used),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         translation_prompt\u001b[39m=\u001b[39mmake_question_to_code_prompt(df),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m particles \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m smc_standard(model, n_particles\u001b[39m=\u001b[39mn_particles)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m results_trial \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgabe-battleship-v1/home/ubuntu/battleship/experiments/final_project/get_smc_baselines.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, p \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(particles):\n",
      "File \u001b[0;32m~/battleship/hfppl/hfppl/inference/smc_standard.py:25\u001b[0m, in \u001b[0;36msmc_standard\u001b[0;34m(model, n_particles, ess_threshold)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m particles:\n\u001b[1;32m     24\u001b[0m     p\u001b[39m.\u001b[39muntwist()\n\u001b[0;32m---> 25\u001b[0m \u001b[39mawait\u001b[39;00m asyncio\u001b[39m.\u001b[39mgather(\u001b[39m*\u001b[39m[p\u001b[39m.\u001b[39mstep() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m particles \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m p\u001b[39m.\u001b[39mdone_stepping()])\n\u001b[1;32m     27\u001b[0m \u001b[39m# Normalize weights\u001b[39;00m\n\u001b[1;32m     28\u001b[0m W \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([p\u001b[39m.\u001b[39mweight \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m particles])\n",
      "File \u001b[0;32m~/battleship/battleship/models.py:174\u001b[0m, in \u001b[0;36mSingleStepQuestionGenerationModel.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    173\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m         token \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext\u001b[39m.\u001b[39mnext_token())\n\u001b[1;32m    175\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    176\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext)\n",
      "File \u001b[0;32m~/battleship/hfppl/hfppl/modeling.py:177\u001b[0m, in \u001b[0;36mModel.sample\u001b[0;34m(self, dist, proposal)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m# Special logic for beam search\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m# if self.mode == \"beam\":\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m#     d = dist if proposal is None else proposal\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39m#         self.score(w)\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m#     return x\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m proposal \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     x, _ \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m dist\u001b[39m.\u001b[39msample()\n\u001b[1;32m    178\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m    179\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/battleship/hfppl/hfppl/distributions/lmcontext.py:31\u001b[0m, in \u001b[0;36mLMNextToken.sample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39ms \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m t\n\u001b[1;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39mmodel_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39mNO_MASK\n\u001b[0;32m---> 31\u001b[0m updated_logprobs \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39mlm\u001b[39m.\u001b[39mnext_token_logprobs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39ms\u001b[39m.\u001b[39mseq)\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39mnext_token_logprobs \u001b[39m=\u001b[39m log_softmax(updated_logprobs \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39mtemp)\n\u001b[1;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m t, logprob\n",
      "File \u001b[0;32m~/battleship/hfppl/hfppl/llms.py:364\u001b[0m, in \u001b[0;36mCachedCausalLM.next_token_logprobs\u001b[0;34m(self, token_ids)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39m# Create a future with the prompt\u001b[39;00m\n\u001b[1;32m    363\u001b[0m future \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mget_running_loop()\u001b[39m.\u001b[39mcreate_future()\n\u001b[0;32m--> 364\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_query(token_ids[base:], future, past)\n\u001b[1;32m    365\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m future\n\u001b[1;32m    367\u001b[0m \u001b[39m# Create new nodes\u001b[39;00m\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/battleship/hfppl/hfppl/llms.py:319\u001b[0m, in \u001b[0;36mCachedCausalLM.add_query\u001b[0;34m(self, query, future, past)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqueries) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size:\n\u001b[0;32m--> 319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_evaluate_queries()\n\u001b[1;32m    320\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimer \u001b[39m=\u001b[39m asyncio\u001b[39m.\u001b[39mget_running_loop()\u001b[39m.\u001b[39mcall_later(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout, \u001b[39mlambda\u001b[39;00m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_evaluate_queries())\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/battleship/hfppl/hfppl/llms.py:304\u001b[0m, in \u001b[0;36mCachedCausalLM.batch_evaluate_queries\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m     pasts \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(input_ids, attention_mask\u001b[39m=\u001b[39;49mattn_masks,\n\u001b[1;32m    305\u001b[0m                      position_ids\u001b[39m=\u001b[39;49mposn_ids, past_key_values\u001b[39m=\u001b[39;49mpasts,\n\u001b[1;32m    306\u001b[0m                      use_cache\u001b[39m=\u001b[39;49mpasts \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    308\u001b[0m \u001b[39mfor\u001b[39;00m (i, q) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(queries):\n\u001b[1;32m    309\u001b[0m     q\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_result(results\u001b[39m.\u001b[39mlogits[i])\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1034\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1031\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1033\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1035\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1036\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1037\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1038\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1039\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1040\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1041\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1042\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1043\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1044\u001b[0m )\n\u001b[1;32m   1046\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1047\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:922\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    912\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    913\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         use_cache,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    923\u001b[0m         hidden_states,\n\u001b[1;32m    924\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    925\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    926\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    927\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    928\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    929\u001b[0m     )\n\u001b[1;32m    931\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:672\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    671\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    673\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    674\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    675\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    676\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    677\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    678\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    679\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    680\u001b[0m )\n\u001b[1;32m    681\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    683\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:406\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    405\u001b[0m \u001b[39m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attn_weights, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39mto(query_states\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    407\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    409\u001b[0m \u001b[39mif\u001b[39;00m attn_output\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim):\n",
      "File \u001b[0;32m~/battleship/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim)\n\u001b[1;32m   1857\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m   1859\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.55 GiB. GPU 0 has a total capacty of 22.19 GiB of which 835.50 MiB is free. Including non-PyTorch memory, this process has 21.37 GiB memory in use. Of the allocated memory 19.53 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "TRIAL_IDS = range(1, 19)\n",
    "# TRIAL_IDS = [13]\n",
    "N_PARTICLES = 100\n",
    "\n",
    "df_results = await run_smc_baseline(\n",
    "    n_particles=N_PARTICLES,\n",
    "    trial_ids=TRIAL_IDS,\n",
    "    model_types=[\"COMBINED\"],\n",
    "    model_cls=SingleStepQuestionGenerationModel,\n",
    "    results_file=\"hfppl_results_single_step.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>completion</th>\n",
       "      <th>translation</th>\n",
       "      <th>score</th>\n",
       "      <th>type</th>\n",
       "      <th>particle</th>\n",
       "      <th>model_type</th>\n",
       "      <th>trial_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>How many blocks are there?</td>\n",
       "      <td>How many blocks are there?</td>\n",
       "      <td>(++ (map (lambda x0 (size x0)) (set AllColors)))</td>\n",
       "      <td>2.029328</td>\n",
       "      <td>final</td>\n",
       "      <td>27</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Note: Take cargo space into consideration.</td>\n",
       "      <td>Note: Take cargo space into consideration.</td>\n",
       "      <td>(++ (map (lambda x0 (size x0)) (set AllColors)))</td>\n",
       "      <td>2.029328</td>\n",
       "      <td>final</td>\n",
       "      <td>55</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Note: Take cargo space into consideration.</td>\n",
       "      <td>Note: Take cargo space into consideration.</td>\n",
       "      <td>(++ (map (lambda x0 (size x0)) (set AllColors)))</td>\n",
       "      <td>2.029328</td>\n",
       "      <td>final</td>\n",
       "      <td>29</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>How many pieces could the purple deck fit into...</td>\n",
       "      <td>How many pieces could the purple deck fit into...</td>\n",
       "      <td>(++ (map (lambda x0 (size x0)) (set AllColors)))</td>\n",
       "      <td>2.029328</td>\n",
       "      <td>final</td>\n",
       "      <td>30</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>How many total tiles can be seen?</td>\n",
       "      <td>How many total tiles can be seen?</td>\n",
       "      <td>(++ (map (lambda x0 (size x0)) (set AllColors)))</td>\n",
       "      <td>2.029328</td>\n",
       "      <td>final</td>\n",
       "      <td>31</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>(== (size Red) 3)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>final</td>\n",
       "      <td>73</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>\\end{code}\\n</td>\n",
       "      <td>\\end{code}\\n</td>\n",
       "      <td>(== (size Red) 3)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>final</td>\n",
       "      <td>19</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Re: Board Game Logic Tile Problem Unsolved\\n</td>\n",
       "      <td>Re: Board Game Logic Tile Problem Unsolved\\n</td>\n",
       "      <td>(define (coloredTiles color)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>final</td>\n",
       "      <td>75</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Is there a ship at 2E?</td>\n",
       "      <td>Is there a ship at 2E?</td>\n",
       "      <td>(not (== (color 2-E) Water))</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>final</td>\n",
       "      <td>76</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>(== (size Red) 3)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>final</td>\n",
       "      <td>50</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prefix  \\\n",
       "27                         How many blocks are there?   \n",
       "55         Note: Take cargo space into consideration.   \n",
       "29         Note: Take cargo space into consideration.   \n",
       "30  How many pieces could the purple deck fit into...   \n",
       "31                  How many total tiles can be seen?   \n",
       "..                                                ...   \n",
       "73                                                 \\n   \n",
       "19                                       \\end{code}\\n   \n",
       "75       Re: Board Game Logic Tile Problem Unsolved\\n   \n",
       "76                             Is there a ship at 2E?   \n",
       "50                                                 \\n   \n",
       "\n",
       "                                           completion  \\\n",
       "27                         How many blocks are there?   \n",
       "55         Note: Take cargo space into consideration.   \n",
       "29         Note: Take cargo space into consideration.   \n",
       "30  How many pieces could the purple deck fit into...   \n",
       "31                  How many total tiles can be seen?   \n",
       "..                                                ...   \n",
       "73                                                 \\n   \n",
       "19                                       \\end{code}\\n   \n",
       "75       Re: Board Game Logic Tile Problem Unsolved\\n   \n",
       "76                             Is there a ship at 2E?   \n",
       "50                                                 \\n   \n",
       "\n",
       "                                         translation     score   type  \\\n",
       "27  (++ (map (lambda x0 (size x0)) (set AllColors)))  2.029328  final   \n",
       "55  (++ (map (lambda x0 (size x0)) (set AllColors)))  2.029328  final   \n",
       "29  (++ (map (lambda x0 (size x0)) (set AllColors)))  2.029328  final   \n",
       "30  (++ (map (lambda x0 (size x0)) (set AllColors)))  2.029328  final   \n",
       "31  (++ (map (lambda x0 (size x0)) (set AllColors)))  2.029328  final   \n",
       "..                                               ...       ...    ...   \n",
       "73                                 (== (size Red) 3)  0.000000  final   \n",
       "19                                 (== (size Red) 3)  0.000000  final   \n",
       "75                      (define (coloredTiles color)  0.000000  final   \n",
       "76                      (not (== (color 2-E) Water))  0.000000  final   \n",
       "50                                 (== (size Red) 3)  0.000000  final   \n",
       "\n",
       "    particle model_type  trial_id  \n",
       "27        27    REGULAR        13  \n",
       "55        55    REGULAR        13  \n",
       "29        29    REGULAR        13  \n",
       "30        30    REGULAR        13  \n",
       "31        31    REGULAR        13  \n",
       "..       ...        ...       ...  \n",
       "73        73    REGULAR        13  \n",
       "19        19    REGULAR        13  \n",
       "75        75    REGULAR        13  \n",
       "76        76    REGULAR        13  \n",
       "50        50    REGULAR        13  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=\"score\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
