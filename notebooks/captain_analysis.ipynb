{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d20f64d",
   "metadata": {},
   "source": [
    "## EIG Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from battleship.utils import resolve_project_path\n",
    "\n",
    "# Path to run directory\n",
    "base_path = resolve_project_path(\"experiments/collaborative/captain_benchmarks/run_4o_mapeig_cot_captain\")\n",
    "\n",
    "# Find all captain.json files in subdirectories\n",
    "captain_files = glob.glob(os.path.join(base_path, '**/captain/captain.json'), recursive=True)\n",
    "\n",
    "# Dictionary to store eig values by file\n",
    "eig_values_by_file = {}\n",
    "# Initialize list to store data for DataFrame\n",
    "eig_data_list = []\n",
    "\n",
    "# Extract eig values from each file\n",
    "for file_path in captain_files:\n",
    "    # Get relative path for naming\n",
    "    rel_path = os.path.relpath(file_path, base_path)\n",
    "    \n",
    "    # Extract round_id from path\n",
    "    # Use regex to extract the part after 'round_' in the relative path\n",
    "    match = re.search(r'round_([a-zA-Z0-9]+)', rel_path)\n",
    "    round_id = match.group(1) if match else None\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    # Extract eig values, skipping None/null values\n",
    "    for idx, datum in enumerate(data):\n",
    "        if 'eig' in datum and datum['eig'] is not None and 'question' in datum and datum['question'] is not None:\n",
    "            question_text = datum['question']['question']['text'] if datum['question']['question'] and 'text' in datum['question']['question'] else \"No question text\"\n",
    "            eig_value = datum['eig']\n",
    "            \n",
    "            eig_questions = datum.get(\"eig_questions\", [])\n",
    "\n",
    "\n",
    "            if eig_questions is not None:\n",
    "                if len(eig_questions) != 0:\n",
    "                    eig_questions = [(q['question']['question']['text'],q['eig'], None) for q in eig_questions]\n",
    "                    max_eig = max([eq[1] for eq in eig_questions if eq[1] is not None])\n",
    "                    eig_questions = [(q[0], q[1], q[1] == max_eig) for q in eig_questions]\n",
    "\n",
    "            # Add to data list\n",
    "            eig_data_list.append({\n",
    "                'round_id': round_id,\n",
    "                'question_idx': idx,\n",
    "                'question': question_text,\n",
    "                'eig': eig_value,\n",
    "                'eig_questions': eig_questions,\n",
    "            })\n",
    "\n",
    "# Create DataFrame from the list\n",
    "model_eig_df = pd.DataFrame(eig_data_list)\n",
    "\n",
    "model_eig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /////////////////////////////////////////////////\n",
    "# This cell calculates EIG for human questions (and saves it to notebooks/human_eig_df.csv)\n",
    "# Caution: This will take 1-2 mins to run if human_eig_df.csv doesn't exist in the notebooks directory\n",
    "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from battleship.agents import EIGCalculator, CodeQuestion, Question\n",
    "from battleship.game import Board\n",
    "\n",
    "# JSON file to pull the code translations of human questions from\n",
    "input_json_path = resolve_project_path(\"experiments/collaborative/spotter_benchmarks/o4-mini_CodeSpotterModel_True.json\")\n",
    "\n",
    "def extract_questions_and_boards_to_dataframe(json_path):\n",
    "    \"\"\"\n",
    "    Extracts all questions asked and the board state at the time they were asked from a JSON file\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the input JSON file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted questions and board states.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"The file {json_path} does not exist.\")\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    extracted_data = []\n",
    "\n",
    "    for entry in data:\n",
    "        if \"question\" in entry and \"occTiles\" in entry:\n",
    "            question = entry[\"question\"]\n",
    "            program = entry[\"program\"]\n",
    "            board_state = entry[\"occTiles\"]\n",
    "            answer = entry[\"answer\"]\n",
    "            true_answer = entry[\"true_answer\"]\n",
    "\n",
    "            if answer.lower() == \"true\":\n",
    "                answer = \"yes\"\n",
    "            if answer.lower() == \"false\":\n",
    "                answer = \"no\"\n",
    "\n",
    "            extracted_data.append({\n",
    "                \"question\": question,\n",
    "                \"program\": program,\n",
    "                \"board_state\": board_state,\n",
    "                \"answer\": answer,\n",
    "                \"true_answer\": true_answer,\n",
    "                \"correct\": answer == true_answer\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(extracted_data)\n",
    "\n",
    "\n",
    "if os.path.exists('human_eig_df.csv'):\n",
    "    human_eig_df = pd.read_csv('human_eig_df.csv')\n",
    "else:\n",
    "    human_eig_df = extract_questions_and_boards_to_dataframe(input_json_path)\n",
    "    human_eig_df = human_eig_df[human_eig_df['correct'] == True]\n",
    "\n",
    "    eig_calculator = EIGCalculator(samples=1000, timeout=15, epsilon=0)\n",
    "\n",
    "    # Add a new column to store EIG values\n",
    "    human_eig_df[\"calculated_eig\"] = None\n",
    "\n",
    "    for idx, row in human_eig_df.iterrows():\n",
    "            # Create a CodeQuestion instance\n",
    "\n",
    "            l = ast.literal_eval(row[\"board_state\"])\n",
    "\n",
    "            code_question = CodeQuestion(\n",
    "                question=Question(row[\"question\"]),\n",
    "                fn_text=row[\"program\"],\n",
    "                translation_prompt=\"\",\n",
    "                completion={}\n",
    "            )\n",
    "\n",
    "            # Convert board_state to a Board instance\n",
    "            board_state = np.array(l)\n",
    "            board = Board(board=board_state)\n",
    "\n",
    "            # Calculate EIG\n",
    "            eig_value = eig_calculator(code_question, board)\n",
    "            human_eig_df.at[idx, \"calculated_eig\"] = eig_value\n",
    "\n",
    "    human_eig_df.to_csv('human_eig_df.csv', index=False)\n",
    "\n",
    "human_eig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'EIG Values': pd.concat([model_eig_df[\"eig\"], human_eig_df[\"calculated_eig\"]], ignore_index=True),\n",
    "    'Source': ['model_eig_df'] * len(model_eig_df) + ['human_eig_df'] * len(human_eig_df)\n",
    "})\n",
    "\n",
    "# Create a boxplot instead of a scatter plot\n",
    "sns.boxplot(data=plot_data, x='Source', y='EIG Values', palette='Set2')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Categorical Scatter Plot of EIG Values')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('EIG Values')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print the average EIG values for both distributions\n",
    "avg_model_eig = model_eig_df[\"eig\"].mean()\n",
    "avg_human_eig = human_eig_df[\"calculated_eig\"].astype(float).mean()\n",
    "\n",
    "print(f\"Average EIG for eig_df: {avg_model_eig:.4f}\")\n",
    "print(f\"Average EIG for output_df: {avg_human_eig:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad155118",
   "metadata": {},
   "source": [
    "## Precision/Recall Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from battleship.run_captain_benchmarks import rebuild_captain_summary_from_results\n",
    "from battleship.utils import resolve_project_path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77baafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ////////////////////////////////////////////////////\n",
    "# This cell defines all the paths for graph generation\n",
    "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "# We expect human round data to be in the battleship-final-data directory from the Drive\n",
    "human_round_data_path = resolve_project_path(\"experiments/collaborative/battleship-final-data/round.csv\")\n",
    "\n",
    "# We expect model round data to be in a 'captain_benchmarks' subdirectory of the collaborative experiments directory\n",
    "# These paths are all given in (LLM name, path) format\n",
    "model_round_data_unresolved_paths = [\n",
    "    ('4o', 'experiments/collaborative/captain_benchmarks/run_4o_eig_captain'),\n",
    "    ('4o','experiments/collaborative/captain_benchmarks/run_4o_llmdecision_captain'),\n",
    "    ('4o','experiments/collaborative/captain_benchmarks/run_4o_mapeig_cot_captain'),\n",
    "    ('4o','experiments/collaborative/captain_benchmarks/run_4o_mapeig_no_cot_captain'),\n",
    "    ('llama_4_scout','experiments/collaborative/captain_benchmarks/run_llama_4_scout_eigcaptain'),\n",
    "    ('llama_4_scout','experiments/collaborative/captain_benchmarks/run_llama_4_scout_llmdecision_captain'),\n",
    "    ('llama_4_scout','experiments/collaborative/captain_benchmarks/run_llama_4_scout_mapeig'),\n",
    "    ('o4-mini','experiments/collaborative/captain_benchmarks/run_o4-mini_eig_captains'),\n",
    "    ('o4-mini','experiments/collaborative/captain_benchmarks/run_o4-mini_mapeig_no_cot_captain'),\n",
    "    ('o4-mini','experiments/collaborative/captain_benchmarks/run_o4-mini_more_seeds'),\n",
    "    ('o4-mini','experiments/collaborative/captain_benchmarks/run_o4-mini_random_map_llmdecision_captains'),\n",
    "]\n",
    "\n",
    "model_round_data_paths = [(name, resolve_project_path(path)) for name, path in model_round_data_unresolved_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ////////////////////////////////////////////////////\n",
    "# This cell processes the human round data \n",
    "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "import os\n",
    "import ast\n",
    "\n",
    "MAX_QUESTIONS = 15\n",
    "\n",
    "# Load the CSV file containing round data\n",
    "\n",
    "# Define the path to the CSV file\n",
    "round_df = pd.read_csv(human_round_data_path)\n",
    "# Ensure occTiles and trueTiles are parsed as lists of lists if they are stored as strings\n",
    "\n",
    "def parse_tiles(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except Exception:\n",
    "            return []\n",
    "    return val\n",
    "\n",
    "round_df['occTiles'] = round_df['occTiles'].apply(parse_tiles)\n",
    "round_df['trueTiles'] = round_df['trueTiles'].apply(parse_tiles)\n",
    "\n",
    "def count_revealed_and_total(occ_tiles, true_tiles):\n",
    "    # Flatten the lists in case they are 2D\n",
    "    occ_flat = [item for sublist in occ_tiles for item in sublist] if occ_tiles else []\n",
    "    true_flat = [item for sublist in true_tiles for item in sublist] if true_tiles else []\n",
    "    # Total revealed: count of values in occTiles that are not -1\n",
    "    total_revealed = sum(1 for v in occ_flat if v != -1)\n",
    "    # True tiles: indices where trueTiles is not 0\n",
    "    true_indices = set(i for i, v in enumerate(true_flat) if v != 0)\n",
    "    # Revealed true tiles: those indices where occTiles is not -1 and trueTiles is not 0\n",
    "    revealed_true = sum(1 for i in true_indices if i < len(occ_flat) and occ_flat[i] != -1)\n",
    "    return pd.Series({'total_revealed': total_revealed, 'revealed_true': revealed_true, 'true_indices': true_indices})\n",
    "\n",
    "round_df[['total_revealed', 'revealed_true', 'true_indices']] = round_df.apply(\n",
    "    lambda row: count_revealed_and_total(row['occTiles'], row['trueTiles']), axis=1\n",
    ")\n",
    "\n",
    "round_df = round_df.assign(\n",
    "    captain_type='human',\n",
    "    spotter_type='human',\n",
    "    round_id=round_df['id'],\n",
    "    seed='human',\n",
    "    hits=round_df['revealed_true'],\n",
    "    misses=round_df['total_revealed'] - round_df['revealed_true'],\n",
    "    question_count=MAX_QUESTIONS - round_df['questionsRemaining'],\n",
    "    precision=lambda df: round_df['revealed_true'] / round_df['total_revealed'],\n",
    "    recall=lambda df: round_df['revealed_true'] / round_df['true_indices'].apply(len),\n",
    "    f1_score=lambda df: 2 * df['precision'] * df['recall'] / (df['precision'] + df['recall']).replace(0, np.nan),\n",
    "    is_won=lambda df: df['recall'] == 1,\n",
    "    llm='human'\n",
    ")[[\n",
    "    'captain_type', 'spotter_type', 'round_id', 'board_id', 'seed', 'hits', \n",
    "    'misses', 'is_won', 'question_count', 'precision', 'recall', 'f1_score', \"llm\"\n",
    "]]\n",
    "\n",
    "round_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc553b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ////////////////////////////////////////////////////\n",
    "# This cell processes the model round data\n",
    "# \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
    "\n",
    "dfs = []\n",
    "for name, path in model_round_data_paths:\n",
    "    df = pd.DataFrame(rebuild_captain_summary_from_results(path))\n",
    "    df['llm'] = name\n",
    "    dfs.append(df)\n",
    "\n",
    "summary_df = pd.concat(dfs, ignore_index=True)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58184756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the columns of summary_df with round_df\n",
    "aligned_summary_df = summary_df[round_df.columns]\n",
    "\n",
    "# Append summary_df to round_df\n",
    "combined_df = pd.concat([round_df, aligned_summary_df], ignore_index=True)\n",
    "\n",
    "# Display the shape of the combined DataFrame\n",
    "combined_df = combined_df[(combined_df['hits'] + combined_df['misses']) <= 40]\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.catplot(data=combined_df, kind=\"box\", col=\"llm\", x=\"captain_type\", y=\"f1_score\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_df = combined_df.sort_values(by='captain_type', ascending=True)\n",
    "sorted_df = sorted_df[sorted_df['captain_type'].isin(['human', 'RandomCaptain', 'MAPCaptain','LLMDecisionCaptain', 'EIGCaptain', 'MAPEIGCaptain'])]  \n",
    "\n",
    "sns.catplot(data=sorted_df, kind=\"box\", x=\"captain_type\", y=\"f1_score\", hue=\"llm\")\n",
    "\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75887334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for captain_type, group in combined_df.groupby('captain_type'):\n",
    "    f1_scores = np.sort(group['f1_score'].dropna())\n",
    "    cdf = np.arange(1, len(f1_scores) + 1) / len(f1_scores)\n",
    "    plt.step(f1_scores, cdf, where='post', label=captain_type)\n",
    "\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('CDF of F1 Score by Captain Type')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919bc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=combined_df, x=\"captain_type\", y=\"precision\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6972e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=combined_df, x=\"captain_type\", y=\"recall\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3115c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"move_count\"] = combined_df[\"hits\"] + combined_df[\"misses\"]\n",
    "\n",
    "sns.barplot(data=combined_df, x=\"captain_type\", y=\"move_count\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battleship_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
