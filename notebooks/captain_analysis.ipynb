{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d20f64d",
   "metadata": {},
   "source": [
    "## EIG Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ffbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from battleship.agents import EIGCalculator, Question, Board\n",
    "from battleship.spotters import create_spotter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "# Load the CSV into a DataFrame\n",
    "stage_csv_path = '/home/ubuntu/new_battleship/battleship/experiments/collaborative/battleship-final-data/stage.csv'\n",
    "round_csv_path = '/home/ubuntu/new_battleship/battleship/experiments/collaborative/battleship-final-data/round.csv'\n",
    "stage_df = pd.read_csv(stage_csv_path)\n",
    "round_df = pd.read_csv(round_csv_path)\n",
    "\n",
    "# Merge stage_df and round_df on stage_df['roundID'] == round_df['id']\n",
    "\n",
    "merged_df = pd.merge(stage_df, round_df, left_on='roundID', right_on='id', how='inner')\n",
    "\n",
    "question_rows = merged_df[merged_df['messageType'] == 'question']\n",
    "questions = question_rows['messageText'].tolist()\n",
    "occ_tiles = question_rows['occTiles_y'].tolist()\n",
    "boards = question_rows['board_id'].tolist()\n",
    "\n",
    "for idx in tqdm(range(len(questions[:75]))):\n",
    "    spotter = create_spotter(spotter_type=\"CodeSpotterModel\", board_id=boards[idx], board_experiment=\"collaborative\", llm=\"gpt-4.1-mini\", use_cot=True)\n",
    "\n",
    "    print(f\"Question {idx + 1}: {questions[idx]}\")\n",
    "    print(f\"Board State {idx + 1}: {occ_tiles[idx]}\")\n",
    "    # Process question\n",
    "    question = Question(text=questions[idx])\n",
    "    result = spotter.answer(\n",
    "        question,\n",
    "        board=Board.from_occ_tiles(occ_tiles[idx]),\n",
    "        history=None,\n",
    "    )\n",
    "\n",
    "    # Calculate EIG if we have a code question\n",
    "    eig_value = None\n",
    "    if result is not None and result.code_question:\n",
    "        try:\n",
    "            calculator = EIGCalculator(seed=0)\n",
    "            eig_value = calculator(\n",
    "                result.code_question, Board.from_occ_tiles(occ_tiles[idx])\n",
    "            )\n",
    "            print(eig_value)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to calculate EIG: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "RUN_ID = 'run_combined'\n",
    "\n",
    "# Path to search for captain.json files\n",
    "base_path = f'/home/ubuntu/new_battleship/battleship/experiments/collaborative/captain_benchmarks/'\n",
    "\n",
    "# Find all captain.json files in subdirectories\n",
    "captain_files = glob.glob(os.path.join(base_path, '**/captain/captain.json'), recursive=True)\n",
    "\n",
    "# Dictionary to store eig values by file\n",
    "eig_values_by_file = {}\n",
    "# Initialize list to store data for DataFrame\n",
    "eig_data_list = []\n",
    "\n",
    "# Extract eig values from each file\n",
    "for file_path in captain_files:\n",
    "    # Get relative path for naming\n",
    "    rel_path = os.path.relpath(file_path, base_path)\n",
    "    \n",
    "    # Extract round_id from path\n",
    "    round_id = rel_path.split('/')[0].split('_')[1]\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        # Extract eig values, skipping None/null values\n",
    "        for idx, datum in enumerate(data):\n",
    "            if 'eig' in datum and datum['eig'] is not None and 'question' in datum and datum['question'] is not None:\n",
    "                question_text = datum['question']['question']['text'] if datum['question']['question'] and 'text' in datum['question']['question'] else \"No question text\"\n",
    "                eig_value = datum['eig']\n",
    "                \n",
    "                eig_questions = datum.get(\"eig_questions\", [])\n",
    "\n",
    "                if len(eig_questions) != 0:\n",
    "                    eig_questions = [(q['question']['question']['text'],q['eig'], None) for q in eig_questions]\n",
    "                    max_eig = max([eq[1] for eq in eig_questions if eq[1] is not None])\n",
    "                    eig_questions = [(q[0], q[1], q[1] == max_eig) for q in eig_questions]\n",
    "\n",
    "                # Add to data list\n",
    "                eig_data_list.append({\n",
    "                    'round_id': round_id,\n",
    "                    'question_idx': idx,\n",
    "                    'question': question_text,\n",
    "                    'eig': eig_value,\n",
    "                    'eig_questions': eig_questions,\n",
    "                })\n",
    "                \n",
    "    except (json.JSONDecodeError, FileNotFoundError, KeyError, TypeError) as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Create DataFrame from the list\n",
    "eig_df = pd.DataFrame(eig_data_list)\n",
    "\n",
    "# Also maintain the old dictionary for backward compatibility\n",
    "for file_path in captain_files:\n",
    "    rel_path = os.path.relpath(file_path, base_path)\n",
    "    round_specific_eigs = eig_df[eig_df['round_id'] == rel_path.split('/')[0].split('_')[1]]['eig'].tolist()\n",
    "    if round_specific_eigs:\n",
    "        eig_values_by_file[rel_path] = round_specific_eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9889ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2bda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe where each row contains a round_id and a question from eig_questions\n",
    "expanded_df = []\n",
    "\n",
    "for _, row in eig_df.iterrows():\n",
    "    # Only process rows that have eig_questions\n",
    "    if row['eig_questions'] and len(row['eig_questions']) > 0:\n",
    "        for question, eig_value, is_max in row['eig_questions']:\n",
    "            expanded_df.append({\n",
    "                'round_id': row['round_id'],\n",
    "                'parent_question': row['question'],\n",
    "                'parent_eig': row['eig'],\n",
    "                'candidate_question': question,\n",
    "                'candidate_eig': eig_value,\n",
    "                'is_max_eig': is_max\n",
    "            })\n",
    "\n",
    "# Create the dataframe\n",
    "eig_questions_df = pd.DataFrame(expanded_df)\n",
    "\n",
    "# Display the first few rows\n",
    "eig_questions_df\n",
    "\n",
    "# Merge with df_combined to get captain_type for each question\n",
    "enriched_df = pd.merge(\n",
    "    eig_questions_df, \n",
    "    df_combined[['round_id', 'captain_type', 'board_id']], \n",
    "    on='round_id', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Create a scatter plot of candidate EIG values\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Get unique round_ids and create a mapping to x-coordinates\n",
    "unique_rounds = enriched_df['round_id'].unique()\n",
    "round_to_x = {round_id: i for i, round_id in enumerate(unique_rounds)}\n",
    "\n",
    "# Add slight random jitter to x positions to avoid overplotting\n",
    "np.random.seed(42)  # For reproducibility\n",
    "x_jitter = np.random.uniform(-0.1, 0.1, len(enriched_df))\n",
    "\n",
    "# Plot non-max points\n",
    "non_max = enriched_df[~enriched_df['is_max_eig']]\n",
    "plt.scatter([round_to_x[r] + j for r, j in zip(non_max['round_id'], x_jitter[:len(non_max)])], \n",
    "            non_max['candidate_eig'], \n",
    "            color='red', alpha=0.6, s=30, label='Alternative Questions')\n",
    "\n",
    "# Plot max points\n",
    "max_points = enriched_df[enriched_df['is_max_eig']]\n",
    "max_points = max_points[max_points['candidate_eig'] <= 1]  # Filter out EIG > 1\n",
    "plt.scatter([round_to_x[r] + j for r, j in zip(max_points['round_id'], x_jitter[len(non_max):len(non_max)+len(max_points)])], \n",
    "            max_points['candidate_eig'], \n",
    "            color='green', marker='*', s=200, label='Selected Questions (Max EIG)')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Round ID')\n",
    "plt.ylabel('Expected Information Gain (EIG)')\n",
    "plt.title('EIG Values of Candidate Questions by Round')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks([i for i in range(len(unique_rounds))], unique_rounds, rotation=90)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"Total candidate questions: {len(enriched_df)}\")\n",
    "print(f\"Number of selected questions (max EIG): {len(max_points)}\")\n",
    "print(f\"Average EIG of selected questions: {max_points['candidate_eig'].mean():.4f}\")\n",
    "print(f\"Average EIG of alternative questions: {non_max['candidate_eig'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4fd847",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to find and process result files\n",
    "def process_result_files():\n",
    "    # Construct path to results directory\n",
    "    results_path = f'/home/ubuntu/new_battleship/battleship/experiments/collaborative/captain_benchmarks/{RUN_ID}/results/'\n",
    "    \n",
    "    # Find all result JSON files\n",
    "    result_files = glob.glob(os.path.join(results_path, '*.json'))\n",
    "    \n",
    "    # List to store results\n",
    "    results_data = []\n",
    "    \n",
    "    # Process each result file\n",
    "    for file_path in result_files:\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "                # Each file might contain a list of results\n",
    "                for item in data:\n",
    "                    results_data.append(item)\n",
    "        except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_results = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Extract round_id from file names in eig_values_by_file\n",
    "    eig_data = []\n",
    "    for file_name in eig_values_by_file:\n",
    "        # Extract round_id from the path (format: round_XXXXXXXX/captain/captain.json)\n",
    "        round_id = file_name.split('/')[0].split('_')[1]\n",
    "        \n",
    "        # Calculate statistics for EIG values\n",
    "        values = eig_values_by_file[file_name]\n",
    "        eig_data.append({\n",
    "            'round_id': round_id,\n",
    "            'eig_count': len(values),\n",
    "            'eig_min': min(values),\n",
    "            'eig_max': max(values),\n",
    "            'eig_avg': sum(values)/len(values),\n",
    "            'eig_median': np.median(values)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame with EIG statistics\n",
    "    df_eig = pd.DataFrame(eig_data)\n",
    "    \n",
    "    # Merge the results and EIG DataFrames on round_id\n",
    "    df_combined = pd.merge(df_results, df_eig, on='round_id', how='left')\n",
    "    \n",
    "    return df_combined\n",
    "\n",
    "# Process the result files and create DataFrame\n",
    "df_combined = process_result_files()\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(f\"Combined DataFrame shape: {df_combined.shape}\")\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ec470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge eig_df with df_combined to get captain_type for each question\n",
    "merged_df = pd.merge(eig_df, df_combined[['round_id', 'captain_type', 'board_id']], on='round_id', how='inner')\n",
    "\n",
    "# Sort the dataframe by captain_type and round_id for better organization\n",
    "merged_df = merged_df.sort_values(by=['captain_type', 'round_id', 'question_idx'])\n",
    "\n",
    "# Display the first few rows to verify the merge\n",
    "print(f\"Merged DataFrame shape: {merged_df.shape}\")\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bb27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot of EIG values divided by captain type\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Filter out any remaining EIG values over 1 (additional safety check)\n",
    "filtered_merged_df = merged_df[merged_df['eig'] <= 1]\n",
    "\n",
    "# Get all unique captain types from filtered data\n",
    "captain_types = filtered_merged_df['captain_type'].unique()\n",
    "n_types = len(captain_types)\n",
    "\n",
    "# Prepare data for boxplot\n",
    "box_data = []\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, n_types))  # Generate colors for each captain type\n",
    "\n",
    "for captain_type in captain_types:\n",
    "    captain_data = filtered_merged_df[filtered_merged_df['captain_type'] == captain_type]['eig']\n",
    "    box_data.append(captain_data)\n",
    "\n",
    "# Create boxplot\n",
    "box = plt.boxplot(box_data, positions=range(n_types), widths=0.6, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "for i, (patch, color) in enumerate(zip(box['boxes'], colors)):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "# Customize plot\n",
    "plt.ylabel('EIG Value')\n",
    "plt.title('EIG Values by Captain Type')\n",
    "plt.xticks(range(n_types), captain_types, rotation=45, ha='right')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add sample size information\n",
    "for i, captain_type in enumerate(captain_types):\n",
    "    n_samples = len(filtered_merged_df[filtered_merged_df['captain_type'] == captain_type])\n",
    "    plt.text(i, plt.ylim()[0] + 0.05 * (plt.ylim()[1] - plt.ylim()[0]), \n",
    "             f'n={n_samples}', ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad155118",
   "metadata": {},
   "source": [
    "## Precision/Recall Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e02e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from battleship.run_captain_benchmarks import rebuild_captain_summary_from_results\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "\n",
    "# Load the CSV file containing round data\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_path = '/home/ubuntu/new_battleship/battleship/experiments/collaborative/battleship-final-data/round.csv'\n",
    "round_df = pd.read_csv(csv_path)\n",
    "# Ensure occTiles and trueTiles are parsed as lists of lists if they are stored as strings\n",
    "\n",
    "def parse_tiles(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return ast.literal_eval(val)\n",
    "        except Exception:\n",
    "            return []\n",
    "    return val\n",
    "\n",
    "round_df['occTiles'] = round_df['occTiles'].apply(parse_tiles)\n",
    "round_df['trueTiles'] = round_df['trueTiles'].apply(parse_tiles)\n",
    "\n",
    "def count_revealed_and_total(occ_tiles, true_tiles):\n",
    "    # Flatten the lists in case they are 2D\n",
    "    occ_flat = [item for sublist in occ_tiles for item in sublist] if occ_tiles else []\n",
    "    true_flat = [item for sublist in true_tiles for item in sublist] if true_tiles else []\n",
    "    # Total revealed: count of values in occTiles that are not -1\n",
    "    total_revealed = sum(1 for v in occ_flat if v != -1)\n",
    "    # True tiles: indices where trueTiles is not 0\n",
    "    true_indices = set(i for i, v in enumerate(true_flat) if v != 0)\n",
    "    # Revealed true tiles: those indices where occTiles is not -1 and trueTiles is not 0\n",
    "    revealed_true = sum(1 for i in true_indices if i < len(occ_flat) and occ_flat[i] != -1)\n",
    "    return pd.Series({'total_revealed': total_revealed, 'revealed_true': revealed_true, 'true_indices': true_indices})\n",
    "\n",
    "round_df[['total_revealed', 'revealed_true', 'true_indices']] = round_df.apply(\n",
    "    lambda row: count_revealed_and_total(row['occTiles'], row['trueTiles']), axis=1\n",
    ")\n",
    "\n",
    "round_df = round_df.assign(\n",
    "    captain_type='human',\n",
    "    spotter_type='human',\n",
    "    round_id=round_df['id'],\n",
    "    seed='human',\n",
    "    hits=round_df['revealed_true'],\n",
    "    misses=round_df['total_revealed'] - round_df['revealed_true'],\n",
    "    question_count=None,\n",
    "    precision=lambda df: round_df['revealed_true'] / round_df['total_revealed'],\n",
    "    recall=lambda df: round_df['revealed_true'] / round_df['true_indices'].apply(len),\n",
    "    f1_score=lambda df: 2 * df['precision'] * df['recall'] / (df['precision'] + df['recall']).replace(0, np.nan),\n",
    "    is_won=lambda df: df['recall'] == 1,\n",
    "    llm='human'\n",
    ")[[\n",
    "    'captain_type', 'spotter_type', 'round_id', 'board_id', 'seed', 'hits', \n",
    "    'misses', 'is_won', 'question_count', 'precision', 'recall', 'f1_score', \"llm\"\n",
    "]]\n",
    "\n",
    "round_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07cbf6e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Filter rows with 'human' captain_type\n",
    "human_df = round_df[round_df['captain_type'] == 'human']\n",
    "\n",
    "# # Calculate the percentile of f1_score for 'human' captain_type\n",
    "# lower_percentile = 0\n",
    "# upper_percentile = 100\n",
    "\n",
    "# lower_threshold = human_df['f1_score'].quantile(lower_percentile / 100)\n",
    "# upper_threshold = human_df['f1_score'].quantile(upper_percentile / 100)\n",
    "\n",
    "# filtered_df = human_df[(human_df['f1_score'] >= lower_threshold) & (human_df['f1_score'] <= upper_threshold)]\n",
    "\n",
    "# elite_round_df = filtered_df\n",
    "\n",
    "# # Display the resulting DataFrame\n",
    "# elite_round_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc553b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = [\n",
    "    (\"llama_4_scout\",'/home/ubuntu/new_battleship/battleship/experiments/collaborative/captain_benchmarks/combined/run_llama_4_scout_combined'),\n",
    "    (\"4o\",'/home/ubuntu/new_battleship/battleship/experiments/collaborative/captain_benchmarks/combined/run_4o'),\n",
    "    (\"o4-mini\",'/home/ubuntu/new_battleship/battleship/experiments/collaborative/captain_benchmarks/combined/run_o4-mini')\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for name, path in data_paths:\n",
    "    df = pd.DataFrame(rebuild_captain_summary_from_results(path))\n",
    "    df['llm'] = name\n",
    "    dfs.append(df)\n",
    "\n",
    "#summary = rebuild_captain_summary_from_results(\"/home/ubuntu/new_battleship/battleship/experiments/collaborative/captain_benchmarks/combined/run_llama_4_scout_combined\")\n",
    "#summary = rebuild_captain_summary_from_results(\"/home/ubuntu/new_battleship/battleship/experiments/collaborative/captain_benchmarks/combined/run_4o\")\n",
    "#summary = rebuild_captain_summary_from_results(\"/home/ubuntu/new_battleship/battleship/experiments/collaborative/captain_benchmarks/combined/run_o4-mini\")\n",
    "\n",
    "summary_df = pd.concat(dfs, ignore_index=True)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58184756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align the columns of summary_df with round_df\n",
    "aligned_summary_df = summary_df[round_df.columns]\n",
    "\n",
    "# Append summary_df to round_df\n",
    "combined_df = pd.concat([round_df, aligned_summary_df], ignore_index=True)\n",
    "\n",
    "# Display the shape of the combined DataFrame\n",
    "combined_df = combined_df[(combined_df['hits'] + combined_df['misses']) <= 40]\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c54efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.catplot(data=combined_df, kind=\"box\", col=\"llm\", x=\"captain_type\", y=\"f1_score\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sorted_df = combined_df.sort_values(by='captain_type', ascending=True)\n",
    "sorted_df = sorted_df[sorted_df['captain_type'].isin(['human', 'RandomCaptain', 'MAPCaptain','LLMDecisionCaptain', 'EIGCaptain', 'MAPEIGCaptain'])]  \n",
    "\n",
    "sns.catplot(data=sorted_df, kind=\"box\", x=\"captain_type\", y=\"f1_score\", hue=\"llm\")\n",
    "\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75887334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for captain_type, group in combined_df.groupby('captain_type'):\n",
    "    f1_scores = np.sort(group['f1_score'].dropna())\n",
    "    cdf = np.arange(1, len(f1_scores) + 1) / len(f1_scores)\n",
    "    plt.step(f1_scores, cdf, where='post', label=captain_type)\n",
    "\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Cumulative Probability')\n",
    "plt.title('CDF of F1 Score by Captain Type')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919bc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=combined_df, x=\"captain_type\", y=\"precision\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6972e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=combined_df, x=\"captain_type\", y=\"recall\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3115c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"move_count\"] = combined_df[\"hits\"] + combined_df[\"misses\"]\n",
    "\n",
    "sns.barplot(data=combined_df, x=\"captain_type\", y=\"move_count\", hue=\"captain_type\")\n",
    "plt.xticks(rotation=45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battleship_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
